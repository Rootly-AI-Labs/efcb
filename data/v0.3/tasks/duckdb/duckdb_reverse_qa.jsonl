{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/storage/wal_replay.cpp\n@@ -816,19 +816,24 @@ void WriteAheadLogDeserializer::ReplayDelete() {\n \t\treturn;\n \t}\n \tif (!state.current_table) {\n-\t\tthrow InternalException(\"Corrupt WAL: delete without table\");\n+\t\tthrow SerializationException(\"delete without a table\");\n \t}\n \n \tD_ASSERT(chunk.ColumnCount() == 1 && chunk.data[0].GetType() == LogicalType::ROW_TYPE);\n \trow_t row_ids[1];\n \tVector row_identifiers(LogicalType::ROW_TYPE, data_ptr_cast(row_ids));\n-\n \tauto source_ids = FlatVector::GetData<row_t>(chunk.data[0]);\n-\t// delete the tuples from the current table\n+\n+\t// Delete the row IDs from the current table.\n+\tauto &storage = state.current_table->GetStorage();\n+\tauto total_rows = storage.GetTotalRows();\n \tTableDeleteState delete_state;\n \tfor (idx_t i = 0; i < chunk.size(); i++) {\n+\t\tif (source_ids[i] >= UnsafeNumericCast<row_t>(total_rows)) {\n+\t\t\tthrow SerializationException(\"invalid row ID delete in WAL\");\n+\t\t}\n \t\trow_ids[0] = source_ids[i];\n-\t\tstate.current_table->GetStorage().Delete(delete_state, context, row_identifiers, 1);\n+\t\tstorage.Delete(delete_state, context, row_identifiers, 1);\n \t}\n }\n \n---\n"}], "ideal": "Title: [Fix] Throw serialisation error when encountering invalid row IDs in WAL delete\n\nBody: Close https://github.com/duckdblabs/duckdb-internal/issues/4556"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/BundleStaticLibs.yml\n@@ -192,14 +192,14 @@ jobs:\n           bash -c \"\n             set -e\n             yum install -y perl-IPC-Cmd gcc-toolset-12 gcc-toolset-12-gcc-c++\n-          \n+            \n             source /opt/rh/gcc-toolset-12/enable\n             export CC=gcc\n             export CXX=g++\n+\n             git config --global --add safe.directory $PWD\n             make gather-libs -C $PWD\n           \"\n-\n       - name: Print platform\n         shell: bash\n         run: ./build/release/duckdb -c \"PRAGMA platform;\"\n---\n.github/workflows/LinuxRelease.yml\n@@ -91,6 +91,7 @@ jobs:\n           source /opt/rh/gcc-toolset-12/enable\n           export CC=gcc\n           export CXX=g++\n+\n           git config --global --add safe.directory $PWD\n           make -C $PWD\n         \"\n---\n"}], "ideal": "Title: Fixup confict\n\nBody: @taniabogatsch, something is off with your cherry-pick, it can't cleanly be merge with main, and I was bothered enough that I just went ahead."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/aggregate/physical_streaming_window.cpp\n@@ -108,7 +108,7 @@ class StreamingWindowState : public OperatorState {\n \n \tstruct LeadLagState {\n \t\t//\tFixed size\n-\t\tstatic constexpr idx_t MAX_BUFFER = 2048U;\n+\t\tstatic constexpr int64_t MAX_BUFFER = 2048;\n \n \t\tstatic bool ComputeOffset(ClientContext &context, BoundWindowExpression &wexpr, int64_t &offset) {\n \t\t\toffset = 1;\n@@ -131,7 +131,7 @@ class StreamingWindowState : public OperatorState {\n \t\t\tif (wexpr.GetExpressionType() == ExpressionType::WINDOW_LEAD) {\n \t\t\t\toffset = -offset;\n \t\t\t}\n-\t\t\treturn idx_t(std::abs(offset)) < MAX_BUFFER;\n+\t\t\treturn std::abs(offset) < MAX_BUFFER;\n \t\t}\n \n \t\tstatic bool ComputeDefault(ClientContext &context, BoundWindowExpression &wexpr, Value &result) {\n---\n"}], "ideal": "Title: Internal #5069: Win32 Cast Simplification\n\nBody: * Just do signed compares to fix Win32 test failure"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/multi_file/multi_file_column_mapper.cpp\n@@ -18,11 +18,10 @@ MultiFileColumnMapper::MultiFileColumnMapper(ClientContext &context, MultiFileRe\n                                              const vector<MultiFileColumnDefinition> &global_columns,\n                                              const vector<ColumnIndex> &global_column_ids,\n                                              optional_ptr<TableFilterSet> filters, MultiFileList &multi_file_list,\n-                                             const MultiFileReaderBindData &bind_data,\n                                              const virtual_column_map_t &virtual_columns)\n     : context(context), multi_file_reader(multi_file_reader), multi_file_list(multi_file_list),\n       reader_data(reader_data), global_columns(global_columns), global_column_ids(global_column_ids),\n-      global_filters(filters), bind_data(bind_data), virtual_columns(virtual_columns) {\n+      global_filters(filters), virtual_columns(virtual_columns) {\n }\n \n struct MultiFileIndexMapping {\n@@ -743,10 +742,10 @@ ResultColumnMapping MultiFileColumnMapper::CreateColumnMappingByMapper(const Col\n \treturn result;\n }\n \n-ResultColumnMapping MultiFileColumnMapper::CreateColumnMapping() {\n+ResultColumnMapping MultiFileColumnMapper::CreateColumnMapping(MultiFileColumnMappingMode mapping_mode) {\n \tauto &reader = *reader_data.reader;\n \tauto &local_columns = reader.GetColumns();\n-\tswitch (bind_data.mapping) {\n+\tswitch (mapping_mode) {\n \tcase MultiFileColumnMappingMode::BY_NAME: {\n \t\t// we have expected types: create a map of name -> (local) column id\n \t\tNameMapper name_map(*this, local_columns);\n@@ -1107,9 +1106,9 @@ unique_ptr<TableFilterSet> MultiFileColumnMapper::CreateFilters(map<idx_t, refer\n \treturn result;\n }\n \n-ReaderInitializeType MultiFileColumnMapper::CreateMapping() {\n+ReaderInitializeType MultiFileColumnMapper::CreateMapping(MultiFileColumnMappingMode mapping_mode) {\n \t// copy global columns and inject any different defaults\n-\tauto result = CreateColumnMapping();\n+\tauto result = CreateColumnMapping(mapping_mode);\n \t//! Evaluate the filters against the column(s) that are constant for this file (not present in the local schema)\n \t//! If any of these fail, the file can be skipped entirely\n \tmap<idx_t, reference<TableFilter>> remaining_filters;\n---\nsrc/common/multi_file/multi_file_reader.cpp\n@@ -371,13 +371,23 @@ MultiFileReader::InitializeGlobalState(ClientContext &context, const MultiFileOp\n \treturn nullptr;\n }\n \n+ReaderInitializeType\n+MultiFileReader::CreateMapping(ClientContext &context, MultiFileReaderData &reader_data,\n+                               const vector<MultiFileColumnDefinition> &global_columns,\n+                               const vector<ColumnIndex> &global_column_ids, optional_ptr<TableFilterSet> filters,\n+                               MultiFileList &multi_file_list, const MultiFileReaderBindData &bind_data,\n+                               const virtual_column_map_t &virtual_columns, MultiFileColumnMappingMode mapping_mode) {\n+\tMultiFileColumnMapper column_mapper(context, *this, reader_data, global_columns, global_column_ids, filters,\n+\t                                    multi_file_list, virtual_columns);\n+\treturn column_mapper.CreateMapping(mapping_mode);\n+}\n+\n ReaderInitializeType MultiFileReader::CreateMapping(\n     ClientContext &context, MultiFileReaderData &reader_data, const vector<MultiFileColumnDefinition> &global_columns,\n     const vector<ColumnIndex> &global_column_ids, optional_ptr<TableFilterSet> filters, MultiFileList &multi_file_list,\n     const MultiFileReaderBindData &bind_data, const virtual_column_map_t &virtual_columns) {\n-\tMultiFileColumnMapper column_mapper(context, *this, reader_data, global_columns, global_column_ids, filters,\n-\t                                    multi_file_list, bind_data, virtual_columns);\n-\treturn column_mapper.CreateMapping();\n+\treturn CreateMapping(context, reader_data, global_columns, global_column_ids, filters, multi_file_list, bind_data,\n+\t                     virtual_columns, bind_data.mapping);\n }\n \n string GetExtendedMultiFileError(const MultiFileBindData &bind_data, const Expression &expr, BaseFileReader &reader,\n---\nsrc/include/duckdb/common/multi_file/multi_file_column_mapper.hpp\n@@ -19,16 +19,15 @@ class MultiFileColumnMapper {\n \tMultiFileColumnMapper(ClientContext &context, MultiFileReader &multi_file_reader, MultiFileReaderData &reader_data,\n \t                      const vector<MultiFileColumnDefinition> &global_columns,\n \t                      const vector<ColumnIndex> &global_column_ids, optional_ptr<TableFilterSet> filters,\n-\t                      MultiFileList &multi_file_list, const MultiFileReaderBindData &bind_data,\n-\t                      const virtual_column_map_t &virtual_columns);\n+\t                      MultiFileList &multi_file_list, const virtual_column_map_t &virtual_columns);\n \n public:\n-\tReaderInitializeType CreateMapping();\n+\tReaderInitializeType CreateMapping(MultiFileColumnMappingMode mapping_mode);\n \n \tvoid ThrowColumnNotFoundError(const string &global_column_name) const;\n \n private:\n-\tResultColumnMapping CreateColumnMapping();\n+\tResultColumnMapping CreateColumnMapping(MultiFileColumnMappingMode mapping_mode);\n \tResultColumnMapping CreateColumnMappingByMapper(const ColumnMapper &mapper);\n \n \tunique_ptr<TableFilterSet> CreateFilters(map<idx_t, reference<TableFilter>> &filters, ResultColumnMapping &mapping);\n@@ -45,7 +44,6 @@ class MultiFileColumnMapper {\n \tconst vector<MultiFileColumnDefinition> &global_columns;\n \tconst vector<ColumnIndex> &global_column_ids;\n \toptional_ptr<TableFilterSet> global_filters;\n-\tconst MultiFileReaderBindData &bind_data;\n \tconst virtual_column_map_t &virtual_columns;\n };\n \n---\nsrc/include/duckdb/common/multi_file/multi_file_reader.hpp\n@@ -106,6 +106,13 @@ struct MultiFileReader {\n \t                                     optional_ptr<MultiFileReaderGlobalState> global_state);\n \n \t//! Create all required mappings from the global types/names to the file-local types/names\n+\tDUCKDB_API virtual ReaderInitializeType\n+\tCreateMapping(ClientContext &context, MultiFileReaderData &reader_data,\n+\t              const vector<MultiFileColumnDefinition> &global_columns, const vector<ColumnIndex> &global_column_ids,\n+\t              optional_ptr<TableFilterSet> filters, MultiFileList &multi_file_list,\n+\t              const MultiFileReaderBindData &bind_data, const virtual_column_map_t &virtual_columns,\n+\t              MultiFileColumnMappingMode mapping_mode);\n+\n \tDUCKDB_API virtual ReaderInitializeType\n \tCreateMapping(ClientContext &context, MultiFileReaderData &reader_data,\n \t              const vector<MultiFileColumnDefinition> &global_columns, const vector<ColumnIndex> &global_column_ids,\n---\n"}], "ideal": "Title: MultiFileReader: Make column mapping mode configurable per-file, instead of requiring it to be set globally\n\nBody: This allows the column mapping mode to be modified on a per-file basis by the MultiFileReader"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/BundleStaticLibs.yml\n@@ -189,7 +189,16 @@ jobs:\n           -e BUILD_BENCHMARK=1                                                   \\\n           -e FORCE_WARN_UNUSED=1                                                 \\\n           quay.io/pypa/manylinux_2_28_${{ matrix.config.image }}                  \\\n-          bash -c \"yum install -y perl-IPC-Cmd && git config --global --add safe.directory $PWD && make gather-libs -C $PWD\"\n+          bash -c \"\n+            set -e\n+            yum install -y perl-IPC-Cmd gcc-toolset-12 gcc-toolset-12-gcc-c++\n+          \n+            source /opt/rh/gcc-toolset-12/enable\n+            export CC=gcc\n+            export CXX=g++\n+            git config --global --add safe.directory $PWD\n+            make gather-libs -C $PWD\n+          \"\n \n       - name: Print platform\n         shell: bash\n---\n.github/workflows/LinuxRelease.yml\n@@ -84,7 +84,16 @@ jobs:\n         -e BUILD_BENCHMARK=1                                                   \\\n         -e FORCE_WARN_UNUSED=1                                                 \\\n         quay.io/pypa/manylinux_2_28_${{ matrix.config.image }}                  \\\n-        bash -c \"yum install -y perl-IPC-Cmd && git config --global --add safe.directory $PWD && make -C $PWD\"\n+        bash -c \"\n+          set -e\n+          yum install -y perl-IPC-Cmd gcc-toolset-12 gcc-toolset-12-gcc-c++\n+        \n+          source /opt/rh/gcc-toolset-12/enable\n+          export CC=gcc\n+          export CXX=g++\n+          git config --global --add safe.directory $PWD\n+          make -C $PWD\n+        \"\n \n     - name: Print platform\n       shell: bash\n---\n"}], "ideal": "Title: Support glibc 2.28 environments in 1.3.x\n\nBody: Backport the changes in https://github.com/duckdb/duckdb/pull/17776 so we build with GCC 12 explicitly in the next bug fix release, instead of only in 1.4."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/actions/build_extensions_dockerized/action.yml\n@@ -17,6 +17,9 @@ inputs:\n   build_complete_extensions_set:\n     description: 'Whether all extensions needs to be built'\n     default: 1\n+  save_cache:\n+    description: 'Should cache be saved'\n+    default: 1\n \n runs:\n   using: \"composite\"\n@@ -79,7 +82,7 @@ runs:\n           mkdir ccache_dir\n \n       - name: Load Ccache\n-        uses: actions/cache@v4\n+        uses: actions/cache/restore@v4\n         with:\n           path: ./ccache_dir\n           key: ccache-extension-distribution-${{ inputs.duckdb_arch }}-${{ steps.ccache_timestamp.outputs.timestamp }}\n@@ -96,6 +99,13 @@ runs:\n         run: |\n           docker run --env-file=docker_env.txt -v `pwd`:/duckdb_build_dir -v `pwd`/ccache_dir:/ccache_dir duckdb/${{ inputs.duckdb_arch }} make release\n \n+      - name: Save Ccache\n+        if: ${{ inputs.save_cache }}\n+        uses: actions/cache/save@v4\n+        with:\n+          path: ./ccache_dir\n+          key: ccache-extension-distribution-${{ inputs.duckdb_arch }}-${{ steps.ccache_timestamp.outputs.timestamp }}\n+\n       - name: Test extension (inside docker)\n         shell: bash\n         if: ${{ inputs.run_tests == 1 }}\n---\n.github/workflows/LinuxRelease.yml\n@@ -152,6 +152,7 @@ jobs:\n           override_git_describe: ${{ inputs.override_git_describe }}\n           build_complete_extensions_set: 1\n           # Note that build_complete_extensions_set needs to be 1 (that is also the default value) for 'Checks extension entries' to work correctly\n+          save_cache: ${{ github.event_name != 'pull_request' && 1 || 0 }}\n \n       - uses: actions/upload-artifact@v4\n         with:\n@@ -196,6 +197,7 @@ jobs:\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n           override_git_describe: ${{ inputs.override_git_describe }}\n           build_complete_extensions_set: ${{ github.event_name != 'pull_request' && 1 || 0 }}\n+          save_cache: ${{ github.event_name != 'pull_request' && 1 || 0 }}\n \n       - uses: actions/upload-artifact@v4\n         with:\n@@ -239,6 +241,7 @@ jobs:\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n           override_git_describe: ${{ inputs.override_git_describe }}\n           build_complete_extensions_set: ${{ github.event_name != 'pull_request' && 1 || 0 }}\n+          save_cache: ${{ github.event_name != 'pull_request' && 1 || 0 }}\n \n       - uses: actions/upload-artifact@v4\n         with:\n---\n"}], "ideal": "Title: Avoid saving ccache on pull_requests\n\nBody: Step further bringing github caching back to be functional, that should translate to faster evaluation cycles on PRs.\r\n\r\nProblem is that currently for the 3 set of linux extensions, that are a bulk of CI time, cache items are added on every PR, making so that cache items from base branches will get evicted, and means less effective caching.\r\n\r\nBasic is as follows:\r\n* PR can access cache items from any predecessor. Cache items produced by PRs can only be reused by the exact same PR\r\n* Base branches (say `v1.3-ossivalis` or `main`) can access cache only from other base branches, but their cache items can be used by anyone.\r\n* When total cache size grows past 10 GB, GitHub will evict older items (that are likely to be the base branches one)\r\n\r\nCurrent situation that happens somewhat frequently is that PR pollute the cache, keep invalidating it, eventually removing the only valuable items in there. This PR aims at producing less items in the global cache."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types/value.cpp\n@@ -168,7 +168,7 @@ Value::Value(const Value &other)\n }\n \n Value::Value(Value &&other) noexcept\n-    : type_(std::move(other.type_)), is_null(other.is_null), value_(other.value_),\n+    : type_(std::move(other.type_)), is_null(other.is_null), value_(std::move(other.value_)),\n       value_info_(std::move(other.value_info_)) {\n }\n \n---\n"}], "ideal": "Title: Value: do std::move also for value_\n\nBody: I stumbled on this while on a rabbit hole around std::locale, I am not really sure if there is any benefit to having a move, but might be? Feel free to close, it's minor either way, this might result in more optimized code-generation but hard to say.\r\n\r\nPotentially counting allocations (over the same workload) might show a difference, but again, I have not checked."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/string_util.cpp\n@@ -105,7 +105,7 @@ string StringUtil::Repeat(const string &str, idx_t n) {\n namespace string_util_internal {\n \n inline void SkipSpaces(const string &str, idx_t &index) {\n-\twhile (index < str.size() && std::isspace(str[index])) {\n+\twhile (index < str.size() && StringUtil::CharacterIsSpace(str[index])) {\n \t\tindex++;\n \t}\n }\n@@ -136,7 +136,9 @@ inline string TakePossiblyQuotedItem(const string &str, idx_t &index, char delim\n \t\tConsumeLetter(str, index, quote);\n \t} else {\n \t\tTakeWhile(\n-\t\t    str, index, [delimiter, quote](char c) { return c != delimiter && c != quote && !std::isspace(c); }, entry);\n+\t\t    str, index,\n+\t\t    [delimiter, quote](char c) { return c != delimiter && c != quote && !StringUtil::CharacterIsSpace(c); },\n+\t\t    entry);\n \t}\n \n \treturn entry;\n---\nsrc/include/duckdb/function/cast/vector_cast_helpers.hpp\n@@ -197,9 +197,9 @@ struct VectorCastHelpers {\n \n \t\tif (STRUCT_KEY) {\n \t\t\tneeds_quotes = true;\n-\t\t} else if (isspace(string_data[0])) {\n+\t\t} else if (StringUtil::CharacterIsSpace(string_data[0])) {\n \t\t\tneeds_quotes = true;\n-\t\t} else if (base_length >= 2 && isspace(string_data[base_length - 1])) {\n+\t\t} else if (base_length >= 2 && StringUtil::CharacterIsSpace(string_data[base_length - 1])) {\n \t\t\tneeds_quotes = true;\n \t\t} else if (StringUtil::CIEquals(string_data, base_length, \"null\", 4)) {\n \t\t\tneeds_quotes = true;\n---\ntools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp\n@@ -471,7 +471,7 @@ int sqlite3_exec(sqlite3 *db,                /* The database on which the SQL ex\n \t\t\t\trc = sqlite3_finalize(pStmt);\n \t\t\t\tpStmt = nullptr;\n \t\t\t\tzSql = zLeftover;\n-\t\t\t\twhile (isspace(zSql[0]))\n+\t\t\t\twhile (StringUtil::CharacterIsSpace(zSql[0]))\n \t\t\t\t\tzSql++;\n \t\t\t\tbreak;\n \t\t\t} else if (rc != SQLITE_ROW) {\n---\n"}], "ideal": "Title: local_agnostic::isspace to avoid spaces be depending on locale\n\nBody: Similar to https://github.com/duckdb/duckdb/pull/17807, this adds locale-independent handling for isspace.\r\n\r\nI have to say I don't remember exactly if I manage to reproduce a problem with this or just looked right to have.\r\n\r\nFeedback on the implementation welcome."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/catalog/catalog.cpp\n@@ -921,9 +921,13 @@ CatalogEntryLookup Catalog::TryLookupEntry(CatalogEntryRetriever &retriever, con\n \tlookups.reserve(entries.size());\n \tfor (auto &entry : entries) {\n \t\toptional_ptr<Catalog> catalog_entry;\n-\t\tcatalog_entry = Catalog::GetCatalogEntry(retriever, entry.catalog);\n+\t\tif (if_not_found == OnEntryNotFound::RETURN_NULL) {\n+\t\t\tcatalog_entry = Catalog::GetCatalogEntry(retriever, entry.catalog);\n+\t\t} else {\n+\t\t\tcatalog_entry = &Catalog::GetCatalog(retriever, entry.catalog);\n+\t\t}\n \t\tif (!catalog_entry) {\n-\t\t\tcontinue;\n+\t\t\treturn {nullptr, nullptr, ErrorData()};\n \t\t}\n \t\tD_ASSERT(catalog_entry);\n \t\tauto lookup_behavior = catalog_entry->CatalogTypeLookupRule(lookup_info.GetCatalogType());\n---\ntest/sql/attach/attach_wal_default.test\n@@ -1,47 +0,0 @@\n-# name: test/sql/attach/attach_wal_default.test\n-# description: Test attaching a WAL with functions as a default value\n-# group: [attach]\n-\n-statement ok\n-SET checkpoint_threshold = '10.0 GB';\n-\n-statement ok\n-PRAGMA disable_checkpoint_on_shutdown;\n-\n-statement ok\n-attach 'blubb.db' as db1;\n-\n-statement ok\n-use db1;\n-\n-statement ok\n-BEGIN\n-\n-statement ok\n-CREATE TABLE IF NOT EXISTS \"table1\" (\n-  cache_key VARCHAR PRIMARY KEY,\n-  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n-);\n-\n-statement ok\n-CREATE TABLE IF NOT EXISTS \"table2\" (\n-  cache_key VARCHAR NOT NULL,\n-  dose DOUBLE NOT NULL,\n-  PRIMARY KEY (cache_key, dose),\n-  FOREIGN KEY (cache_key) REFERENCES \"table1\"(cache_key)\n-);\n-\n-statement ok\n-COMMIT\n-\n-statement ok\n-attach ':memory:' as other;\n-\n-statement ok\n-use other;\n-\n-statement ok\n-detach db1;\n-\n-statement ok\n-attach 'blubb.db' as db2;\n---\n"}], "ideal": "Title: Revert \"Avoid early-out when catalog lookup fails - instead finish all look-ups\"\n\nBody: \r\nThis reverts commit 3c2f966d1fc2d91df28bd7b7dcc70bfd4d0ee638."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/persistent/physical_copy_to_file.cpp\n@@ -439,7 +439,7 @@ unique_ptr<GlobalSinkState> PhysicalCopyToFile::GetGlobalSinkState(ClientContext\n \t\t}\n \n \t\tauto state = make_uniq<CopyToFunctionGlobalState>(context);\n-\t\tif (!per_thread_output && rotate) {\n+\t\tif (!per_thread_output && rotate && write_empty_file) {\n \t\t\tauto global_lock = state->lock.GetExclusiveLock();\n \t\t\tstate->global_state = CreateFileState(context, *state, *global_lock);\n \t\t}\n@@ -497,6 +497,9 @@ void PhysicalCopyToFile::WriteRotateInternal(ExecutionContext &context, GlobalSi\n \twhile (true) {\n \t\t// Grab global lock and dereference the current file state (and corresponding lock)\n \t\tauto global_guard = g.lock.GetExclusiveLock();\n+\t\tif (!g.global_state) {\n+\t\t\tg.global_state = CreateFileState(context.client, *sink_state, *global_guard);\n+\t\t}\n \t\tauto &file_state = *g.global_state;\n \t\tauto &file_lock = *g.file_write_lock_if_rotating;\n \t\tif (rotate && function.rotate_next_file(file_state, *bind_data, file_size_bytes)) {\n@@ -530,7 +533,7 @@ SinkResultType PhysicalCopyToFile::Sink(ExecutionContext &context, DataChunk &ch\n \tauto &g = input.global_state.Cast<CopyToFunctionGlobalState>();\n \tauto &l = input.local_state.Cast<CopyToFunctionLocalState>();\n \n-\tif (!write_empty_file) {\n+\tif (!write_empty_file && !rotate) {\n \t\t// if we are only writing the file when there are rows to write we need to initialize here\n \t\tg.Initialize(context.client, *this);\n \t}\n---\nsrc/planner/binder/statement/bind_copy.cpp\n@@ -266,10 +266,6 @@ BoundStatement Binder::BindCopyTo(CopyStatement &stmt, CopyToType copy_to_type)\n \t\t}\n \t}\n \tif (!write_empty_file) {\n-\t\tif (rotate) {\n-\t\t\tthrow NotImplementedException(\n-\t\t\t    \"Can't combine WRITE_EMPTY_FILE false with file rotation (e.g., ROW_GROUPS_PER_FILE)\");\n-\t\t}\n \t\tif (per_thread_output) {\n \t\t\tthrow NotImplementedException(\"Can't combine WRITE_EMPTY_FILE false with PER_THREAD_OUTPUT\");\n \t\t}\n---\ntest/sql/copy/parquet/writer/skip_empty_write.test\n@@ -48,13 +48,21 @@ copy (from tbl where i = 20000) to '__TEST_DIR__/empty.parquet' (WRITE_EMPTY_FIL\n \n endloop\n \n+# write_empty_file with file_size_bytes\n+query I\n+copy (select 42 where 42=84) to '__TEST_DIR__/empty_file_size_bytes/' (FORMAT PARQUET, WRITE_EMPTY_FILE false, FILENAME_PATTERN '{uuidv7}.parquet', FILE_SIZE_BYTES 128)\n+----\n+0\n \n-# these combinations are not allowed\n-statement error\n-copy tbl to '__TEST_DIR__/empty.parquet' (WRITE_EMPTY_FILE false, ROW_GROUPS_PER_FILE 1)\n+query I\n+SELECT COUNT(*) FROM glob('__TEST_DIR__/empty_file_size_bytes/*.parquet')\n ----\n-Can't combine\n+0\n \n+statement ok\n+copy tbl to '__TEST_DIR__/empty_row_groups_per_file.parquet' (WRITE_EMPTY_FILE false, ROW_GROUPS_PER_FILE 1)\n+\n+# these combinations are not allowed\n statement error\n copy empty_tbl to '__TEST_DIR__/empty.parquet' (WRITE_EMPTY_FILE false, PARTITION_BY (i))\n ----\n---\ntest/sql/copy/row_groups_per_file.test\n@@ -54,6 +54,13 @@ SELECT count(*) FROM glob('__TEST_DIR__/row_groups_per_file3/*.parquet')\n ----\n 3\n \n+# file rotation with return stats\n+query IIIIII\n+COPY bigdata TO '__TEST_DIR__/row_groups_per_file_stats/' (FORMAT PARQUET, WRITE_EMPTY_FILE false, FILENAME_PATTERN '{uuid}', ROW_GROUP_SIZE 3000, ROW_GROUPS_PER_FILE 2, RETURN_STATS);\n+----\n+<REGEX>:.*row_groups_per_file_stats.*[a-zA-Z0-9-]{36}.parquet\t<REGEX>:\\d+\t<REGEX>:\\d+\t<REGEX>:\\d+\t<REGEX>:{'\"col_a\"'={column_size_bytes=\\d+, max=\\d+, min=\\d+, null_count=0}, '\"col_b\"'={column_size_bytes=\\d+, max=\\d+, min=\\d+, null_count=0}}\tNULL\n+<REGEX>:.*row_groups_per_file_stats.*[a-zA-Z0-9-]{36}.parquet\t<REGEX>:\\d+\t<REGEX>:\\d+\t<REGEX>:\\d+\t<REGEX>:{'\"col_a\"'={column_size_bytes=\\d+, max=\\d+, min=\\d+, null_count=0}, '\"col_b\"'={column_size_bytes=\\d+, max=\\d+, min=\\d+, null_count=0}}\tNULL\n+\n # now we crank up the threads\n statement ok\n PRAGMA verify_parallelism\n---\n"}], "ideal": "Title: Support file rotation with WRITE_EMPTY_FILE false\n\nBody: This PR adds support for file rotation (setting `file_size_bytes` or `row_groups_per_file`) together with the `write_empty_file false` flag. "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/optimizer/pushdown/pushdown_aggregate.cpp\n@@ -50,8 +50,8 @@ unique_ptr<LogicalOperator> FilterPushdown::PushdownAggregate(unique_ptr<Logical\n \t\t}\n \t\t// no aggregate! we are filtering on a group\n \t\t// we can only push this down if the filter is in all grouping sets\n-\t\tif (aggr.grouping_sets.empty()) {\n-\t\t\t// empty grouping set - we cannot pushdown the filter\n+\t\tif (aggr.groups.empty()) {\n+\t\t\t// empty group - we cannot pushdown the filter\n \t\t\tcontinue;\n \t\t}\n \n---\ntest/sql/tpcds/tpcds_sf001.test_slow\n@@ -153,3 +153,7 @@ PRAGMA tpcds(${i})\n \n endloop\n \n+query II\n+explain select * from (pivot store_sales on ss_store_sk in (1,2,4,7,8,10) using sum(ss_ext_list_price) group by ss_item_sk) where ss_item_sk<100;\n+----\n+physical_plan\t<REGEX>:.*SEQ_SCAN.*Filters.*\n---\n"}], "ideal": "Title: Pushdown pivot filter\n\nBody: This pr try to implement Pivot filter pushdown mentioned in #17458.\r\n\r\nPivot convert to Aggregate and not set grouping_set, but this should be still safe to pushdown filter in such condition. Maybe entire code below could remove.\r\n```\r\nif (aggr.grouping_set.empty()) {\r\n\t// empty grouping set - we cannot pushdown the filter\r\n\tcontinue;\r\n}\r\n```"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/expression.cpp\n@@ -68,7 +68,7 @@ bool Expression::PropagatesNullValues() const {\n \tif (type == ExpressionType::OPERATOR_IS_NULL || type == ExpressionType::OPERATOR_IS_NOT_NULL ||\n \t    type == ExpressionType::COMPARE_NOT_DISTINCT_FROM || type == ExpressionType::COMPARE_DISTINCT_FROM ||\n \t    type == ExpressionType::CONJUNCTION_OR || type == ExpressionType::CONJUNCTION_AND ||\n-\t    type == ExpressionType::OPERATOR_COALESCE) {\n+\t    type == ExpressionType::OPERATOR_COALESCE || type == ExpressionType::CASE_EXPR) {\n \t\treturn false;\n \t}\n \tbool propagate_null_values = true;\n---\ntest/sql/subquery/scalar/test_correlated_subquery.test\n@@ -192,3 +192,10 @@ NULL\tNULL\n 2\t42\n 3\t42\n \n+query II\n+SELECT i, (SELECT CASE WHEN sum(i) > 1 THEN 0 ELSE 1 END FROM integers WHERE i=i1.i) FROM integers i1;\n+----\n+1\t1\n+2\t0\n+3\t0\n+NULL\t1\n---\n"}], "ideal": "Title: Fix propagatesNullValues for case expr\n\nBody: The following query returned unexpected results:\r\n```sql\r\nINSERT INTO integers VALUES (1), (2), (3), (NULL);\r\n\r\nSELECT i, (SELECT CASE WHEN sum(i) > 1 THEN 0 ELSE 1 END FROM integers WHERE i=i1.i) FROM integers i1;\r\n```\r\n```\r\nExpected result:\r\n----\r\n1       1\r\n2       0\r\n3       0\r\nNULL    1\r\n\r\nActual result:\r\n----\r\n1       1\r\n2       0\r\n3       0\r\nNULL    NULL\r\n```\r\n\r\nThis is because `PropagatesNullValues` does not strictly account for `case_expr`. A simple fix is to make it return `false` (Perhaps we can return more precise results based on the child)."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/include/writer/parquet_write_operators.hpp\n@@ -138,7 +138,9 @@ struct ParquetBaseStringOperator : public BaseParquetOperator {\n \n \ttemplate <class SRC, class TGT>\n \tstatic idx_t GetRowSize(const Vector &vector, idx_t index) {\n-\t\treturn FlatVector::GetData<string_t>(vector)[index].GetSize();\n+\t\t// This needs to add the 4 bytes (just like WriteSize) otherwise we underestimate and we have to realloc\n+\t\t// This seriously harms performance, mostly by making it very inconsistent (see internal issue #4990)\n+\t\treturn sizeof(uint32_t) + FlatVector::GetData<string_t>(vector)[index].GetSize();\n \t}\n };\n \n---\n"}], "ideal": "Title: Take string size into account in `GetRowSize` in `ParquetWriter`\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/4990"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types/row/tuple_data_allocator.cpp\n@@ -559,7 +559,6 @@ void TupleDataAllocator::FindHeapPointers(TupleDataChunkState &chunk_state, Sele\n \tD_ASSERT(!layout.AllConstant());\n \tconst auto row_locations = FlatVector::GetData<data_ptr_t>(chunk_state.row_locations);\n \tconst auto heap_locations = FlatVector::GetData<data_ptr_t>(chunk_state.heap_locations);\n-\tconst auto heap_sizes = FlatVector::GetData<idx_t>(chunk_state.heap_sizes);\n \n \tfor (const auto &col_idx : layout.GetVariableColumns()) {\n \t\tif (not_found_count == 0) {\n@@ -579,7 +578,7 @@ void TupleDataAllocator::FindHeapPointers(TupleDataChunkState &chunk_state, Sele\n \t\t\tfor (idx_t i = 0; i < not_found_count; i++) {\n \t\t\t\tconst auto idx = not_found.get_index(i);\n \t\t\t\tconst auto &row_location = row_locations[idx] + base_col_offset;\n-\t\t\t\tD_ASSERT(heap_sizes[idx] != 0);\n+\t\t\t\tD_ASSERT(FlatVector::GetData<idx_t>(chunk_state.heap_sizes)[idx] != 0);\n \n \t\t\t\t// We always serialize a NullValue<string_t>, which isn't inlined if this build flag is enabled\n \t\t\t\t// So we need to grab the pointer from here even if the string is NULL\n@@ -606,7 +605,7 @@ void TupleDataAllocator::FindHeapPointers(TupleDataChunkState &chunk_state, Sele\n \t\t\tfor (idx_t i = 0; i < not_found_count; i++) {\n \t\t\t\tconst auto idx = not_found.get_index(i);\n \t\t\t\tconst auto &row_location = row_locations[idx] + base_col_offset;\n-\t\t\t\tD_ASSERT(heap_sizes[idx] != 0);\n+\t\t\t\tD_ASSERT(FlatVector::GetData<idx_t>(chunk_state.heap_sizes)[idx] != 0);\n \n \t\t\t\tValidityBytes row_mask(row_location, layout.ColumnCount());\n \t\t\t\tif (row_mask.RowIsValid(row_mask.GetValidityEntryUnsafe(entry_idx), idx_in_entry)) {\n---\nsrc/execution/radix_partitioned_hashtable.cpp\n@@ -35,7 +35,9 @@ RadixPartitionedHashTable::RadixPartitionedHashTable(GroupingSet &grouping_set_p\n \tgroup_types_copy.emplace_back(LogicalType::HASH);\n \n \tauto layout = make_shared_ptr<TupleDataLayout>();\n-\tlayout->Initialize(std::move(group_types_copy), AggregateObject::CreateAggregateObjects(op.bindings));\n+\tauto aggregate_objects = AggregateObject::CreateAggregateObjects(op.bindings);\n+\tconst auto align = !aggregate_objects.empty();\n+\tlayout->Initialize(std::move(group_types_copy), std::move(aggregate_objects), align);\n \tlayout_ptr = std::move(layout);\n }\n \n@@ -330,6 +332,10 @@ idx_t RadixHTConfig::MaximumSinkRadixBits() const {\n }\n \n idx_t RadixHTConfig::SinkCapacity() const {\n+\tif (number_of_threads <= GROW_STRATEGY_THREAD_THRESHOLD) {\n+\t\t// Grow strategy, start off a bit bigger\n+\t\treturn 262144;\n+\t}\n \t// Start off tiny, we can adapt with DecideAdaptation later\n \treturn 32768;\n }\n---\n"}], "ideal": "Title: Ensure we use the same layout in `RadixPartitionedHashTable` and `GroupedAggregateHashTable`\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/5070\r\n\r\nI've also fixed a build warning, and slightly increased the size of the initial hash table if we have low thread counts."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/config/out_of_tree_extensions.cmake\n@@ -64,6 +64,13 @@ if (NOT MINGW AND NOT \"${OS_NAME}\" STREQUAL \"linux\" AND NOT ${WASM_ENABLED})\n endif()\n endif()\n \n+################# DUCKLAKE\n+duckdb_extension_load(ducklake\n+        LOAD_TESTS\n+        GIT_URL https://github.com/duckdb/ducklake\n+        GIT_TAG d4e144737bc3f88aef1d8768cb2ef162b0db9f09\n+        )\n+\n ################# EXCEL\n duckdb_extension_load(excel\n     LOAD_TESTS\n---\n"}], "ideal": "Title: add ducklake to out_of_tree_extensions.cmake\n\nBody: This PR adds `ducklake` extension bumped to the latest verified GIT_TAG to the `OOTE.cmake` file."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/column_reader.cpp\n@@ -713,6 +713,12 @@ void ColumnReader::ApplyPendingSkips(data_ptr_t define_out, data_ptr_t repeat_ou\n \n \twhile (to_skip > 0) {\n \t\tauto skip_now = ReadPageHeaders(to_skip);\n+\t\tif (page_is_filtered_out) {\n+\t\t\t// the page has been filtered out entirely - skip\n+\t\t\tpage_rows_available -= skip_now;\n+\t\t\tto_skip -= skip_now;\n+\t\t\tcontinue;\n+\t\t}\n \t\tconst auto all_valid = PrepareRead(skip_now, define_out, repeat_out, 0);\n \n \t\tconst auto define_ptr = all_valid ? nullptr : static_cast<uint8_t *>(define_out);\n---\n"}], "ideal": "Title: Check page filtered out flag before reading it\n\nBody: This PR fixes #17759. The original parquet file is too large, and I was unable to reproduce this issue when I reduced the file size, so I did not add the test file."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/Windows.yml\n@@ -265,6 +265,8 @@ jobs:\n            msystem: MINGW64\n            update: true\n            install: git mingw-w64-x86_64-toolchain mingw-w64-x86_64-cmake mingw-w64-x86_64-ninja\n+           cache: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n+\n        # see here: https://gist.github.com/scivision/1de4fd6abea9ba6b2d87dc1e86b5d2ce\n        - name: Put MSYS2_MinGW64 on PATH\n          # there is not yet an environment variable for this path from msys2/setup-msys2\n---\n"}], "ideal": "Title: [chore] Avoid caching msys artifacts on PRs\n\nBody: Current status of cache shows stuff is not really long lived, with a bunch of space (I have not measured, possibly 20%+) busy with repeated msys2 items, that are only cached on the same PR.\r\n\r\nMoved to regular behaviour that is caching only on main branch."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/BundleStaticLibs.yml\n@@ -189,8 +189,17 @@ jobs:\n           -e BUILD_BENCHMARK=1                                                   \\\n           -e FORCE_WARN_UNUSED=1                                                 \\\n           quay.io/pypa/manylinux_2_28_${{ matrix.config.image }}                  \\\n-          bash -c \"yum install -y perl-IPC-Cmd && git config --global --add safe.directory $PWD && make gather-libs -C $PWD\"\n-\n+          bash -c \"\n+            set -e\n+            yum install -y perl-IPC-Cmd gcc-toolset-12 gcc-toolset-12-gcc-c++\n+            \n+            source /opt/rh/gcc-toolset-12/enable\n+            export CC=gcc\n+            export CXX=g++\n+\n+            git config --global --add safe.directory $PWD\n+            make gather-libs -C $PWD\n+          \"\n       - name: Print platform\n         shell: bash\n         run: ./build/release/duckdb -c \"PRAGMA platform;\"\n---\n.github/workflows/LinuxRelease.yml\n@@ -84,7 +84,17 @@ jobs:\n         -e BUILD_BENCHMARK=1                                                   \\\n         -e FORCE_WARN_UNUSED=1                                                 \\\n         quay.io/pypa/manylinux_2_28_${{ matrix.config.image }}                  \\\n-        bash -c \"yum install -y perl-IPC-Cmd && git config --global --add safe.directory $PWD && make -C $PWD\"\n+        bash -c \"\n+          set -e\n+          yum install -y perl-IPC-Cmd gcc-toolset-12 gcc-toolset-12-gcc-c++\n+        \n+          source /opt/rh/gcc-toolset-12/enable\n+          export CC=gcc\n+          export CXX=g++\n+\n+          git config --global --add safe.directory $PWD\n+          make -C $PWD\n+        \"\n \n     - name: Print platform\n       shell: bash\n---\n"}], "ideal": "Title: Support glibc 2.28 environments\n\nBody: Ref: Issue #17632\r\n\r\nThis builds with GCC 12 explicitly instead of GCC 14. Building with GCC 14 creates binaries incompatible with mainstream Linux distributions using glibc 2.28.\r\n\r\ncc: @taniabogatsch "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/actions/build_extensions/action.yml\n@@ -60,6 +60,9 @@ inputs:\n   build_out_of_tree_extensions:\n     description: 'Build out-of-tree extensions'\n     default: 1\n+  build_complete_extensions_set:\n+    description: 'Whether all extensions needs to be built'\n+    default: 1\n   bundle_static_lib_mode:\n     description: 'Build the default bundled extensions to publish the static libs'\n     default: 0\n@@ -111,6 +114,7 @@ runs:\n       run: |\n         echo \"VCPKG_TOOLCHAIN_PATH=$VCPKG_ROOT/scripts/buildsystems/vcpkg.cmake\" >> $GITHUB_ENV\n         echo \"VCPKG_TARGET_TRIPLET=${{ inputs.vcpkg_target_triplet }}\" >> $GITHUB_ENV\n+        echo \"BUILD_COMPLETE_EXTENSION_SET=${{ inputs.build_complete_extensions_set }}\" >> docker_env.txt\n \n     - name: workaround for https://github.com/duckdb/duckdb/issues/8360\n       if: inputs.vcpkg_target_triplet == 'x64-windows-static-md'\n---\n.github/actions/build_extensions_dockerized/action.yml\n@@ -14,6 +14,9 @@ inputs:\n   override_git_describe:\n     description: 'Override git describe'\n     default: ''\n+  build_complete_extensions_set:\n+    description: 'Whether all extensions needs to be built'\n+    default: 1\n \n runs:\n   using: \"composite\"\n@@ -62,6 +65,7 @@ runs:\n           echo \"DUCKDB_PLATFORM=${{ inputs.duckdb_arch }}\" >> docker_env.txt\n           echo \"OVERRIDE_GIT_DESCRIBE=${{ inputs.override_git_describe }}\" >> docker_env.txt\n           echo \"LINUX_CI_IN_DOCKER=1\" >> docker_env.txt\n+          echo \"BUILD_COMPLETE_EXTENSION_SET=${{ inputs.build_complete_extensions_set }}\" >> docker_env.txt\n \n       - name: Generate timestamp for Ccache entry\n         shell: bash\n---\n.github/config/out_of_tree_extensions.cmake\n@@ -109,7 +109,7 @@ if (NOT MINGW AND NOT ${WASM_ENABLED})\n endif()\n \n # mingw CI with all extensions at once is somehow not happy\n-if (NOT MINGW)\n+if (NOT MINGW AND NOT ${BUILD_COMPLETE_EXTENSION_SET})\n ################# SPATIAL\n duckdb_extension_load(spatial\n     DONT_LINK LOAD_TESTS\n@@ -169,7 +169,7 @@ duckdb_extension_load(fts\n )\n \n ################# ENCODINGS\n-if (NOT ${WASM_ENABLED})\n+if (NOT ${WASM_ENABLED} AND NOT ${BUILD_COMPLETE_EXTENSION_SET})\n duckdb_extension_load(encodings\n         LOAD_TESTS\n         DONT_LINK\n---\n.github/workflows/LinuxRelease.yml\n@@ -150,6 +150,8 @@ jobs:\n           duckdb_arch: ${{ matrix.duckdb_arch }}\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n           override_git_describe: ${{ inputs.override_git_describe }}\n+          build_complete_extensions_set: 1\n+          # Note that build_complete_extensions_set needs to be 1 (that is also the default value) for 'Checks extension entries' to work correctly\n \n       - uses: actions/upload-artifact@v4\n         with:\n@@ -193,6 +195,7 @@ jobs:\n           duckdb_arch: ${{ matrix.duckdb_arch }}\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n           override_git_describe: ${{ inputs.override_git_describe }}\n+          build_complete_extensions_set: ${{ github.event_name != 'pull_request' && 1 || 0 }}\n \n       - uses: actions/upload-artifact@v4\n         with:\n@@ -235,6 +238,7 @@ jobs:\n           duckdb_arch: ${{ matrix.duckdb_arch }}\n           run_tests: ${{ inputs.skip_tests != 'true' }}\n           override_git_describe: ${{ inputs.override_git_describe }}\n+          build_complete_extensions_set: ${{ github.event_name != 'pull_request' && 1 || 0 }}\n \n       - uses: actions/upload-artifact@v4\n         with:\n---\n.github/workflows/Windows.yml\n@@ -325,6 +325,7 @@ jobs:\n          run_tests: ${{ inputs.skip_tests != 'true' && 1 || 0 }}\n          run_autoload_tests: ${{ inputs.skip_tests != 'true' && 1 || 0 }}\n          unittest_script: python3 scripts/run_tests_one_by_one.py ./build/release/test/Release/unittest.exe\n+         build_complete_extensions_set: ${{ github.event_name != 'pull_request' && 1 || 0 }}\n \n      - uses: actions/upload-artifact@v4\n        with:\n---\nCMakeLists.txt\n@@ -104,6 +104,11 @@ if (DUCKDB_EXPLICIT_PLATFORM)\n     add_definitions(-DDUCKDB_CUSTOM_PLATFORM=${DUCKDB_EXPLICIT_PLATFORM})\n endif()\n \n+option (BUILD_COMPLETE_EXTENSION_SET \"Whether we need to actually build the complete set\" TRUE)\n+if (DEFINED ENV{BUILD_COMPLETE_EXTENSION_SET})\n+     set(BUILD_COMPLETE_EXTENSION_SET \"$ENV{BUILD_COMPLETE_EXTENSION_SET}\")\n+endif()\n+\n option (WASM_ENABLED \"Are DuckDB-Wasm extensions build enabled\" FALSE)\n if (DEFINED ENV{WASM_EXTENSIONS})\n      set(WASM_ENABLED \"$ENV{WASM_EXTENSIONS}\")\n---\n"}], "ideal": "Title: Skip encodings and spatial extensions on PRs\n\nBody: Add infra so that a subset of extensions can be tested on each PR, that should speed up CI times with limited risks.\r\n\r\nCurrently skips `encodings` and `spatial` on PRs. Which extensions is up for discussions, I would be for expanding even more, given most extensions are not actually impacting CI surface.\r\n\r\nI would like to see whether this actually works (minor implementations details migth be off), then we can discuss.\r\nThis could also be paired with other ideas to improve PR process, like having a tag \"extra tests\" that opt-in to the full set of tests, but this is worth having also in isolation I think."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/planner/expression/bound_between_expression.hpp\n@@ -37,10 +37,10 @@ class BoundBetweenExpression : public Expression {\n \tstatic unique_ptr<Expression> Deserialize(Deserializer &deserializer);\n \n public:\n-\tExpressionType LowerComparisonType() {\n+\tExpressionType LowerComparisonType() const {\n \t\treturn lower_inclusive ? ExpressionType::COMPARE_GREATERTHANOREQUALTO : ExpressionType::COMPARE_GREATERTHAN;\n \t}\n-\tExpressionType UpperComparisonType() {\n+\tExpressionType UpperComparisonType() const {\n \t\treturn upper_inclusive ? ExpressionType::COMPARE_LESSTHANOREQUALTO : ExpressionType::COMPARE_LESSTHAN;\n \t}\n \n---\n"}], "ideal": "Title: Mark Upper/LowerComparisonType as const\n\nBody: I wanted to use these methods on a `const` reference to a\n`BoundBetweenExpression` in pg_duckdb, but was unable to.\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/join_hashtable.cpp\n@@ -258,7 +258,6 @@ static void GetRowPointersInternal(DataChunk &keys, TupleDataChunkState &key_sta\n \n \t// densify hashes: If there is no sel, flatten the hashes, else densify via UnifiedVectorFormat\n \tif (has_row_sel) {\n-\n \t\thashes_v.ToUnifiedFormat(count, hashes_unified_v);\n \t\tuses_unified = true;\n \n@@ -271,8 +270,7 @@ static void GetRowPointersInternal(DataChunk &keys, TupleDataChunkState &key_sta\n \t\t\thashes_dense[i] = hashes_unified[uvf_index];\n \t\t}\n \t} else {\n-\t\thashes_v.Flatten(count);\n-\t\tstate.hashes_dense_v.Reference(hashes_v);\n+\t\tVectorOperations::Copy(hashes_v, state.hashes_dense_v, count, 0, 0);\n \t}\n \n \t// the number of keys that match for all iterations of the following loop\n@@ -282,7 +280,6 @@ static void GetRowPointersInternal(DataChunk &keys, TupleDataChunkState &key_sta\n \tidx_t elements_to_probe_count = count;\n \n \tdo {\n-\n \t\tconst idx_t keys_to_compare_count = ProbeForPointers<USE_SALTS>(state, ht, entries, hashes_v, pointers_result_v,\n \t\t                                                                row_sel, elements_to_probe_count, has_row_sel);\n \n---\n"}], "ideal": "Title: Don't Flatten() then Reference()\n\nBody: Fixing segfault https://github.com/duckdblabs/duckdb-internal/issues/5052\r\n \r\nThanks @lnkuiper for the fix! \r\nCC @gropaul\r\nCC @ywelsch "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/shell/shell.cpp\n@@ -5049,11 +5049,14 @@ int SQLITE_CDECL wmain(int argc, wchar_t **wargv) {\n \t\t\tif (i == argc - 1) {\n \t\t\t\tbreak;\n \t\t\t}\n+\t\t\tauto old_bail = bail_on_error;\n+\t\t\tbail_on_error = true;\n \t\t\tz = cmdline_option_value(argc, argv, ++i);\n \t\t\tif (!data.ProcessFile(string(z))) {\n \t\t\t\tfree(azCmd);\n \t\t\t\treturn 1;\n \t\t\t}\n+\t\t\tbail_on_error = old_bail;\n \t\t} else if (strcmp(z, \"-cmd\") == 0 || strcmp(z, \"-c\") == 0 || strcmp(z, \"-s\") == 0) {\n \t\t\tif (strcmp(z, \"-c\") == 0 || strcmp(z, \"-s\") == 0) {\n \t\t\t\treadStdin = false;\n---\n"}], "ideal": "Title: CLI: make -f always bail on error\n\nBody: When running `duckdb -f [file]` - if an error is encountered we stop instead of continuing to execute the remainder of the file."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/actions/build_extensions/action.yml\n@@ -103,7 +103,7 @@ runs:\n       if: inputs.vcpkg_build == 1\n       uses: lukka/run-vcpkg@v11.1\n       with:\n-        vcpkgGitCommitId: 5e5d0e1cd7785623065e77eff011afdeec1a3574\n+        vcpkgGitCommitId: ce613c41372b23b1f51333815feb3edd87ef8a8b\n \n     - name: Set vcpkg env variables\n       if: inputs.vcpkg_build == 1\n---\n.github/actions/build_extensions_dockerized/action.yml\n@@ -44,7 +44,7 @@ runs:\n         run: |\n           docker build \\\n             --build-arg 'vcpkg_url=https://github.com/microsoft/vcpkg.git' \\\n-            --build-arg 'vcpkg_commit=5e5d0e1cd7785623065e77eff011afdeec1a3574' \\\n+            --build-arg 'vcpkg_commit=ce613c41372b23b1f51333815feb3edd87ef8a8b' \\\n             --build-arg 'extra_toolchains=;python3;' \\\n             -t duckdb/${{ inputs.duckdb_arch }} \\\n             ./extension-ci-tools/docker/${{ inputs.duckdb_arch }}\n@@ -62,10 +62,6 @@ runs:\n           echo \"DUCKDB_PLATFORM=${{ inputs.duckdb_arch }}\" >> docker_env.txt\n           echo \"OVERRIDE_GIT_DESCRIBE=${{ inputs.override_git_describe }}\" >> docker_env.txt\n           echo \"LINUX_CI_IN_DOCKER=1\" >> docker_env.txt\n-          # TODO: no more cross compilation for extension builds\n-          echo \"TOOLCHAIN_FLAGS=${{ inputs.duckdb_arch == 'linux_arm64' && '-DCMAKE_C_COMPILER=aarch64-linux-gnu-gcc -DCMAKE_CXX_COMPILER=aarch64-linux-gnu-g++ -DCMAKE_Fortran_COMPILER=aarch64-linux-gnu-gfortran' || '' }}\" >> docker_env.txt\n-          echo \"CC=${{ inputs.duckdb_arch == 'linux_arm64' && 'aarch64-linux-gnu-gcc' || '' }}\" >> docker_env.txt\n-          echo \"CXX=${{ inputs.duckdb_arch == 'linux_arm64' && 'aarch64-linux-gnu-g++' || '' }}\" >> docker_env.txt \n \n       - name: Generate timestamp for Ccache entry\n         shell: bash\n@@ -98,6 +94,6 @@ runs:\n \n       - name: Test extension (inside docker)\n         shell: bash\n-        if: ${{ inputs.run_tests == 1 && inputs.duckdb_arch != 'linux_arm64'}}\n+        if: ${{ inputs.run_tests == 1 }}\n         run: |\n           docker run --env-file=docker_env.txt -v `pwd`:/duckdb_build_dir -v `pwd`/ccache_dir:/ccache_dir duckdb/${{ inputs.duckdb_arch }} make test_release\n---\n.github/workflows/LinuxRelease.yml\n@@ -128,8 +128,6 @@ jobs:\n         build/release/benchmark/benchmark_runner benchmark/micro/update/update_with_join.benchmark\n         build/release/duckdb -c \"COPY (SELECT 42) TO '/dev/stdout' (FORMAT PARQUET)\" | cat\n \n- # Linux extensions for builds that use C++11 ABI, currently these are all linux builds based on ubuntu >= 18 (e.g. NodeJS)\n- # note that the linux-release-64 is based on the manylinux-based extensions, which are built in .github/workflows/Python.yml\n  linux-extensions-64:\n     # Builds extensions for linux_amd64\n     name: Linux Extensions (x64)\n@@ -218,7 +216,7 @@ jobs:\n  linux-extensions-64-aarch64:\n     # Builds extensions for linux_arm64\n     name: Linux Extensions (aarch64)\n-    runs-on: ubuntu-latest\n+    runs-on: ubuntu-24.04-arm\n     needs: linux-extensions-64\n     strategy:\n       matrix:\n---\n.github/workflows/Wasm.yml\n@@ -63,7 +63,7 @@ jobs:\n       - name: Setup vcpkg\n         uses: lukka/run-vcpkg@v11.1\n         with:\n-          vcpkgGitCommitId: '5e5d0e1cd7785623065e77eff011afdeec1a3574'\n+          vcpkgGitCommitId: 'ce613c41372b23b1f51333815feb3edd87ef8a8b'\n           vcpkgGitURL: 'https://github.com/microsoft/vcpkg.git'\n \n       - name: Setup Ccache\n---\nscripts/merge_vcpkg_deps.py\n@@ -58,7 +58,7 @@ def prefix_overlay_port(overlay_port):\n \n data = {\n     \"description\": f\"Auto-generated vcpkg.json for combined DuckDB extension build, generated by 'scripts/merge_vcpkg_deps.py'\",\n-    \"builtin-baseline\": \"5e5d0e1cd7785623065e77eff011afdeec1a3574\",\n+    \"builtin-baseline\": \"ce613c41372b23b1f51333815feb3edd87ef8a8b\",\n     \"dependencies\": final_deduplicated_deps,\n     \"overrides\": [{\"name\": \"openssl\", \"version\": \"3.0.8\"}],\n }\n---\n"}], "ideal": "Title: Fix linux extension ci\n\nBody: Apply changes from https://github.com/duckdb/duckdb/pull/17720 to `v1.3.0-ossivalis` as well."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/aggregate/holistic/quantile.cpp\n@@ -6,7 +6,6 @@\n #include \"duckdb/common/operator/abs.hpp\"\n #include \"core_functions/aggregate/quantile_state.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\n-#include \"duckdb/common/queue.hpp\"\n #include \"duckdb/common/serializer/serializer.hpp\"\n #include \"duckdb/common/serializer/deserializer.hpp\"\n #include \"duckdb/function/aggregate/sort_key_helpers.hpp\"\n@@ -588,7 +587,7 @@ AggregateFunction GetContinuousQuantileList(const LogicalType &type) {\n //===--------------------------------------------------------------------===//\n // Quantile binding\n //===--------------------------------------------------------------------===//\n-static const Value &CheckQuantile(const Value &quantile_val) {\n+static Value CheckQuantile(const Value &quantile_val) {\n \tif (quantile_val.IsNull()) {\n \t\tthrow BinderException(\"QUANTILE parameter cannot be NULL\");\n \t}\n@@ -820,7 +819,7 @@ struct ContinuousQuantileListFunction {\n };\n \n template <class OP>\n-AggregateFunction EmptyQuantileFunction(LogicalType input, LogicalType result, const LogicalType &extra_arg) {\n+AggregateFunction EmptyQuantileFunction(LogicalType input, const LogicalType &result, const LogicalType &extra_arg) {\n \tAggregateFunction fun({std::move(input)}, std::move(result), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr,\n \t                      OP::Bind);\n \tif (extra_arg.id() != LogicalTypeId::INVALID) {\n---\nextension/core_functions/include/core_functions/aggregate/quantile_sort_tree.hpp\n@@ -144,7 +144,7 @@ struct QuantileCompare {\n \t\tconst auto lval = accessor_l(lhs);\n \t\tconst auto rval = accessor_r(rhs);\n \n-\t\treturn desc ? (rval < lval) : (lval < rval);\n+\t\treturn desc ? LessThan::Operation(rval, lval) : LessThan::Operation(lval, rval);\n \t}\n };\n \n---\ntest/sql/aggregate/aggregates/test_quantile_disc.test\n@@ -268,3 +268,25 @@ query I\n SELECT quantile_disc(r, 0.1) FROM quantile WHERE 1=0\n ----\n NULL\n+\n+# NaNs\n+foreach fp float double\n+\n+loop i 0 10\n+\n+query I\n+with a as (\n+\tselect 'NaN'::${fp} as num \n+\tunion all \n+\tselect num::${fp} as num from generate_series(1,99) as tbl(num)\n+\tunion all \n+\tselect 'NaN'::${fp} as num\n+) \n+select quantile_disc(num, 0.9) c1 \n+from a;\n+----\n+91.0\n+\n+endloop\n+\n+endloop\n---\n"}], "ideal": "Title: Issue #17725: Quantile NaN Compare\n\nBody: * Use LessThan operator instead of naked C++ compares\r\n\r\nfixes: duckdb#17725\r\nfixes: duckdblabs/duckdb-internal#5036"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/tests/fast/arrow/test_arrow_offsets.py\n@@ -75,13 +75,13 @@ def expected_result(col1_null, col2_null, expected):\n     return [(col1, col2)]\n \n \n-test_nulls = lambda: mark.parametrize(\n+null_test_parameters = lambda: mark.parametrize(\n     ['col1_null', 'col2_null'], [(False, True), (True, False), (True, True), (False, False)]\n )\n \n \n class TestArrowOffsets(object):\n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_strings(self, duckdb_cursor, col1_null, col2_null):\n         col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n         if col1_null:\n@@ -105,7 +105,7 @@ def test_struct_of_strings(self, duckdb_cursor, col1_null, col2_null):\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, '131072')\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_bools(self, duckdb_cursor, col1_null, col2_null):\n         tuples = [False for i in range(0, MAGIC_ARRAY_SIZE)]\n         tuples[-1] = True\n@@ -138,7 +138,7 @@ def test_struct_of_bools(self, duckdb_cursor, col1_null, col2_null):\n             (pa_date64(), datetime.date(1970, 1, 1)),\n         ],\n     )\n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_dates(self, duckdb_cursor, constructor, expected, col1_null, col2_null):\n         tuples = [i for i in range(0, MAGIC_ARRAY_SIZE)]\n \n@@ -163,7 +163,7 @@ def test_struct_of_dates(self, duckdb_cursor, constructor, expected, col1_null,\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, expected)\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_enum(self, duckdb_cursor, col1_null, col2_null):\n         enum_type = pa.dictionary(pa.int64(), pa.utf8())\n \n@@ -190,7 +190,7 @@ def test_struct_of_enum(self, duckdb_cursor, col1_null, col2_null):\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, 'green')\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_blobs(self, duckdb_cursor, col1_null, col2_null):\n         col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n         if col1_null:\n@@ -214,7 +214,7 @@ def test_struct_of_blobs(self, duckdb_cursor, col1_null, col2_null):\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, b'131072')\n \n-    @test_nulls()\n+    @null_test_parameters()\n     @pytest.mark.parametrize(\n         [\"constructor\", \"unit\", \"expected\"],\n         [\n@@ -252,7 +252,7 @@ def test_struct_of_time(self, duckdb_cursor, constructor, unit, expected, col1_n\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, expected)\n \n-    @test_nulls()\n+    @null_test_parameters()\n     # NOTE: there is sadly no way to create a 'interval[months]' (tiM) type from pyarrow\n     @pytest.mark.parametrize(\n         [\"constructor\", \"expected\", \"converter\"],\n@@ -287,7 +287,7 @@ def test_struct_of_interval(self, duckdb_cursor, constructor, expected, converte\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, expected)\n \n-    @test_nulls()\n+    @null_test_parameters()\n     @pytest.mark.parametrize(\n         [\"constructor\", \"unit\", \"expected\"],\n         [\n@@ -322,7 +322,7 @@ def test_struct_of_duration(self, duckdb_cursor, constructor, unit, expected, co\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, expected)\n \n-    @test_nulls()\n+    @null_test_parameters()\n     @pytest.mark.parametrize(\n         [\"constructor\", \"unit\", \"expected\"],\n         [\n@@ -360,7 +360,7 @@ def test_struct_of_timestamp_tz(self, duckdb_cursor, constructor, unit, expected\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, expected)\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_large_blobs(self, duckdb_cursor, col1_null, col2_null):\n         col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n         if col1_null:\n@@ -384,7 +384,7 @@ def test_struct_of_large_blobs(self, duckdb_cursor, col1_null, col2_null):\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, b'131072')\n \n-    @test_nulls()\n+    @null_test_parameters()\n     @pytest.mark.parametrize(\n         [\"precision_scale\", \"expected\"],\n         [\n@@ -425,7 +425,7 @@ def test_struct_of_decimal(self, duckdb_cursor, precision_scale, expected, col1_\n         ).fetchall()\n         assert res == expected_result(col1_null, col2_null, expected)\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_small_list(self, duckdb_cursor, col1_null, col2_null):\n         col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n         if col1_null:\n@@ -455,7 +455,7 @@ def test_struct_of_small_list(self, duckdb_cursor, col1_null, col2_null):\n             res2 = ['131072', '131072', '131072']\n         assert res == [(res1, res2)]\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_fixed_size_list(self, duckdb_cursor, col1_null, col2_null):\n         col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n         if col1_null:\n@@ -485,7 +485,7 @@ def test_struct_of_fixed_size_list(self, duckdb_cursor, col1_null, col2_null):\n             res2 = ('131072', '131072', '131072')\n         assert res == [(res1, res2)]\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_fixed_size_blob(self, duckdb_cursor, col1_null, col2_null):\n         col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n         if col1_null:\n@@ -516,7 +516,7 @@ def test_struct_of_fixed_size_blob(self, duckdb_cursor, col1_null, col2_null):\n             res2 = (b'131072', b'131073', b'131074')\n         assert res == [(res1, res2)]\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_list_of_blobs(self, duckdb_cursor, col1_null, col2_null):\n         col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n         if col1_null:\n@@ -547,7 +547,7 @@ def test_struct_of_list_of_blobs(self, duckdb_cursor, col1_null, col2_null):\n             res2 = [b'131072', b'131073', b'131074']\n         assert res == [(res1, res2)]\n \n-    @test_nulls()\n+    @null_test_parameters()\n     def test_struct_of_list_of_list(self, duckdb_cursor, col1_null, col2_null):\n         col1 = [i for i in range(0, MAGIC_ARRAY_SIZE)]\n         if col1_null:\n---\n"}], "ideal": "Title: Rename decorator from test_nulls to null_test_parameters\n\nBody: Hopefully fixes Python CI"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/binder/expression/bind_operator_expression.cpp\n@@ -77,8 +77,9 @@ LogicalType ExpressionBinder::ResolveOperatorType(OperatorExpression &op, vector\n \tcase ExpressionType::OPERATOR_COALESCE: {\n \t\treturn ResolveCoalesceType(op, children);\n \t}\n-\tcase ExpressionType::OPERATOR_TRY:\n-\t\treturn ExpressionBinder::GetExpressionReturnType(*children[0]);\n+\tcase ExpressionType::OPERATOR_TRY: {\n+\t\treturn children[0]->return_type;\n+\t}\n \tcase ExpressionType::OPERATOR_NOT:\n \t\treturn ResolveNotType(op, children);\n \tdefault:\n---\ntest/sql/error/test_try_expression.test\n@@ -112,3 +112,11 @@ statement error\n select try(CAST((select 'ABC') as INTEGER))\n ----\n TRY can not be used in combination with a scalar subquery\n+\n+# The values below are literals, which do not have a PhysicalType, caused a crash before fixing duckdb-internal/issues/5047\n+\n+statement ok\n+select TRY('hello');\n+\n+statement ok\n+select TRY(123);\n---\n"}], "ideal": "Title: [Dev] Fix `TRY` expression crash on literals\n\nBody: This PR fixes https://github.com/duckdblabs/duckdb-internal/issues/5047"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/include/parquet_file_metadata_cache.hpp\n@@ -20,7 +20,7 @@ enum class ParquetCacheValidity { VALID, INVALID, UNKNOWN };\n class ParquetFileMetadataCache : public ObjectCacheEntry {\n public:\n \tParquetFileMetadataCache(unique_ptr<duckdb_parquet::FileMetaData> file_metadata, CachingFileHandle &handle,\n-\t                         unique_ptr<GeoParquetFileMetadata> geo_metadata);\n+\t                         unique_ptr<GeoParquetFileMetadata> geo_metadata, idx_t footer_size);\n \t~ParquetFileMetadataCache() override = default;\n \n \t//! Parquet file metadata\n@@ -29,6 +29,9 @@ class ParquetFileMetadataCache : public ObjectCacheEntry {\n \t//! GeoParquet metadata\n \tunique_ptr<GeoParquetFileMetadata> geo_metadata;\n \n+\t//! Parquet footer size\n+\tidx_t footer_size;\n+\n public:\n \tstatic string ObjectType();\n \tstring GetObjectType() override;\n---\nextension/parquet/parquet_file_metadata_cache.cpp\n@@ -6,9 +6,10 @@ namespace duckdb {\n \n ParquetFileMetadataCache::ParquetFileMetadataCache(unique_ptr<duckdb_parquet::FileMetaData> file_metadata,\n                                                    CachingFileHandle &handle,\n-                                                   unique_ptr<GeoParquetFileMetadata> geo_metadata)\n+                                                   unique_ptr<GeoParquetFileMetadata> geo_metadata,\n+                                                   idx_t footer_size)\n     : metadata(std::move(file_metadata)), geo_metadata(std::move(geo_metadata)), validate(handle.Validate()),\n-      last_modified(handle.GetLastModifiedTime()), version_tag(handle.GetVersionTag()) {\n+      last_modified(handle.GetLastModifiedTime()), version_tag(handle.GetVersionTag()), footer_size(footer_size) {\n }\n \n string ParquetFileMetadataCache::ObjectType() {\n---\nextension/parquet/parquet_metadata.cpp\n@@ -582,6 +582,12 @@ void ParquetMetaDataOperatorData::BindFileMetaData(vector<LogicalType> &return_t\n \n \tnames.emplace_back(\"footer_signing_key_metadata\");\n \treturn_types.emplace_back(LogicalType::VARCHAR);\n+\n+\tnames.emplace_back(\"file_size_bytes\");\n+\treturn_types.emplace_back(LogicalType::UBIGINT);\n+\n+\tnames.emplace_back(\"footer_size\");\n+\treturn_types.emplace_back(LogicalType::UBIGINT);\n }\n \n void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const vector<LogicalType> &return_types,\n@@ -610,6 +616,11 @@ void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const\n \tcurrent_chunk.SetValue(6, 0,\n \t                       ParquetElementStringVal(meta_data->footer_signing_key_metadata,\n \t                                               meta_data->__isset.footer_signing_key_metadata));\n+\t//  file_size_bytes\n+\tcurrent_chunk.SetValue(7, 0, Value::UBIGINT(reader->GetHandle().GetFileSize()));\n+\t//  footer_size\n+\tcurrent_chunk.SetValue(8, 0, Value::UBIGINT(reader->metadata->footer_size));\n+\n \tcurrent_chunk.SetCardinality(1);\n \tcollection.Append(current_chunk);\n \tcollection.InitializeScan(scan_state);\n---\nextension/parquet/parquet_reader.cpp\n@@ -103,6 +103,7 @@ LoadMetadata(ClientContext &context, Allocator &allocator, CachingFileHandle &fi\n \t}\n \n \tbool footer_encrypted;\n+\tuint32_t footer_len;\n \t// footer size is not provided - read it from the back\n \tif (!footer_size.IsValid()) {\n \t\t// We have to do two reads here:\n@@ -128,7 +129,6 @@ LoadMetadata(ClientContext &context, Allocator &allocator, CachingFileHandle &fi\n \t\ttransport.SetLocation(file_size - 8);\n \t\ttransport.read(buf.ptr, 8);\n \n-\t\tuint32_t footer_len;\n \t\tParseParquetFooter(buf.ptr, file_handle.GetPath(), file_size, encryption_config, footer_len, footer_encrypted);\n \n \t\tauto metadata_pos = file_size - (footer_len + 8);\n@@ -137,7 +137,7 @@ LoadMetadata(ClientContext &context, Allocator &allocator, CachingFileHandle &fi\n \t\t\ttransport.Prefetch(metadata_pos, footer_len);\n \t\t}\n \t} else {\n-\t\tauto footer_len = UnsafeNumericCast<uint32_t>(footer_size.GetIndex());\n+\t\tfooter_len = UnsafeNumericCast<uint32_t>(footer_size.GetIndex());\n \t\tif (footer_len == 0 || file_size < 12 + footer_len) {\n \t\t\tthrow InvalidInputException(\"Invalid footer length provided for file '%s'\", file_handle.GetPath());\n \t\t}\n@@ -173,7 +173,7 @@ LoadMetadata(ClientContext &context, Allocator &allocator, CachingFileHandle &fi\n \n \t// Try to read the GeoParquet metadata (if present)\n \tauto geo_metadata = GeoParquetFileMetadata::TryRead(*metadata, context);\n-\treturn make_shared_ptr<ParquetFileMetadataCache>(std::move(metadata), file_handle, std::move(geo_metadata));\n+\treturn make_shared_ptr<ParquetFileMetadataCache>(std::move(metadata), file_handle, std::move(geo_metadata), footer_len);\n }\n \n LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, ParquetColumnSchema &schema) const {\n---\ntest/sql/copy/parquet/file_metadata.test\n@@ -3,7 +3,15 @@\n \n require parquet\n \n-query IIIIIII\n+statement ok\n+SET parquet_metadata_cache = true;\n+\n+query IIIIIIIII\n+SELECT * FROM parquet_file_metadata('data/parquet-testing/arrow/alltypes_dictionary.parquet')\n+----\n+data/parquet-testing/arrow/alltypes_dictionary.parquet\timpala version 1.3.0-INTERNAL (build 8a48ddb1eff84592b3fc06bc6f51ec120e1fffc9)\t2\t1\t1\tNULL\tNULL\t1698\t723\n+\n+query IIIIIIIII\n SELECT * FROM parquet_file_metadata('data/parquet-testing/arrow/alltypes_dictionary.parquet')\n ----\n-data/parquet-testing/arrow/alltypes_dictionary.parquet\timpala version 1.3.0-INTERNAL (build 8a48ddb1eff84592b3fc06bc6f51ec120e1fffc9)\t2\t1\t1\tNULL\tNULL\n+data/parquet-testing/arrow/alltypes_dictionary.parquet\timpala version 1.3.0-INTERNAL (build 8a48ddb1eff84592b3fc06bc6f51ec120e1fffc9)\t2\t1\t1\tNULL\tNULL\t1698\t723\n---\n"}], "ideal": "Title: Expose file_size_bytes and footer_size in parquet_file_metadata\n\nBody: Hi, like many others I have been playing around with DuckLake in the last couple of days, and I was trying to figure out if I could create the required tables from a set of Parquet files I already had laying around. I found that it was tricky to create the `ducklake_data_file` table because (as far as I'm aware) there is no easy way to get the total size and footer size of Parquet files at the moment. I've tried to implement that functionality in this PR, by adding `file_size_bytes` and `footer_size` columns to the `parquet_file_metadata` function.\r\n\r\n```\r\nD SELECT file_name, file_size_bytes, footer_size FROM parquet_file_metadata('data/parquet-testing/arrow/alltypes_dictionary.parquet');\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                       file_name                        \u2502 file_size_bytes \u2502 footer_size \u2502\r\n\u2502                        varchar                         \u2502     uint64      \u2502   uint32    \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 data/parquet-testing/arrow/alltypes_dictionary.parquet \u2502      1698       \u2502     723     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nI'm not fully sure whether exposing this information through `OpenFileInfo` is the right way to go, so I'm happy to hear any feedback!"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/include/parquet_dbp_encoder.hpp\n@@ -155,8 +155,8 @@ class DbpEncoder {\n \t\t\tint64_t verification_data[NUMBER_OF_VALUES_IN_A_MINIBLOCK];\n \t\t\tByteBuffer byte_buffer(data_ptr_cast(data_packed), write_size);\n \t\t\tbitpacking_width_t bitpack_pos = 0;\n-\t\t\tParquetDecodeUtils::BitUnpack(byte_buffer, bitpack_pos, verification_data, NUMBER_OF_VALUES_IN_A_MINIBLOCK,\n-\t\t\t                              width);\n+\t\t\tParquetDecodeUtils::BitUnpack(byte_buffer, bitpack_pos, reinterpret_cast<uint64_t *>(verification_data),\n+\t\t\t                              NUMBER_OF_VALUES_IN_A_MINIBLOCK, width);\n \t\t\tfor (idx_t i = 0; i < NUMBER_OF_VALUES_IN_A_MINIBLOCK; i++) {\n \t\t\t\tD_ASSERT(src[i] == verification_data[i]);\n \t\t\t}\n---\ntest/parquet/bss_roundtrip.test_slow\n@@ -23,7 +23,7 @@ from doubles\n \n \n statement ok\n-copy doubles to '__TEST_DIR__/bss.parquet';\n+copy doubles to '__TEST_DIR__/bss.parquet' (PARQUET_VERSION V2);\n \n query IIII nosort q0\n from '__TEST_DIR__/bss.parquet';\n@@ -44,7 +44,7 @@ from floats\n \n \n statement ok\n-copy floats to '__TEST_DIR__/bss.parquet';\n+copy floats to '__TEST_DIR__/bss.parquet' (PARQUET_VERSION V2);\n \n query IIII nosort q1\n from '__TEST_DIR__/bss.parquet';\n---\ntest/parquet/test_internal_5021.test\n@@ -0,0 +1,11 @@\n+# name: test/parquet/test_internal_5021.test\n+# description: Internal issue 5021: Assertion failure in DbpEncoder when writing Parquet V2\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+CREATE TABLE tbl AS SELECT 'hello world' || i str FROM range(11) t(i);\n+\n+statement ok\n+COPY tbl TO '__TEST_DIR__/file.parquet' (PARQUET_VERSION 'V2', DICTIONARY_SIZE_LIMIT 1);\n---\n"}], "ideal": "Title: Fix wrong assertion in Parquet DBP encoder\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/5021\r\n\r\nWe accidentally used an `int64_t` instead of a `uint64_t` during verification, so while the behavior was correct, the verification ran into an issue."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/BundleStaticLibs.yml\n@@ -105,7 +105,7 @@ jobs:\n \n   bundle-mingw-static-lib:\n     name: Windows MingW static libs\n-    runs-on: windows-2019\n+    runs-on: windows-latest\n     env:\n       ENABLE_EXTENSION_AUTOLOADING: 1\n       ENABLE_EXTENSION_AUTOINSTALL: 1\n---\n.github/workflows/Python.yml\n@@ -294,7 +294,7 @@ jobs:\n \n   win-python3:\n       name: Python 3 Windows\n-      runs-on: windows-2019\n+      runs-on: windows-latest\n       needs: linux-python3-10\n       strategy:\n        matrix:\n---\n.github/workflows/R.yml\n@@ -43,7 +43,7 @@ jobs:\n   rstats-windows-extensions:\n     # Builds extensions for windows_amd64_rtools\n     name: R Package Windows (Extensions)\n-    runs-on: windows-2019\n+    runs-on: windows-latest\n     steps:\n       - uses: actions/checkout@v4\n         with:\n---\n.github/workflows/Windows.yml\n@@ -60,7 +60,7 @@ jobs:\n  win-release-64:\n     # Builds binaries for windows_amd64\n     name: Windows (64 Bit)\n-    runs-on: windows-2019\n+    runs-on: windows-latest\n     steps:\n     - uses: actions/checkout@v4\n       with:\n@@ -150,7 +150,7 @@ jobs:\n  win-release-32:\n     name: Windows (32 Bit)\n     if: github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' || inputs.run_all == 'true'\n-    runs-on: windows-2019\n+    runs-on: windows-latest\n     needs: win-release-64\n \n     steps:\n@@ -190,7 +190,7 @@ jobs:\n  win-release-arm64:\n    name: Windows (ARM64)\n    if: github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' || inputs.run_all == 'true'\n-   runs-on: windows-2019\n+   runs-on: windows-latest\n    needs: win-release-64\n \n    steps:\n@@ -253,7 +253,7 @@ jobs:\n \n  mingw:\n      name: MinGW (64 Bit)\n-     runs-on: windows-2019\n+     runs-on: windows-latest\n      if: ${{ inputs.skip_tests != 'true' }}\n      needs: win-release-64\n      steps:\n@@ -297,7 +297,7 @@ jobs:\n  win-extensions-64:\n    # Builds extensions for windows_amd64\n    name: Windows Extensions (64-bit)\n-   runs-on: windows-2019\n+   runs-on: windows-latest\n    needs: win-release-64\n    steps:\n      - name: Keep \\n line endings\n@@ -343,7 +343,7 @@ jobs:\n       duckdb_sha: ${{ github.sha }}\n \n  win-packaged-upload:\n-   runs-on: windows-2019\n+   runs-on: windows-latest\n    needs:\n      - win-release-64\n      - win-release-arm64\n---\n"}], "ideal": "Title: Deprecate windows-2019 runners \n\nBody: > The Windows Server 2019 runner image will be fully unsupported by June 30, 2025. To raise awareness of the upcoming removal, we will temporarily fail jobs using Windows Server 2019. Builds that are scheduled to run during the brownout periods will fail. The brownouts are scheduled for the following dates and times:\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/execution/operator/csv_scanner/csv_reader_options.hpp\n@@ -79,7 +79,7 @@ struct CSVReaderOptions {\n \tvector<string> name_list;\n \t//! If the names and types were set by the columns parameter\n \tbool columns_set = false;\n-\t//! Types considered as candidates for auto-detection ordered by descending specificity (~ from high to low)\n+\t//! Types considered as candidates for auto-detection ordered by ascending specificity (~ from low to high)\n \tvector<LogicalType> auto_type_candidates = {\n \t    LogicalType::VARCHAR,      LogicalType::DOUBLE,    LogicalType::BIGINT,\n \t    LogicalType::TIMESTAMP_TZ, LogicalType::TIMESTAMP, LogicalType::DATE,\n---\n"}], "ideal": "Title: csv_scanner: correct code comment\n\nBody: correct explanation about specificity by inverting it\r\n\r\nSQLNULL has the highest specificity not the lowest VARCHAR is the fallback and has therefore the lowest specificity\r\n\r\nSee https://github.com/duckdb/duckdb/blob/dcf0e1c8936d74be48fd1cc0309638117b43aa47/src/execution/operator/csv_scanner/util/csv_reader_options.cpp#L523-L530\r\n\r\nI was confused by the code comment in the header file while improving the documentation since it contradicts the actual specifity weights in the cpp file.\r\nhttps://github.com/duckdb/duckdb-web/pull/5459"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/join/physical_hash_join.cpp\n@@ -17,14 +17,11 @@\n #include \"duckdb/parallel/thread_context.hpp\"\n #include \"duckdb/planner/expression/bound_aggregate_expression.hpp\"\n #include \"duckdb/planner/expression/bound_reference_expression.hpp\"\n-#include \"duckdb/planner/filter/conjunction_filter.hpp\"\n #include \"duckdb/planner/filter/constant_filter.hpp\"\n #include \"duckdb/planner/filter/in_filter.hpp\"\n-#include \"duckdb/planner/filter/null_filter.hpp\"\n #include \"duckdb/planner/filter/optional_filter.hpp\"\n #include \"duckdb/planner/table_filter.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n-#include \"duckdb/storage/storage_manager.hpp\"\n #include \"duckdb/storage/temporary_memory_manager.hpp\"\n \n namespace duckdb {\n@@ -779,7 +776,9 @@ unique_ptr<DataChunk> JoinFilterPushdownInfo::Finalize(ClientContext &context, o\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t\t// if the HT is small we can generate a complete \"OR\" filter\n-\t\t\tif (ht && ht->Count() > 1 && ht->Count() <= dynamic_or_filter_threshold) {\n+\t\t\t// but only if the join condition is equality.\n+\t\t\tif (ht && ht->Count() > 1 && ht->Count() <= dynamic_or_filter_threshold &&\n+\t\t\t    cmp == ExpressionType::COMPARE_EQUAL) {\n \t\t\t\tPushInFilter(info, *ht, op, filter_idx, filter_col_idx);\n \t\t\t}\n \n---\ntest/sql/optimizer/plan/test_filter_pushdown.test\n@@ -183,3 +183,51 @@ SELECT * FROM (SELECT 0=1 AS cond FROM integers i1, integers i2) a1 WHERE cond O\n query T\n SELECT * FROM (SELECT 0=1 AS cond FROM integers i1, integers i2 GROUP BY 1) a1 WHERE cond ORDER BY 1\n ----\n+\n+# Disable IN generation with inequalities\n+statement ok\n+CREATE TABLE cohort (\n+  person_id INTEGER,\n+  cohort_start_date DATE,\n+  cohort_end_date DATE\n+);\n+\n+statement ok\n+INSERT INTO cohort (person_id, cohort_start_date, cohort_end_date) VALUES\n+  (1, DATE '2021-01-01', DATE '2021-02-15'),\n+  (1, DATE '2021-01-01', NULL);\n+\n+statement ok\n+CREATE TABLE obs (\n+  person_id INTEGER,\n+  observation_period_start_date DATE\n+);\n+\n+statement ok\n+INSERT INTO obs (person_id, observation_period_start_date) VALUES\n+  (1, DATE '2010-01-01'),\n+  (2, DATE '2010-01-01');\n+\n+query IIII\n+SELECT q01.*\n+FROM (\n+  SELECT LHS.*, observation_period_start_date\n+  FROM (\n+    SELECT q01.*\n+    FROM (\n+      SELECT\n+        person_id,\n+        cohort_start_date,\n+        COALESCE(cohort_end_date, cohort_start_date) AS cohort_end_date\n+      FROM cohort\n+    ) q01\n+    WHERE (cohort_start_date <= cohort_end_date)\n+  ) LHS\n+  INNER JOIN obs\n+    ON (LHS.person_id = obs.person_id)\n+) q01\n+WHERE (cohort_end_date >= observation_period_start_date)\n+ORDER BY ALL;\n+----\n+1\t2021-01-01\t2021-01-01\t2010-01-01\n+1\t2021-01-01\t2021-02-15\t2010-01-01\n---\n"}], "ideal": "Title: Internal #5022: IN Pushdown Equalities\n\nBody: * Only generate IN pushdown filters for equality conditions\r\n\r\nfixes: duckdblabs/duckdb-internal#5022"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/scalar/operator/arithmetic.cpp\n@@ -882,9 +882,15 @@ ScalarFunctionSet OperatorMultiplyFun::GetFunctions() {\n \tmultiply.AddFunction(\n \t    ScalarFunction({LogicalType::INTERVAL, LogicalType::DOUBLE}, LogicalType::INTERVAL,\n \t                   ScalarFunction::BinaryFunction<interval_t, double, interval_t, MultiplyOperator>));\n+\tmultiply.AddFunction(\n+\t    ScalarFunction({LogicalType::DOUBLE, LogicalType::INTERVAL}, LogicalType::INTERVAL,\n+\t                   ScalarFunction::BinaryFunction<double, interval_t, interval_t, MultiplyOperator>));\n \tmultiply.AddFunction(\n \t    ScalarFunction({LogicalType::BIGINT, LogicalType::INTERVAL}, LogicalType::INTERVAL,\n \t                   ScalarFunction::BinaryFunction<int64_t, interval_t, interval_t, MultiplyOperator>));\n+\tmultiply.AddFunction(\n+\t    ScalarFunction({LogicalType::INTERVAL, LogicalType::BIGINT}, LogicalType::INTERVAL,\n+\t                   ScalarFunction::BinaryFunction<interval_t, int64_t, interval_t, MultiplyOperator>));\n \tfor (auto &func : multiply.functions) {\n \t\tScalarFunction::SetReturnsError(func);\n \t}\n---\ntest/fuzzer/sqlsmith/interval_multiply_overflow.test\n@@ -11,4 +11,4 @@ statement error\n SELECT multiply(\"interval\", \"bigint\") \n FROM all_types;\n ----\n-Overflow in multiplication of INTERVAL\n+can't be cast because the value is out of range for the destination type INT32\n---\ntest/sql/function/interval/test_interval_muldiv.test\n@@ -79,3 +79,25 @@ FROM INTERVAL_MULDIV_TBL;\n 1 day 04:48:00\n 4 days 04:48:00\n 9 months 39 days 16:33:36\n+\n+# Commutativity\n+query I\n+select (interval '1 days') * 0.5::DOUBLE;\n+----\n+12:00:00\n+\n+query I\n+select 0.5::DOUBLE * (interval '1 days');\n+----\n+12:00:00\n+\n+query I\n+select 2::BIGINT * (interval '1 days');\n+----\n+2 days\n+\n+query I\n+select (interval '1 days') * 2::BIGINT;\n+----\n+2 days\n+\n---\n"}], "ideal": "Title: Internal #4995: Commutative INTERVAL Multiply\n\nBody: * Add missing commutative variants for DOUBLE and BIGINT\r\n* Fix broken test that no one seemed to have noticed...\r\n\r\nfixes: duckdblabs/duckdb-internal#4995"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "test/sqlite/result_helper.cpp\n@@ -219,15 +219,16 @@ bool TestResultHelper::CheckQueryResult(const Query &query, ExecuteContext &cont\n \t} else {\n \t\tbool hash_compare_error = false;\n \t\tif (query_has_label) {\n-\t\t\t// the query has a label: check if the hash has already been computed\n-\t\t\tauto entry = runner.hash_label_map.find(query_label);\n-\t\t\tif (entry == runner.hash_label_map.end()) {\n-\t\t\t\t// not computed yet: add it tot he map\n-\t\t\t\trunner.hash_label_map[query_label] = hash_value;\n-\t\t\t\trunner.result_label_map[query_label] = std::move(owned_result);\n-\t\t\t} else {\n-\t\t\t\thash_compare_error = entry->second != hash_value;\n-\t\t\t}\n+\t\t\trunner.hash_label_map.WithLock([&](unordered_map<string, CachedLabelData> &map) {\n+\t\t\t\t// the query has a label: check if the hash has already been computed\n+\t\t\t\tauto entry = map.find(query_label);\n+\t\t\t\tif (entry == map.end()) {\n+\t\t\t\t\t// not computed yet: add it tot he map\n+\t\t\t\t\tmap.emplace(query_label, CachedLabelData(hash_value, std::move(owned_result)));\n+\t\t\t\t} else {\n+\t\t\t\t\thash_compare_error = entry->second.hash != hash_value;\n+\t\t\t\t}\n+\t\t\t});\n \t\t}\n \t\tstring expected_hash;\n \t\tif (result_is_hash) {\n@@ -237,10 +238,13 @@ bool TestResultHelper::CheckQueryResult(const Query &query, ExecuteContext &cont\n \t\t}\n \t\tif (hash_compare_error) {\n \t\t\tQueryResult *expected_result = nullptr;\n-\t\t\tif (runner.result_label_map.find(query_label) != runner.result_label_map.end()) {\n-\t\t\t\texpected_result = runner.result_label_map[query_label].get();\n-\t\t\t}\n-\t\t\tlogger.WrongResultHash(expected_result, result, expected_hash, hash_value);\n+\t\t\trunner.hash_label_map.WithLock([&](unordered_map<string, CachedLabelData> &map) {\n+\t\t\t\tauto it = map.find(query_label);\n+\t\t\t\tif (it != map.end()) {\n+\t\t\t\t\texpected_result = it->second.result.get();\n+\t\t\t\t}\n+\t\t\t\tlogger.WrongResultHash(expected_result, result, expected_hash, hash_value);\n+\t\t\t});\n \t\t\treturn false;\n \t\t}\n \t\tREQUIRE(!hash_compare_error);\n---\ntest/sqlite/sqllogic_command.cpp\n@@ -258,6 +258,20 @@ Statement::Statement(SQLLogicTestRunner &runner) : Command(runner) {\n Query::Query(SQLLogicTestRunner &runner) : Command(runner) {\n }\n \n+ResetLabel::ResetLabel(SQLLogicTestRunner &runner) : Command(runner) {\n+}\n+\n+void ResetLabel::ExecuteInternal(ExecuteContext &context) const {\n+\trunner.hash_label_map.WithLock([&](unordered_map<string, CachedLabelData> &map) {\n+\t\tauto it = map.find(query_label);\n+\t\t//! should we allow this to be missing at all?\n+\t\tif (it == map.end()) {\n+\t\t\tFAIL_LINE(file_name, query_line, 0);\n+\t\t}\n+\t\tmap.erase(it);\n+\t});\n+}\n+\n RestartCommand::RestartCommand(SQLLogicTestRunner &runner, bool load_extensions_p)\n     : Command(runner), load_extensions(load_extensions_p) {\n }\n---\ntest/sqlite/sqllogic_command.hpp\n@@ -92,6 +92,21 @@ class Statement : public Command {\n \t}\n };\n \n+class ResetLabel : public Command {\n+public:\n+\tResetLabel(SQLLogicTestRunner &runner);\n+\n+public:\n+\tvoid ExecuteInternal(ExecuteContext &context) const override;\n+\n+\tbool SupportsConcurrent() const override {\n+\t\treturn true;\n+\t}\n+\n+public:\n+\tstring query_label;\n+};\n+\n class Query : public Command {\n public:\n \tQuery(SQLLogicTestRunner &runner);\n---\ntest/sqlite/sqllogic_parser.cpp\n@@ -155,6 +155,7 @@ bool SQLLogicParser::IsSingleLineStatement(SQLLogicToken &token) {\n \tcase SQLLogicTokenType::SQLLOGIC_HALT:\n \tcase SQLLogicTokenType::SQLLOGIC_MODE:\n \tcase SQLLogicTokenType::SQLLOGIC_SET:\n+\tcase SQLLogicTokenType::SQLLOGIC_RESET:\n \tcase SQLLogicTokenType::SQLLOGIC_LOOP:\n \tcase SQLLogicTokenType::SQLLOGIC_FOREACH:\n \tcase SQLLogicTokenType::SQLLOGIC_CONCURRENT_LOOP:\n@@ -198,6 +199,8 @@ SQLLogicTokenType SQLLogicParser::CommandToToken(const string &token) {\n \t\treturn SQLLogicTokenType::SQLLOGIC_MODE;\n \t} else if (token == \"set\") {\n \t\treturn SQLLogicTokenType::SQLLOGIC_SET;\n+\t} else if (token == \"reset\") {\n+\t\treturn SQLLogicTokenType::SQLLOGIC_RESET;\n \t} else if (token == \"loop\") {\n \t\treturn SQLLogicTokenType::SQLLOGIC_LOOP;\n \t} else if (token == \"concurrentloop\") {\n---\ntest/sqlite/sqllogic_parser.hpp\n@@ -24,6 +24,7 @@ enum class SQLLogicTokenType {\n \tSQLLOGIC_HALT,\n \tSQLLOGIC_MODE,\n \tSQLLOGIC_SET,\n+\tSQLLOGIC_RESET,\n \tSQLLOGIC_LOOP,\n \tSQLLOGIC_FOREACH,\n \tSQLLOGIC_CONCURRENT_LOOP,\n---\ntest/sqlite/sqllogic_test_runner.cpp\n@@ -888,6 +888,19 @@ void SQLLogicTestRunner::ExecuteFile(string script) {\n \t\t\t} else {\n \t\t\t\tparser.Fail(\"unrecognized set parameter: %s\", token.parameters[0]);\n \t\t\t}\n+\t\t} else if (token.type == SQLLogicTokenType::SQLLOGIC_RESET) {\n+\t\t\tif (token.parameters.size() != 2) {\n+\t\t\t\tparser.Fail(\"Expected reset [type] [name] (e.g reset label my_label)\");\n+\t\t\t}\n+\t\t\tauto &reset_type = token.parameters[0];\n+\t\t\tauto &reset_item = token.parameters[1];\n+\t\t\tif (StringUtil::CIEquals(\"label\", reset_type)) {\n+\t\t\t\tauto reset_label_command = make_uniq<ResetLabel>(*this);\n+\t\t\t\treset_label_command->query_label = reset_item;\n+\t\t\t\tExecuteCommand(std::move(reset_label_command));\n+\t\t\t} else {\n+\t\t\t\tparser.Fail(\"unrecognized reset parameter: %s\", reset_type);\n+\t\t\t}\n \t\t} else if (token.type == SQLLogicTokenType::SQLLOGIC_LOOP ||\n \t\t           token.type == SQLLogicTokenType::SQLLOGIC_CONCURRENT_LOOP) {\n \t\t\tif (token.parameters.size() != 3) {\n---\ntest/sqlite/sqllogic_test_runner.hpp\n@@ -20,6 +20,28 @@ class SQLLogicParser;\n \n enum class RequireResult { PRESENT, MISSING };\n \n+struct CachedLabelData {\n+public:\n+\tCachedLabelData(const string &hash, unique_ptr<QueryResult> result) : hash(hash), result(std::move(result)) {\n+\t}\n+\n+public:\n+\tstring hash;\n+\tunique_ptr<QueryResult> result;\n+};\n+\n+struct HashLabelMap {\n+public:\n+\tvoid WithLock(std::function<void(unordered_map<string, CachedLabelData> &map)> cb) {\n+\t\tstd::lock_guard<std::mutex> guard(lock);\n+\t\tcb(map);\n+\t}\n+\n+public:\n+\tstd::mutex lock;\n+\tunordered_map<string, CachedLabelData> map;\n+};\n+\n class SQLLogicTestRunner {\n public:\n \tSQLLogicTestRunner(string dbpath);\n@@ -52,8 +74,7 @@ class SQLLogicTestRunner {\n \tunordered_set<string> always_fail_error_messages = {\"differs from original result!\", \"INTERNAL\"};\n \n \t//! The map converting the labels to the hash values\n-\tunordered_map<string, string> hash_label_map;\n-\tunordered_map<string, duckdb::unique_ptr<QueryResult>> result_label_map;\n+\tHashLabelMap hash_label_map;\n \tmutex log_lock;\n \n public:\n---\n"}], "ideal": "Title: [SQLLogicTester] Introduce `reset label <query label>` in the tester\n\nBody: Using the \"query label\":\r\n```sql\r\nquery III nosort my_query_label\r\nselect a, b, c;\r\n----\r\n\r\n```\r\n\r\nThis will store the query result and the hash of the result, so when this same label is used again, we will compare against the saved hash\r\n\r\nThis works great, but is problematic in situations where loops are involved, for example:\r\n```sql\r\nforeach var hello test world\r\n\r\nquery I nosort my_label\r\nselect '${var}'\r\n\r\nquery I nosort my_label\r\nselect '${var}'\r\n\r\nendloop\r\n```\r\nThis causes an error on the second iteration, because the result `test` is compared against the saved result from the first iteration, `hello`.\r\n\r\nWith the introduction of `reset label <query label>`, we can reset the label at the end of the loop, like so:\r\n```sql\r\nforeach var hello test world\r\n\r\nquery I nosort my_label\r\nselect '${var}'\r\n\r\nquery I nosort my_label\r\nselect '${var}'\r\n\r\nreset label my_label\r\n\r\nendloop\r\n```\r\n\r\nNow the test passes successfully."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types/uuid.cpp\n@@ -1,4 +1,5 @@\n #include \"duckdb/common/types/uuid.hpp\"\n+#include \"duckdb/common/chrono.hpp\"\n #include \"duckdb/common/random_engine.hpp\"\n \n namespace duckdb {\n---\n"}], "ideal": "Title: Fix windows-2025 build errors\n\nBody: GHA is retiring windows-2019 runner image. This commit fixes a build error on windows-2025\r\n\r\n> The Windows Server 2019 runner image will be fully unsupported by June 30, 2025. To raise awareness of the upcoming removal, we will temporarily fail jobs using Windows Server 2019. Builds that are scheduled to run during the brownout periods will fail. The brownouts are scheduled for the following dates and times:\r\n> \r\n> June 3 13:00-21:00 UTC\r\n> June 10 13:00-21:00 UTC\r\n> June 17 13:00-21:00 UTC\r\n> June 24 13:00-21:00 UTC"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/reader/decimal_column_reader.cpp\n@@ -46,7 +46,7 @@ double ParquetDecimalUtils::ReadDecimalValue(const_data_ptr_t pointer, idx_t siz\n }\n \n unique_ptr<ColumnReader> ParquetDecimalUtils::CreateReader(ParquetReader &reader, const ParquetColumnSchema &schema) {\n-\tif (schema.type_length > 0) {\n+\tif (schema.parquet_type == Type::FIXED_LEN_BYTE_ARRAY) {\n \t\treturn CreateDecimalReaderInternal<true>(reader, schema);\n \t} else {\n \t\treturn CreateDecimalReaderInternal<false>(reader, schema);\n---\nextension/parquet/reader/string_column_reader.cpp\n@@ -11,7 +11,7 @@ namespace duckdb {\n StringColumnReader::StringColumnReader(ParquetReader &reader, const ParquetColumnSchema &schema)\n     : ColumnReader(reader, schema) {\n \tfixed_width_string_length = 0;\n-\tif (schema.type_length > 0) {\n+\tif (schema.parquet_type == Type::FIXED_LEN_BYTE_ARRAY) {\n \t\tfixed_width_string_length = schema.type_length;\n \t}\n }\n---\n"}], "ideal": "Title: Parquet Reader: only read strings as fixed length strings if the type is FIXED_LEN_BYTE_ARRAY\n\nBody: Fixes a regression introduced in https://github.com/duckdb/duckdb/pull/16161\r\n\r\nType length may also be set for variable-length byte arrays (in which case it should be ignored)."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/actions/build_extensions/action.yml\n@@ -103,7 +103,7 @@ runs:\n       if: inputs.vcpkg_build == 1\n       uses: lukka/run-vcpkg@v11.1\n       with:\n-        vcpkgGitCommitId: 5e5d0e1cd7785623065e77eff011afdeec1a3574\n+        vcpkgGitCommitId: ce613c41372b23b1f51333815feb3edd87ef8a8b\n \n     - name: Set vcpkg env variables\n       if: inputs.vcpkg_build == 1\n---\n.github/actions/build_extensions_dockerized/action.yml\n@@ -44,7 +44,7 @@ runs:\n         run: |\n           docker build \\\n             --build-arg 'vcpkg_url=https://github.com/microsoft/vcpkg.git' \\\n-            --build-arg 'vcpkg_commit=5e5d0e1cd7785623065e77eff011afdeec1a3574' \\\n+            --build-arg 'vcpkg_commit=ce613c41372b23b1f51333815feb3edd87ef8a8b' \\\n             --build-arg 'extra_toolchains=;python3;' \\\n             -t duckdb/${{ inputs.duckdb_arch }} \\\n             ./extension-ci-tools/docker/${{ inputs.duckdb_arch }}\n@@ -62,10 +62,6 @@ runs:\n           echo \"DUCKDB_PLATFORM=${{ inputs.duckdb_arch }}\" >> docker_env.txt\n           echo \"OVERRIDE_GIT_DESCRIBE=${{ inputs.override_git_describe }}\" >> docker_env.txt\n           echo \"LINUX_CI_IN_DOCKER=1\" >> docker_env.txt\n-          # TODO: no more cross compilation for extension builds\n-          echo \"TOOLCHAIN_FLAGS=${{ inputs.duckdb_arch == 'linux_arm64' && '-DCMAKE_C_COMPILER=aarch64-linux-gnu-gcc -DCMAKE_CXX_COMPILER=aarch64-linux-gnu-g++ -DCMAKE_Fortran_COMPILER=aarch64-linux-gnu-gfortran' || '' }}\" >> docker_env.txt\n-          echo \"CC=${{ inputs.duckdb_arch == 'linux_arm64' && 'aarch64-linux-gnu-gcc' || '' }}\" >> docker_env.txt\n-          echo \"CXX=${{ inputs.duckdb_arch == 'linux_arm64' && 'aarch64-linux-gnu-g++' || '' }}\" >> docker_env.txt \n \n       - name: Generate timestamp for Ccache entry\n         shell: bash\n@@ -98,6 +94,6 @@ runs:\n \n       - name: Test extension (inside docker)\n         shell: bash\n-        if: ${{ inputs.run_tests == 1 && inputs.duckdb_arch != 'linux_arm64'}}\n+        if: ${{ inputs.run_tests == 1 }}\n         run: |\n           docker run --env-file=docker_env.txt -v `pwd`:/duckdb_build_dir -v `pwd`/ccache_dir:/ccache_dir duckdb/${{ inputs.duckdb_arch }} make test_release\n---\n.github/workflows/LinuxRelease.yml\n@@ -128,8 +128,6 @@ jobs:\n         build/release/benchmark/benchmark_runner benchmark/micro/update/update_with_join.benchmark\n         build/release/duckdb -c \"COPY (SELECT 42) TO '/dev/stdout' (FORMAT PARQUET)\" | cat\n \n- # Linux extensions for builds that use C++11 ABI, currently these are all linux builds based on ubuntu >= 18 (e.g. NodeJS)\n- # note that the linux-release-64 is based on the manylinux-based extensions, which are built in .github/workflows/Python.yml\n  linux-extensions-64:\n     # Builds extensions for linux_amd64\n     name: Linux Extensions (x64)\n@@ -218,7 +216,7 @@ jobs:\n  linux-extensions-64-aarch64:\n     # Builds extensions for linux_arm64\n     name: Linux Extensions (aarch64)\n-    runs-on: ubuntu-latest\n+    runs-on: ubuntu-24.04-arm\n     needs: linux-extensions-64\n     strategy:\n       matrix:\n---\n.github/workflows/Wasm.yml\n@@ -63,7 +63,7 @@ jobs:\n       - name: Setup vcpkg\n         uses: lukka/run-vcpkg@v11.1\n         with:\n-          vcpkgGitCommitId: '5e5d0e1cd7785623065e77eff011afdeec1a3574'\n+          vcpkgGitCommitId: 'ce613c41372b23b1f51333815feb3edd87ef8a8b'\n           vcpkgGitURL: 'https://github.com/microsoft/vcpkg.git'\n \n       - name: Setup Ccache\n---\nscripts/merge_vcpkg_deps.py\n@@ -58,7 +58,7 @@ def prefix_overlay_port(overlay_port):\n \n data = {\n     \"description\": f\"Auto-generated vcpkg.json for combined DuckDB extension build, generated by 'scripts/merge_vcpkg_deps.py'\",\n-    \"builtin-baseline\": \"5e5d0e1cd7785623065e77eff011afdeec1a3574\",\n+    \"builtin-baseline\": \"ce613c41372b23b1f51333815feb3edd87ef8a8b\",\n     \"dependencies\": final_deduplicated_deps,\n     \"overrides\": [{\"name\": \"openssl\", \"version\": \"3.0.8\"}],\n }\n---\n"}], "ideal": "Title: fix linux extension ci\n\nBody: by moving to manylinux images and bumping vcpkg"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "data/csv/17705.csv\n@@ -0,0 +1,4 @@\n+day_night,operation_no,flight_date_time\n+Night,38615452,2022/01/27 11:04:57 PM\n+Night,38615452,2022/01/27 11:04:57 PM\n+Night,38615475,2022/01/27 11:09:20 PM\n\\ No newline at end of file\n---\nextension/icu/icu-datefunc.cpp\n@@ -82,10 +82,12 @@ bool ICUDateFunc::TrySetTimeZone(icu::Calendar *calendar, const string_t &tz_id)\n \treturn true;\n }\n \n-void ICUDateFunc::SetTimeZone(icu::Calendar *calendar, const string_t &tz_id) {\n+void ICUDateFunc::SetTimeZone(icu::Calendar *calendar, const string_t &tz_id, string *error_message) {\n \tstring tz_str = tz_id.GetString();\n-\tauto tz = ICUHelpers::GetTimeZone(tz_str);\n-\tcalendar->adoptTimeZone(tz.release());\n+\tauto tz = ICUHelpers::GetTimeZone(tz_str, error_message);\n+\tif (tz) {\n+\t\tcalendar->adoptTimeZone(tz.release());\n+\t}\n }\n \n timestamp_t ICUDateFunc::GetTimeUnsafe(icu::Calendar *calendar, uint64_t micros) {\n---\nextension/icu/icu-strptime.cpp\n@@ -293,7 +293,12 @@ struct ICUStrptime : public ICUDateFunc {\n \n \t\t\t\t    // Change TZ if one was provided.\n \t\t\t\t    if (tz.GetSize()) {\n-\t\t\t\t\t    SetTimeZone(calendar, tz);\n+\t\t\t\t\t    string error_msg;\n+\t\t\t\t\t    SetTimeZone(calendar, tz, &error_msg);\n+\t\t\t\t\t    if (!error_msg.empty()) {\n+\t\t\t\t\t\t    HandleCastError::AssignError(error_msg, parameters);\n+\t\t\t\t\t\t    mask.SetInvalid(idx);\n+\t\t\t\t\t    }\n \t\t\t\t    }\n \n \t\t\t\t    // Now get the parts in the given time zone\n---\nextension/icu/icu_extension.cpp\n@@ -252,15 +252,20 @@ unique_ptr<icu::TimeZone> ICUHelpers::TryGetTimeZone(string &tz_str) {\n \treturn GetTimeZoneInternal(tz_str, candidates);\n }\n \n-unique_ptr<icu::TimeZone> ICUHelpers::GetTimeZone(string &tz_str) {\n+unique_ptr<icu::TimeZone> ICUHelpers::GetTimeZone(string &tz_str, string *error_message) {\n \tvector<string> candidates;\n \tauto tz = GetTimeZoneInternal(tz_str, candidates);\n \tif (tz) {\n \t\treturn tz;\n \t}\n \tstring candidate_str =\n \t    StringUtil::CandidatesMessage(StringUtil::TopNJaroWinkler(candidates, tz_str), \"Candidate time zones\");\n-\n+\tif (error_message) {\n+\t\tstd::stringstream ss;\n+\t\tss << \"Unknown TimeZone '\" << tz_str << \"'!\\n\" << candidate_str;\n+\t\t*error_message = ss.str();\n+\t\treturn nullptr;\n+\t}\n \tthrow NotImplementedException(\"Unknown TimeZone '%s'!\\n%s\", tz_str, candidate_str);\n }\n \n---\nextension/icu/include/icu-datefunc.hpp\n@@ -50,7 +50,7 @@ struct ICUDateFunc {\n \t//! Tries to set the time zone for the calendar and returns false if it is not valid.\n \tstatic bool TrySetTimeZone(icu::Calendar *calendar, const string_t &tz_id);\n \t//! Sets the time zone for the calendar. Throws if it is not valid\n-\tstatic void SetTimeZone(icu::Calendar *calendar, const string_t &tz_id);\n+\tstatic void SetTimeZone(icu::Calendar *calendar, const string_t &tz_id, string *error_message = nullptr);\n \t//! Gets the timestamp from the calendar, throwing if it is not in range.\n \tstatic bool TryGetTime(icu::Calendar *calendar, uint64_t micros, timestamp_t &result);\n \t//! Gets the timestamp from the calendar, throwing if it is not in range.\n---\nextension/icu/include/icu-helpers.hpp\n@@ -18,7 +18,7 @@ struct ICUHelpers {\n \t//! Tries to get a time zone - returns nullptr if the timezone is not found\n \tstatic unique_ptr<icu::TimeZone> TryGetTimeZone(string &tz_str);\n \t//! Gets a time zone - throws an error if the timezone is not found\n-\tstatic unique_ptr<icu::TimeZone> GetTimeZone(string &tz_str);\n+\tstatic unique_ptr<icu::TimeZone> GetTimeZone(string &tz_str, string *error_message = nullptr);\n \n \tstatic TimestampComponents GetComponents(timestamp_tz_t ts, icu::Calendar *calendar);\n \n---\ntest/sql/copy/csv/timestamp_with_tz.test\n@@ -18,6 +18,11 @@ require icu\n \n require no_extension_autoloading \"FIXME: In CSV value conversion we don't autoload ICU\"\n \n+query I\n+SELECT TRY_CAST('2022/01/27 11:04:57 PM' AS TIMESTAMPTZ);\n+----\n+NULL\n+\n # we can load this into a timestamptz table\n statement ok\n CREATE TABLE tbl_tz(id int, ts timestamptz);\n@@ -33,3 +38,10 @@ SELECT * FROM tbl_tz\n ----\n 1\t2021-05-25 04:55:03.382494+00\n 2\t2021-05-25 09:55:03.382494+00\n+\n+query III\n+FROM read_csv('data/csv/17705.csv')\n+----\n+Night\t38615452\t2022/01/27 11:04:57 PM\n+Night\t38615452\t2022/01/27 11:04:57 PM\n+Night\t38615475\t2022/01/27 11:09:20 PM\n---\n"}], "ideal": "Title: Pop up ICU errors to the csv sniffer\n\nBody: Fix: https://github.com/duckdb/duckdb/issues/17705"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/subquery/flatten_dependent_join.cpp\n@@ -114,7 +114,8 @@ unique_ptr<LogicalOperator> FlattenDependentJoins::Decorrelate(unique_ptr<Logica\n \t\tdelim_join->children[1] =\n \t\t    flatten.PushDownDependentJoin(std::move(delim_join->children[1]), propagate_null_values, lateral_depth);\n \t\tdata_offset = flatten.data_offset;\n-\t\tdelim_offset = flatten.delim_offset;\n+\t\tauto left_offset = delim_join->children[0]->GetColumnBindings().size();\n+\t\tdelim_offset = left_offset + flatten.delim_offset;\n \n \t\tRewriteCorrelatedExpressions rewriter(base_binding, correlated_map, lateral_depth);\n \t\trewriter.VisitOperator(*plan);\n---\ntest/issues/general/test_17700.test\n@@ -0,0 +1,29 @@\n+# name: test/issues/general/test_17700.test\n+# description: Issue 17446 - unexpected internal error in multi-join query (ExpressionExecutor::Execute called with a result vector of type BOOLEAN that does not match expression type INTEGER)\n+# group: [general]\n+\n+statement ok\n+CREATE TABLE t0(c0 BOOLEAN);\n+\n+statement ok\n+CREATE TABLE t2(c1 INT);\n+\n+statement ok\n+CREATE TABLE t3(c0 INT);\n+\n+statement ok\n+INSERT INTO t3(c0) VALUES (1);\n+\n+statement ok\n+INSERT INTO t0(c0) VALUES (TRUE);\n+\n+statement ok\n+CREATE VIEW v0(c0, c1, c2) AS SELECT DISTINCT t0.c0 AS col_0, t0.c0 AS col_1, t3.c0 AS col_2 FROM t0, t3;\n+\n+statement ok\n+INSERT INTO t2(c1) VALUES (1);\n+\n+query IIIII\n+SELECT * FROM t2, v0 INNER JOIN ( SELECT t2.c1 AS col_0 WHERE v0.c2) as subQuery0 ON v0.c2;\n+----\n+1\ttrue\ttrue\t1\t1\n---\n"}], "ideal": "Title: Correctly setting the delim offset\n\nBody: This PR fixes #17700."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/join/physical_hash_join.cpp\n@@ -778,10 +778,6 @@ unique_ptr<DataChunk> JoinFilterPushdownInfo::Finalize(ClientContext &context, o\n \t\t\t\t// hash table e.g. because they are part of a RIGHT join\n \t\t\t\tcontinue;\n \t\t\t}\n-\t\t\t// if the HT is small we can generate a complete \"OR\" filter\n-\t\t\tif (ht && ht->Count() > 1 && ht->Count() <= dynamic_or_filter_threshold) {\n-\t\t\t\tPushInFilter(info, *ht, op, filter_idx, filter_col_idx);\n-\t\t\t}\n \n \t\t\tif (Value::NotDistinctFrom(min_val, max_val)) {\n \t\t\t\t// min = max - single value\n@@ -794,19 +790,26 @@ unique_ptr<DataChunk> JoinFilterPushdownInfo::Finalize(ClientContext &context, o\n \t\t\t\t// for non-equalities, the range must be half-open\n \t\t\t\t// e.g., for lhs < rhs we can only use lhs <= max\n \t\t\t\tswitch (cmp) {\n-\t\t\t\tcase ExpressionType::COMPARE_EQUAL:\n-\t\t\t\tcase ExpressionType::COMPARE_GREATERTHAN:\n-\t\t\t\tcase ExpressionType::COMPARE_GREATERTHANOREQUALTO: {\n+\t\t\t\tcase ExpressionType::COMPARE_EQUAL: {\n+\t\t\t\t\t// if the HT is small we can generate a complete \"OR\" filter\n+\t\t\t\t\tif (ht && ht->Count() > 1 && ht->Count() <= dynamic_or_filter_threshold) {\n+\t\t\t\t\t\tPushInFilter(info, *ht, op, filter_idx, filter_col_idx);\n+\t\t\t\t\t}\n \t\t\t\t\tauto greater_equals =\n \t\t\t\t\t    make_uniq<ConstantFilter>(ExpressionType::COMPARE_GREATERTHANOREQUALTO, std::move(min_val));\n \t\t\t\t\tinfo.dynamic_filters->PushFilter(op, filter_col_idx, std::move(greater_equals));\n+\t\t\t\t\tauto less_equals =\n+\t\t\t\t\t    make_uniq<ConstantFilter>(ExpressionType::COMPARE_LESSTHANOREQUALTO, std::move(max_val));\n+\t\t\t\t\tinfo.dynamic_filters->PushFilter(op, filter_col_idx, std::move(less_equals));\n \t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tdefault:\n+\t\t\t\tcase ExpressionType::COMPARE_GREATERTHAN:\n+\t\t\t\tcase ExpressionType::COMPARE_GREATERTHANOREQUALTO: {\n+\t\t\t\t\tauto greater_equals =\n+\t\t\t\t\t    make_uniq<ConstantFilter>(ExpressionType::COMPARE_GREATERTHANOREQUALTO, std::move(min_val));\n+\t\t\t\t\tinfo.dynamic_filters->PushFilter(op, filter_col_idx, std::move(greater_equals));\n \t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tswitch (cmp) {\n-\t\t\t\tcase ExpressionType::COMPARE_EQUAL:\n \t\t\t\tcase ExpressionType::COMPARE_LESSTHAN:\n \t\t\t\tcase ExpressionType::COMPARE_LESSTHANOREQUALTO: {\n \t\t\t\t\tauto less_equals =\n---\nsrc/execution/operator/scan/physical_table_scan.cpp\n@@ -30,7 +30,7 @@ class TableScanGlobalSourceState : public GlobalSourceState {\n \t\t}\n \n \t\tif (op.function.init_global) {\n-\t\t\tauto filters = table_filters ? *table_filters : GetTableFilters(op);\n+\t\t\tauto filters = GetTableFilters(op);\n \t\t\tTableFunctionInitInput input(op.bind_data.get(), op.column_ids, op.projection_ids, filters,\n \t\t\t                             op.extra_info.sample_options);\n \n---\ntest/issues/general/test_17693.test\n@@ -0,0 +1,52 @@\n+# name: test/issues/general/test_17693.test\n+# description: Issue 17693 - Filter inequality condition fail\n+# group: [general]\n+\n+statement ok\n+pragma enable_verification\n+\n+statement ok\n+create table source(\n+  id uuid,\n+  created_at int\n+);\n+\n+statement ok\n+create table temporal_data(\n+  id uuid,\n+  start_ts_inclusive int,\n+  end_ts_exclusive int,\n+  value text\n+);\n+\n+statement ok\n+insert into source (id, created_at)\n+values\n+('faceb00c-feed-dead-beef-abcdef123456'::uuid, 100),\n+('feed1234-5678-abcd-eeee-bada55ba11ad'::uuid, 111),\n+('decafbad-c0de-cafe-babe-0000deadbeef'::uuid, 122)\n+;\n+\n+statement ok\n+insert into temporal_data (id, start_ts_inclusive, end_ts_exclusive, value)\n+values\n+('faceb00c-feed-dead-beef-abcdef123456'::uuid, 100,122, 'Beef'),\n+('feed1234-5678-abcd-eeee-bada55ba11ad'::uuid, 111,144, 'Pork'),\n+('decafbad-c0de-cafe-babe-0000deadbeef'::uuid, 122,166, 'Double Shot Espresso')\n+;\n+\n+query III\n+select\n+  source.id,\n+  temporal_data.value,\n+  created_at\n+from source\n+join temporal_data\n+  on source.id = temporal_data.id\n+    and source.created_at >= temporal_data.start_ts_inclusive\n+    and source.created_at < temporal_data.end_ts_exclusive\n+where source.created_at < 122\n+order by created_at;\n+----\n+faceb00c-feed-dead-beef-abcdef123456\tBeef\t100\n+feed1234-5678-abcd-eeee-bada55ba11ad\tPork\t111\n---\n"}], "ideal": "Title: Fix #17693: only equal join condition can use InFilter\n\nBody: This pr try to fix #17693 \r\n\r\nIf I haven't miss anything I think  the modification in #17317 may have some fault. InFilter can only be used in Inequality join condition only when we can prove probe side column's value must exist in build side (ex. columns have foreign key constraint)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "test/sqlite/result_helper.cpp\n@@ -88,7 +88,9 @@ bool TestResultHelper::CheckQueryResult(const Query &query, ExecuteContext &cont\n \n \tvector<string> comparison_values;\n \tif (values.size() == 1 && ResultIsFile(values[0])) {\n-\t\tauto fname = runner.LoopReplacement(values[0], context.running_loops);\n+\t\tauto fname = StringUtil::Replace(values[0], \"<FILE>:\", \"\");\n+\t\tfname = runner.ReplaceKeywords(fname);\n+\t\tfname = runner.LoopReplacement(fname, context.running_loops);\n \t\tstring csv_error;\n \t\tcomparison_values = LoadResultFromFile(fname, result.names, expected_column_count, csv_error);\n \t\tif (!csv_error.empty()) {\n@@ -316,7 +318,6 @@ vector<string> TestResultHelper::LoadResultFromFile(string fname, vector<string>\n \tConnection con(db);\n \tauto threads = MaxValue<idx_t>(std::thread::hardware_concurrency(), 1);\n \tcon.Query(\"PRAGMA threads=\" + to_string(threads));\n-\tfname = StringUtil::Replace(fname, \"<FILE>:\", \"\");\n \n \tstring struct_definition = \"STRUCT_PACK(\";\n \tfor (idx_t i = 0; i < names.size(); i++) {\n---\n"}], "ideal": "Title: [SQLLogicTester] Replace keywords in `<FILE>:pattern` result for the `query` statement\n\nBody: This PR is part of fixing https://github.com/duckdblabs/duckdb-internal/issues/5006\r\n\r\nThis is required for `duckdb-iceberg`, as it uses `<FILE>:` for its TPCH tests, which needs a `__WORKING_DIRECTORY__` to function when called from duckdb/duckdb\r\n(https://github.com/duckdb/duckdb-iceberg/pull/270)\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "CMakeLists.txt\n@@ -249,7 +249,7 @@ set(VERSIONING_TAG_MATCH \"v*.*.*\")\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-option(MAIN_BRANCH_VERSIONING \"Versioning scheme for main branch\" FALSE)\n+option(MAIN_BRANCH_VERSIONING \"Versioning scheme for main branch\" TRUE)\n if(${MAIN_BRANCH_VERSIONING})\n   set(VERSIONING_TAG_MATCH \"v*.*.0\")\n endif()\n---\nscripts/amalgamation.py\n@@ -279,7 +279,7 @@ def git_commit_hash():\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-MAIN_BRANCH_VERSIONING = False\n+MAIN_BRANCH_VERSIONING = True\n if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n     MAIN_BRANCH_VERSIONING = False\n if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n---\nscripts/package_build.py\n@@ -148,7 +148,7 @@ def get_relative_path(source_dir, target_file):\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-MAIN_BRANCH_VERSIONING = False\n+MAIN_BRANCH_VERSIONING = True\n if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n     MAIN_BRANCH_VERSIONING = False\n if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n---\ntools/pythonpkg/setup.py\n@@ -389,7 +389,7 @@ def setup_data_files(data_files):\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-MAIN_BRANCH_VERSIONING = False\n+MAIN_BRANCH_VERSIONING = True\n if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n     MAIN_BRANCH_VERSIONING = False\n if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n---\n"}], "ideal": "Title: Revert \"set default for MAIN_BRANCH_VERSIONING to false\"\n\nBody: This reverts commit 7cd2e2439017200753d120ec74b5575639b3844b.\r\n\r\nAfter this `main` branch, and branch on top of that, should get named like `v1.4.0-devX` where X is the distance, in commits from v1.3.0, while `v1.3-ossivalis` and derived branches will get a numbering like `v1.3.N-devX`, where X is the distance in commits from v1.3.0 while N is basically 1 for now, and 2 after 1.3.1.\r\n\r\nThis unlocks publishing nightly builds also on `main`.\r\n\r\nWith @c-herrewijn "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/function.cpp\n@@ -36,6 +36,10 @@ bool TableFunctionData::Equals(const FunctionData &other) const {\n \treturn false;\n }\n \n+bool FunctionData::SupportStatementCache() const {\n+\treturn true;\n+}\n+\n Function::Function(string name_p) : name(std::move(name_p)) {\n }\n Function::~Function() {\n---\nsrc/include/duckdb/common/multi_file/multi_file_states.hpp\n@@ -84,6 +84,9 @@ struct MultiFileBindData : public TableFunctionData {\n \tvoid Initialize(ClientContext &, BaseUnionData &union_data) {\n \t\tInitialize(std::move(union_data.reader));\n \t}\n+\tbool SupportStatementCache() const override {\n+\t\treturn false;\n+\t}\n \n \tunique_ptr<FunctionData> Copy() const override;\n };\n---\nsrc/include/duckdb/function/function.hpp\n@@ -61,6 +61,7 @@ struct FunctionData {\n \tDUCKDB_API virtual unique_ptr<FunctionData> Copy() const = 0;\n \tDUCKDB_API virtual bool Equals(const FunctionData &other) const = 0;\n \tDUCKDB_API static bool Equals(const FunctionData *left, const FunctionData *right);\n+\tDUCKDB_API virtual bool SupportStatementCache() const;\n \n \ttemplate <class TARGET>\n \tTARGET &Cast() {\n---\nsrc/planner/binder/tableref/bind_basetableref.cpp\n@@ -298,6 +298,9 @@ unique_ptr<BoundTableRef> Binder::Bind(BaseTableRef &ref) {\n \n \t\tunique_ptr<FunctionData> bind_data;\n \t\tauto scan_function = table.GetScanFunction(context, bind_data, table_lookup);\n+\t\tif (bind_data && !bind_data->SupportStatementCache()) {\n+\t\t\tSetAlwaysRequireRebind();\n+\t\t}\n \t\t// TODO: bundle the type and name vector in a struct (e.g PackedColumnMetadata)\n \t\tvector<LogicalType> table_types;\n \t\tvector<string> table_names;\n---\nsrc/planner/binder/tableref/bind_table_function.cpp\n@@ -240,6 +240,9 @@ unique_ptr<LogicalOperator> Binder::BindTableFunctionInternal(TableFunction &tab\n \t\tthrow InvalidInputException(\"Cannot call function \\\"%s\\\" directly - it has no bind function\",\n \t\t                            table_function.name);\n \t}\n+\tif (bind_data && !bind_data->SupportStatementCache()) {\n+\t\tSetAlwaysRequireRebind();\n+\t}\n \tif (return_types.size() != return_names.size()) {\n \t\tthrow InternalException(\"Failed to bind \\\"%s\\\": return_types/names must have same size\", table_function.name);\n \t}\n---\n"}], "ideal": "Title: Allow table functions to disable statement caching\n\nBody: Currently we assume all plans can be cached - this allows table functions to opt out of statement caching. If this is opted out of we will always rebind when re-executing a prepared statement instead."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/storage/caching_file_system.cpp\n@@ -113,7 +113,7 @@ BufferHandle CachingFileHandle::Read(data_ptr_t &buffer, idx_t &nr_bytes) {\n \n \t// If we can't seek, we can't use the cache for these calls,\n \t// because we won't be able to seek over any parts we skipped by reading from the cache\n-\tif (!external_file_cache.IsEnabled() || !GetFileHandle().CanSeek()) {\n+\tif (!external_file_cache.IsEnabled() || !CanSeek()) {\n \t\tresult = external_file_cache.GetBufferManager().Allocate(MemoryTag::EXTERNAL_FILE_CACHE, nr_bytes);\n \t\tbuffer = result.Ptr();\n \t\tnr_bytes = NumericCast<idx_t>(GetFileHandle().Read(buffer, nr_bytes));\n---\n"}], "ideal": "Title: Do not get file handle unnecessarily. \n\nBody: `GetFileHandle()` bypasses a check to `validate` which tells the caching_file_system to prefer file data in cache. By calling `CanSeek()` first there is a check to the cache if the file is in cache and if seeking is possible. This avoids an unnecessary head request for full file reads (like avro on Iceberg)."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/include/writer/templated_column_writer.hpp\n@@ -403,7 +403,7 @@ class StandardColumnWriter : public PrimitiveColumnWriter {\n \t\t\tbreak;\n \t\t}\n \t\tcase duckdb_parquet::Encoding::BYTE_STREAM_SPLIT: {\n-\t\t\tif (page_state.bss_initialized) {\n+\t\t\tif (!page_state.bss_initialized) {\n \t\t\t\tpage_state.bss_encoder.BeginWrite(BufferAllocator::Get(writer.GetContext()));\n \t\t\t\tpage_state.bss_initialized = true;\n \t\t\t}\n---\nsrc/storage/caching_file_system.cpp\n@@ -285,11 +285,16 @@ BufferHandle CachingFileHandle::TryInsertFileRange(BufferHandle &pin, data_ptr_t\n \tauto &ranges = cached_file.Ranges(guard);\n \n \t// Start at lower_bound (first range with location not less than location of newly created range)\n+\tconst auto this_end = location + nr_bytes;\n \tauto it = ranges.lower_bound(location);\n \tif (it != ranges.begin()) {\n \t\t--it;\n \t}\n \twhile (it != ranges.end()) {\n+\t\tif (it->second->location >= this_end) {\n+\t\t\t// We're past the requested location\n+\t\t\tbreak;\n+\t\t}\n \t\tif (it->second->GetOverlap(*new_file_range) == CachedFileRangeOverlap::FULL) {\n \t\t\t// Another thread has read a range that fully contains the requested range in the meantime\n \t\t\tauto other_pin = TryReadFromFileRange(guard, *it->second, buffer, nr_bytes, location);\n@@ -300,11 +305,9 @@ BufferHandle CachingFileHandle::TryInsertFileRange(BufferHandle &pin, data_ptr_t\n \t\t\tcontinue;\n \t\t}\n \t\t// Check if the new range overlaps with a cached one\n-\t\tbool break_loop = false;\n \t\tswitch (new_file_range->GetOverlap(*it->second)) {\n \t\tcase CachedFileRangeOverlap::NONE:\n-\t\t\tbreak_loop = true; // We iterated past potential overlaps\n-\t\t\tbreak;\n+\t\t\tbreak; // No overlap, still useful\n \t\tcase CachedFileRangeOverlap::PARTIAL:\n \t\t\tbreak; // The newly created range does not fully contain this range, so it is still useful\n \t\tcase CachedFileRangeOverlap::FULL:\n@@ -315,9 +318,6 @@ BufferHandle CachingFileHandle::TryInsertFileRange(BufferHandle &pin, data_ptr_t\n \t\tdefault:\n \t\t\tthrow InternalException(\"Unknown CachedFileRangeOverlap\");\n \t\t}\n-\t\tif (break_loop) {\n-\t\t\tbreak;\n-\t\t}\n \n \t\t++it;\n \t}\n---\n"}], "ideal": "Title: Bugfixes\n\nBody: Fixes:\r\n1. https://github.com/duckdb/duckdb/issues/17682 (missed a `!`, used uninitialized variable in Parquet BSS encoder)\r\n2. https://github.com/duckdblabs/duckdb-internal/issues/4999 (`ExternalFileCache` assertion failure because we exited loop too early)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/persistent/physical_copy_to_file.cpp\n@@ -408,12 +408,9 @@ void CheckDirectory(FileSystem &fs, const string &file_path, CopyOverwriteMode o\n unique_ptr<GlobalSinkState> PhysicalCopyToFile::GetGlobalSinkState(ClientContext &context) const {\n \tif (partition_output || per_thread_output || rotate) {\n \t\tauto &fs = FileSystem::GetFileSystem(context);\n-\t\tif (fs.FileExists(file_path)) {\n-\t\t\t// the target file exists AND is a file (not a directory)\n-\t\t\tif (fs.IsRemoteFile(file_path)) {\n-\t\t\t\t// for remote files we cannot do anything - as we cannot delete the file\n-\t\t\t\tthrow IOException(\"Cannot write to \\\"%s\\\" - it exists and is a file, not a directory!\", file_path);\n-\t\t\t} else {\n+\t\tif (!fs.IsRemoteFile(file_path)) {\n+\t\t\tif (fs.FileExists(file_path)) {\n+\t\t\t\t// the target file exists AND is a file (not a directory)\n \t\t\t\t// for local files we can remove the file if OVERWRITE_OR_IGNORE is enabled\n \t\t\t\tif (overwrite_mode == CopyOverwriteMode::COPY_OVERWRITE) {\n \t\t\t\t\tfs.RemoveFile(file_path);\n---\n"}], "ideal": "Title: Partitioned copy: don't check if file exists for remote files\n\nBody: `FileExists` returns true for root buckets on S3 (e.g. `s3://root-bucket/`). This causes partitioned copy operations like the following to fail currently:\r\n\r\n```sql\r\ncopy (select 42 i, 1 p) to 's3://root-bucket/' (format parquet, partition_by p);\r\n-- Cannot write to \"s3://root-bucket/\" - it exists and is a file, not a directory!\r\n```\r\n\r\nThe check (\"is this a file or is this a directory\") doesn't really make sense on blob stores to begin with - so just skip it for remote files.\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/juliapkg/Project.toml\n@@ -1,7 +1,7 @@\n name = \"DuckDB\"\n uuid = \"d2f5444f-75bc-4fdf-ac35-56f514c445e1\"\n authors = [\"Mark Raasveldt <mark@duckdblabs.com\", \"Hannes M\u00fchleisen <hannes@duckdblabs.com>\"]\n-version = \"1.2.2\"\n+version = \"1.3.0\"\n \n [deps]\n DBInterface = \"a10d1c49-ce27-4219-8d33-6db1a4562965\"\n@@ -14,7 +14,7 @@ WeakRefStrings = \"ea10d353-3f73-51f8-a26c-33c1cb351aa5\"\n \n [compat]\n DBInterface = \"2.5\"\n-DuckDB_jll = \"1.2.2\"\n+DuckDB_jll = \"1.3.0\"\n FixedPointDecimals = \"0.4, 0.5, 0.6\"\n Tables = \"1.7\"\n WeakRefStrings = \"1.4\"\n---\n"}], "ideal": "Title: bump DuckDB_jll to v1.3.0\n\nBody: related Julia PR: https://github.com/JuliaPackaging/Yggdrasil/pull/11287\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "test/sql/table_function/test_range_function.test\n@@ -223,9 +223,9 @@ select * from generate_series(0, 10, 9223372036854775807);\n ----\n 0\n \n-query II\n+query II rowsort\n select * FROM generate_series(1, 3, 1) AS _(x), generate_series(x, 2, 1) AS __(y);\n ----\n-2\t2\n 1\t1\n 1\t2\n+2\t2\n---\n"}], "ideal": "Title: Add rowsort in generate_series test #43\n\nBody: Make the test non-sensitive to ordering, as set threads=1 gives correct answer but in different order. (discussed [here](https://github.com/duckdblabs/motherduck/issues/317))"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "test/sqlite/sqllogic_test_runner.cpp\n@@ -555,7 +555,7 @@ RequireResult SQLLogicTestRunner::CheckRequire(SQLLogicParser &parser, const vec\n \t\t}\n \t}\n \n-\tif (!config->options.autoload_known_extensions) {\n+\tif (!config->options.autoload_known_extensions || excluded_from_autoloading) {\n \t\tauto result = ExtensionHelper::LoadExtension(*db, param);\n \t\tif (result == ExtensionLoadResult::LOADED_EXTENSION) {\n \t\t\t// add the extension to the list of loaded extensions\n@@ -566,8 +566,6 @@ RequireResult SQLLogicTestRunner::CheckRequire(SQLLogicParser &parser, const vec\n \t\t\t// extension known but not build: skip this test\n \t\t\treturn RequireResult::MISSING;\n \t\t}\n-\t} else if (excluded_from_autoloading) {\n-\t\treturn RequireResult::MISSING;\n \t}\n \treturn RequireResult::PRESENT;\n }\n---\n"}], "ideal": "Title: Test runner: when excluded from autoloading, still attempt standard load\n\nBody: This allows to run more tests with `LOCAL_EXTENSION_REPO` flag enabled, given otherwise presence of any non-autoloadable extension would be the same as failure."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb.h\n@@ -3342,6 +3342,14 @@ Retrieves the extra info of the function as set in `duckdb_scalar_function_set_e\n */\n DUCKDB_C_API void *duckdb_scalar_function_get_extra_info(duckdb_function_info info);\n \n+/*!\n+Retrieves the extra info of the function as set in the bind info.\n+\n+* @param info The info object.\n+* @return The extra info.\n+*/\n+DUCKDB_C_API void *duckdb_scalar_function_bind_get_extra_info(duckdb_bind_info info);\n+\n /*!\n Gets the scalar function's bind data set by `duckdb_scalar_function_set_bind_data`.\n \n---\nsrc/include/duckdb/main/capi/extension_api.hpp\n@@ -485,6 +485,7 @@ typedef struct {\n \tvoid (*duckdb_scalar_function_set_bind_data)(duckdb_bind_info info, void *bind_data,\n \t                                             duckdb_delete_callback_t destroy);\n \tvoid *(*duckdb_scalar_function_get_bind_data)(duckdb_function_info info);\n+\tvoid *(*duckdb_scalar_function_bind_get_extra_info)(duckdb_bind_info info);\n \t// New string functions that are added\n \n \tchar *(*duckdb_value_to_string)(duckdb_value value);\n@@ -927,6 +928,7 @@ inline duckdb_ext_api_v1 CreateAPIv1() {\n \tresult.duckdb_scalar_function_get_client_context = duckdb_scalar_function_get_client_context;\n \tresult.duckdb_scalar_function_set_bind_data = duckdb_scalar_function_set_bind_data;\n \tresult.duckdb_scalar_function_get_bind_data = duckdb_scalar_function_get_bind_data;\n+\tresult.duckdb_scalar_function_bind_get_extra_info = duckdb_scalar_function_bind_get_extra_info;\n \tresult.duckdb_value_to_string = duckdb_value_to_string;\n \tresult.duckdb_create_map_value = duckdb_create_map_value;\n \tresult.duckdb_create_union_value = duckdb_create_union_value;\n---\nsrc/include/duckdb/main/capi/header_generation/apis/v1/unstable/new_scalar_function_functions.json\n@@ -6,6 +6,7 @@\n     \"duckdb_scalar_function_bind_set_error\",\n     \"duckdb_scalar_function_get_client_context\",\n     \"duckdb_scalar_function_set_bind_data\",\n-    \"duckdb_scalar_function_get_bind_data\"\n+    \"duckdb_scalar_function_get_bind_data\",\n+    \"duckdb_scalar_function_bind_get_extra_info\"\n   ]\n }\n---\nsrc/include/duckdb/main/capi/header_generation/functions/scalar_functions.json\n@@ -298,6 +298,23 @@\n                 \"return_value\": \"The extra info.\"\n             }\n         },\n+        {\n+            \"name\": \"duckdb_scalar_function_bind_get_extra_info\",\n+            \"return_type\": \"void *\",\n+            \"params\": [\n+                {\n+                    \"type\": \"duckdb_bind_info\",\n+                    \"name\": \"info\"\n+                }\n+            ],\n+            \"comment\": {\n+                \"description\": \"Retrieves the extra info of the function as set in the bind info.\\n\\n\",\n+                \"param_comments\": {\n+                    \"info\": \"The info object.\"\n+                },\n+                \"return_value\": \"The extra info.\"\n+            }\n+        },\n         {\n             \"name\": \"duckdb_scalar_function_get_bind_data\",\n             \"return_type\": \"void *\",\n---\nsrc/include/duckdb_extension.h\n@@ -558,6 +558,7 @@ typedef struct {\n \tvoid (*duckdb_scalar_function_set_bind_data)(duckdb_bind_info info, void *bind_data,\n \t                                             duckdb_delete_callback_t destroy);\n \tvoid *(*duckdb_scalar_function_get_bind_data)(duckdb_function_info info);\n+\tvoid *(*duckdb_scalar_function_bind_get_extra_info)(duckdb_bind_info info);\n #endif\n \n // New string functions that are added\n@@ -1013,11 +1014,12 @@ typedef struct {\n #define duckdb_get_table_names                  duckdb_ext_api.duckdb_get_table_names\n \n // Version unstable_new_scalar_function_functions\n-#define duckdb_scalar_function_set_bind           duckdb_ext_api.duckdb_scalar_function_set_bind\n-#define duckdb_scalar_function_set_bind_data      duckdb_ext_api.duckdb_scalar_function_set_bind_data\n-#define duckdb_scalar_function_bind_set_error     duckdb_ext_api.duckdb_scalar_function_bind_set_error\n-#define duckdb_scalar_function_get_bind_data      duckdb_ext_api.duckdb_scalar_function_get_bind_data\n-#define duckdb_scalar_function_get_client_context duckdb_ext_api.duckdb_scalar_function_get_client_context\n+#define duckdb_scalar_function_set_bind            duckdb_ext_api.duckdb_scalar_function_set_bind\n+#define duckdb_scalar_function_set_bind_data       duckdb_ext_api.duckdb_scalar_function_set_bind_data\n+#define duckdb_scalar_function_bind_set_error      duckdb_ext_api.duckdb_scalar_function_bind_set_error\n+#define duckdb_scalar_function_bind_get_extra_info duckdb_ext_api.duckdb_scalar_function_bind_get_extra_info\n+#define duckdb_scalar_function_get_bind_data       duckdb_ext_api.duckdb_scalar_function_get_bind_data\n+#define duckdb_scalar_function_get_client_context  duckdb_ext_api.duckdb_scalar_function_get_client_context\n \n // Version unstable_new_string_functions\n #define duckdb_value_to_string duckdb_ext_api.duckdb_value_to_string\n---\nsrc/main/capi/scalar_function-c.cpp\n@@ -227,6 +227,14 @@ void *duckdb_scalar_function_get_extra_info(duckdb_function_info info) {\n \treturn function_info.bind_data.info.extra_info;\n }\n \n+void *duckdb_scalar_function_bind_get_extra_info(duckdb_bind_info info) {\n+\tif (!info) {\n+\t\treturn nullptr;\n+\t}\n+\tauto &bind_info = GetCScalarFunctionBindInfo(info);\n+\treturn bind_info.bind_data.info.extra_info;\n+}\n+\n void *duckdb_scalar_function_get_bind_data(duckdb_function_info info) {\n \tif (!info) {\n \t\treturn nullptr;\n---\ntest/api/capi/capi_scalar_functions.cpp\n@@ -195,11 +195,12 @@ TEST_CASE(\"Test Scalar Functions - strings & extra_info\", \"[capi]\") {\n \tCAPITester tester;\n \tduckdb::unique_ptr<CAPIResult> result;\n \n-\tauto string_data = (char *)malloc(100);\n+\tauto string_data = reinterpret_cast<char *>(malloc(100));\n \tstrcpy(string_data, \"my_prefix\");\n+\tauto extra_info = reinterpret_cast<duckdb_function_info>(string_data);\n \n \tREQUIRE(tester.OpenDatabase(nullptr));\n-\tCAPIRegisterStringInfo(tester.connection, \"my_prefix\", (duckdb_function_info)string_data, free);\n+\tCAPIRegisterStringInfo(tester.connection, \"my_prefix\", extra_info, free);\n \n \t// now call it\n \tresult = tester.Query(\"SELECT my_prefix('hello_world')\");\n@@ -411,12 +412,19 @@ struct ConnectionIdStruct {\n };\n \n void GetConnectionIdBind(duckdb_bind_info info) {\n+\t// Get the extra info.\n+\tauto extra_info_ptr = duckdb_scalar_function_bind_get_extra_info(info);\n+\tauto extra_info = string(reinterpret_cast<const char *>(extra_info_ptr));\n+\tif (extra_info.empty()) {\n+\t\treturn;\n+\t}\n+\n \tduckdb_client_context context;\n \tduckdb_scalar_function_get_client_context(info, &context);\n-\n \tauto connection_id = duckdb_client_context_get_connection_id(context);\n \tduckdb_destroy_client_context(&context);\n \n+\t// Set the connection id.\n \tauto bind_data = reinterpret_cast<ConnectionIdStruct *>(malloc(sizeof(ConnectionIdStruct)));\n \tbind_data->connection_id = connection_id;\n \tduckdb_scalar_function_set_bind_data(info, bind_data, free);\n@@ -448,6 +456,12 @@ static void CAPIRegisterGetConnectionId(duckdb_connection connection) {\n \tduckdb_scalar_function_set_bind(function, GetConnectionIdBind);\n \tduckdb_scalar_function_set_function(function, GetConnectionId);\n \n+\t// Set some extra info to retrieve during binding.\n+\tauto string_data = reinterpret_cast<char *>(malloc(100));\n+\tstrcpy(string_data, \"my_prefix\");\n+\tauto extra_info = reinterpret_cast<duckdb_function_info>(string_data);\n+\tduckdb_scalar_function_set_extra_info(function, extra_info, free);\n+\n \t// Register and cleanup.\n \tstatus = duckdb_register_scalar_function(connection, function);\n \tREQUIRE(status == DuckDBSuccess);\n---\n"}], "ideal": "Title: [C API] Expose duckdb_scalar_function_bind_get_extra_info\n\nBody: ```c\r\n/*!\r\nRetrieves the extra info of the function as set in the bind info.\r\n* @param info The info object.\r\n* @return The extra info.\r\n*/\r\nDUCKDB_C_API void *duckdb_scalar_function_bind_get_extra_info(duckdb_bind_info info);\r\n```"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "test/sqlite/result_helper.cpp\n@@ -229,16 +229,18 @@ bool TestResultHelper::CheckQueryResult(const Query &query, ExecuteContext &cont\n \t\t\t\thash_compare_error = entry->second != hash_value;\n \t\t\t}\n \t\t}\n+\t\tstring expected_hash;\n \t\tif (result_is_hash) {\n+\t\t\texpected_hash = values[0];\n \t\t\tD_ASSERT(values.size() == 1);\n-\t\t\thash_compare_error = values[0] != hash_value;\n+\t\t\thash_compare_error = expected_hash != hash_value;\n \t\t}\n \t\tif (hash_compare_error) {\n \t\t\tQueryResult *expected_result = nullptr;\n \t\t\tif (runner.result_label_map.find(query_label) != runner.result_label_map.end()) {\n \t\t\t\texpected_result = runner.result_label_map[query_label].get();\n \t\t\t}\n-\t\t\tlogger.WrongResultHash(expected_result, result);\n+\t\t\tlogger.WrongResultHash(expected_result, result, expected_hash, hash_value);\n \t\t\treturn false;\n \t\t}\n \t\tREQUIRE(!hash_compare_error);\n---\ntest/sqlite/sqllogic_test_logger.cpp\n@@ -243,21 +243,29 @@ void SQLLogicTestLogger::SplitMismatch(idx_t row_number, idx_t expected_column_c\n \tPrintLineSep();\n }\n \n-void SQLLogicTestLogger::WrongResultHash(QueryResult *expected_result, MaterializedQueryResult &result) {\n+void SQLLogicTestLogger::WrongResultHash(QueryResult *expected_result, MaterializedQueryResult &result,\n+                                         const string &expected_hash, const string &actual_hash) {\n+\tPrintErrorHeader(\"Wrong result hash!\");\n+\tPrintLineSep();\n+\tPrintSQL();\n+\tPrintLineSep();\n+\tPrintHeader(\"Expected result:\");\n+\tPrintLineSep();\n \tif (expected_result) {\n \t\texpected_result->Print();\n \t} else {\n \t\tstd::cerr << \"???\" << std::endl;\n \t}\n-\tPrintErrorHeader(\"Wrong result hash!\");\n \tPrintLineSep();\n-\tPrintSQL();\n-\tPrintLineSep();\n-\tPrintHeader(\"Expected result:\");\n+\tPrintHeader(\"Expected result hash:\");\n+\tstd::cerr << expected_hash << std::endl;\n \tPrintLineSep();\n \tPrintHeader(\"Actual result:\");\n \tPrintLineSep();\n \tresult.Print();\n+\tPrintLineSep();\n+\tPrintHeader(\"Actual result hash\");\n+\tstd::cerr << actual_hash << std::endl;\n }\n \n void SQLLogicTestLogger::UnexpectedStatement(bool expect_ok, MaterializedQueryResult &result) {\n---\ntest/sqlite/sqllogic_test_logger.hpp\n@@ -46,7 +46,8 @@ class SQLLogicTestLogger {\n \tvoid ColumnCountMismatchCorrectResult(idx_t original_expected_columns, idx_t expected_column_count,\n \t                                      MaterializedQueryResult &result);\n \tvoid SplitMismatch(idx_t row_number, idx_t expected_column_count, idx_t split_count);\n-\tvoid WrongResultHash(QueryResult *expected_result, MaterializedQueryResult &result);\n+\tvoid WrongResultHash(QueryResult *expected_result, MaterializedQueryResult &result, const string &expected_hash,\n+\t                     const string &actual_hash);\n \tvoid UnexpectedStatement(bool expect_ok, MaterializedQueryResult &result);\n \tvoid ExpectedErrorMismatch(const string &expected_error, MaterializedQueryResult &result);\n \tvoid InternalException(MaterializedQueryResult &result);\n---\n"}], "ideal": "Title: Output hashes in unittest and fix order\n\nBody: Currently, no information about the hashes is shown when a unittest fails because of a hash value mismatch, even though this information is readily available.\r\n\r\nThis small PR aims to include this information in the output while also making it less confusing: before, it printed an 'Expected result' header without supplying any results (or rather, printing it in the previous lines before the introductory header). \r\n\r\n```\r\n\tPrintHeader(\"Expected result:\");\r\n\tPrintLineSep();        <--- There should be a result here\r\n\tPrintHeader(\"Actual result:\");\r\n```\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/src/arrow/arrow_array_stream.cpp\n@@ -447,6 +447,10 @@ py::object TransformFilterRecursive(TableFilter &filter, vector<string> column_r\n \t\t}\n \t\treturn TransformFilterRecursive(or_filter, column_ref, timezone_config, type);\n \t}\n+\tcase TableFilterType::DYNAMIC_FILTER: {\n+\t\t//! Ignore dynamic filters for now, not necessary for correctness\n+\t\treturn py::none();\n+\t}\n \tdefault:\n \t\tthrow NotImplementedException(\"Pushdown Filter Type %s is not currently supported in PyArrow Scans\",\n \t\t                              EnumUtil::ToString(filter.filter_type));\n---\ntools/pythonpkg/tests/fast/arrow/test_filter_pushdown.py\n@@ -1013,3 +1013,9 @@ def assert_equal_results(con, arrow_table, query):\n         assert_equal_results(duckdb_cursor, arrow_table, \"select * from {table} where a <= 'NaN'::FLOAT\")\n         assert_equal_results(duckdb_cursor, arrow_table, \"select * from {table} where a = 'NaN'::FLOAT\")\n         assert_equal_results(duckdb_cursor, arrow_table, \"select * from {table} where a != 'NaN'::FLOAT\")\n+\n+    def test_dynamic_filter(self, duckdb_cursor):\n+        t = pa.Table.from_pydict({\"a\": [3, 24, 234, 234, 234, 234, 234, 234, 234, 45, 2, 5, 2, 45]})\n+        duckdb_cursor.register(\"t\", t)\n+        res = duckdb_cursor.sql(\"SELECT a FROM t ORDER BY a LIMIT 11\").fetchall()\n+        assert len(res) == 11\n---\n"}], "ideal": "Title: [Python][Dev] Ignore `DYNAMIC_FILTER` TableFilters in filter pushdown\n\nBody: This PR fixes #17638 "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/optimizer/topn_optimizer.hpp\n@@ -8,6 +8,7 @@\n \n #pragma once\n \n+#include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/common/constants.hpp\"\n \n namespace duckdb {\n@@ -17,13 +18,18 @@ class Optimizer;\n \n class TopN {\n public:\n+\texplicit TopN(ClientContext &context);\n+\n \t//! Optimize ORDER BY + LIMIT to TopN\n \tunique_ptr<LogicalOperator> Optimize(unique_ptr<LogicalOperator> op);\n \t//! Whether we can perform the optimization on this operator\n-\tstatic bool CanOptimize(LogicalOperator &op);\n+\tstatic bool CanOptimize(LogicalOperator &op, optional_ptr<ClientContext> context = nullptr);\n \n private:\n \tvoid PushdownDynamicFilters(LogicalTopN &op);\n+\n+private:\n+\tClientContext &context;\n };\n \n } // namespace duckdb\n---\nsrc/optimizer/optimizer.cpp\n@@ -224,7 +224,7 @@ void Optimizer::RunBuiltInOptimizers() {\n \n \t// transform ORDER BY + LIMIT to TopN\n \tRunOptimizer(OptimizerType::TOP_N, [&]() {\n-\t\tTopN topn;\n+\t\tTopN topn(context);\n \t\tplan = topn.Optimize(std::move(plan));\n \t});\n \n---\nsrc/optimizer/topn_optimizer.cpp\n@@ -14,7 +14,10 @@\n \n namespace duckdb {\n \n-bool TopN::CanOptimize(LogicalOperator &op) {\n+TopN::TopN(ClientContext &context_p) : context(context_p) {\n+}\n+\n+bool TopN::CanOptimize(LogicalOperator &op, optional_ptr<ClientContext> context) {\n \tif (op.type == LogicalOperatorType::LOGICAL_LIMIT) {\n \t\tauto &limit = op.Cast<LogicalLimit>();\n \n@@ -28,14 +31,21 @@ bool TopN::CanOptimize(LogicalOperator &op) {\n \t\t}\n \n \t\tauto child_op = op.children[0].get();\n+\t\tif (context) {\n+\t\t\t// estimate child cardinality if the context is available\n+\t\t\tchild_op->EstimateCardinality(*context);\n+\t\t}\n \n-\t\tauto constant_limit = static_cast<double>(limit.limit_val.GetConstantValue());\n-\t\tauto child_card = static_cast<double>(child_op->estimated_cardinality);\n+\t\tif (child_op->has_estimated_cardinality) {\n+\t\t\t// only check if we should switch to full sorting if we have estimated cardinality\n+\t\t\tauto constant_limit = static_cast<double>(limit.limit_val.GetConstantValue());\n+\t\t\tauto child_card = static_cast<double>(child_op->estimated_cardinality);\n \n-\t\t// if the child cardinality is not 98 times more than the\n-\t\tbool limit_is_large = constant_limit > 5000;\n-\t\tif (constant_limit > child_card * 0.007 && limit_is_large) {\n-\t\t\treturn false;\n+\t\t\t// if the limit is > 0.7% of the child cardinality, sorting the whole table is faster\n+\t\t\tbool limit_is_large = constant_limit > 5000;\n+\t\t\tif (constant_limit > child_card * 0.007 && limit_is_large) {\n+\t\t\t\treturn false;\n+\t\t\t}\n \t\t}\n \n \t\twhile (child_op->type == LogicalOperatorType::LOGICAL_PROJECTION) {\n@@ -116,7 +126,7 @@ void TopN::PushdownDynamicFilters(LogicalTopN &op) {\n }\n \n unique_ptr<LogicalOperator> TopN::Optimize(unique_ptr<LogicalOperator> op) {\n-\tif (CanOptimize(*op)) {\n+\tif (CanOptimize(*op, &context)) {\n \n \t\tvector<unique_ptr<LogicalOperator>> projections;\n \n---\n"}], "ideal": "Title: Don't bail on TopN optimization if we don't have a cardinality\n\nBody: Fixes a bug introduced by https://github.com/duckdb/duckdb/pull/17140\r\n\r\nThe computation was wrong because it did not check `has_estimated_cardinality` before grabbing it. This PR computes the cardinality if it was not yet there. These fields should probably be changed to `optional_idx` in the future so this doesn't happen."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/parquet_reader.cpp\n@@ -1116,7 +1116,7 @@ void ParquetReader::InitializeScan(ClientContext &context, ParquetReaderScanStat\n \t\t\tstate.prefetch_mode = false;\n \t\t}\n \n-\t\tstate.file_handle = fs.OpenFile(file_handle->GetPath(), flags);\n+\t\tstate.file_handle = fs.OpenFile(file, flags);\n \t}\n \tstate.adaptive_filter.reset();\n \tstate.scan_filters.clear();\n---\n"}], "ideal": "Title: initialize the read with the OpenFile info and not just the path\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/4961\r\n\r\nAnother part of the fix is to fix the HTTPFileCache, but that needs to be done in the httpfs extension. \r\nThis uses the OpenFileInfo `file` to create the new file handle. This way, we also copy all of the extended properties of the OpenFileInfo like `validate_external_cache` and `file_size`. This makes it easier to prevent unnecessary extra (1) head requests on large files. But if there are multiple files that are all a bit larger, this these HEAD requests add up"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/main/secret/secret_manager.hpp\n@@ -222,6 +222,7 @@ class DefaultSecretGenerator : public DefaultGenerator {\n \tunique_ptr<CatalogEntry> CreateDefaultEntryInternal(const string &entry_name);\n \n \tSecretManager &secret_manager;\n+\tmutex lock;\n \tcase_insensitive_set_t persistent_secrets;\n };\n \n---\nsrc/main/secret/secret_manager.cpp\n@@ -646,6 +646,7 @@ DefaultSecretGenerator::DefaultSecretGenerator(Catalog &catalog, SecretManager &\n }\n \n unique_ptr<CatalogEntry> DefaultSecretGenerator::CreateDefaultEntryInternal(const string &entry_name) {\n+\tlock_guard<mutex> guard(lock);\n \tauto secret_lu = persistent_secrets.find(entry_name);\n \tif (secret_lu == persistent_secrets.end()) {\n \t\treturn nullptr;\n@@ -718,6 +719,7 @@ unique_ptr<CatalogEntry> DefaultSecretGenerator::CreateDefaultEntry(ClientContex\n vector<string> DefaultSecretGenerator::GetDefaultEntries() {\n \tvector<string> ret;\n \n+\tlock_guard<mutex> guard(lock);\n \tfor (const auto &res : persistent_secrets) {\n \t\tret.push_back(res);\n \t}\n---\n"}], "ideal": "Title: DefaultSecretGenerator: require lock for modifying persistent_secrets\n\nBody: `persistent_secrets` are modified - and we can load secrets from multiple connections, so this needs a lock."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/aggregate/physical_streaming_window.cpp\n@@ -4,7 +4,6 @@\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"duckdb/function/aggregate_function.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n-#include \"duckdb/planner/expression/bound_reference_expression.hpp\"\n #include \"duckdb/planner/expression/bound_window_expression.hpp\"\n \n namespace duckdb {\n@@ -413,6 +412,7 @@ void StreamingWindowState::AggregateState::Execute(ExecutionContext &context, Da\n \n \t// Compute the arguments\n \tauto &arg_chunk = aggr_state.arg_chunk;\n+\targ_chunk.Reset();\n \texecutor.Execute(input, arg_chunk);\n \targ_chunk.Flatten();\n \n---\ntest/sql/window/test_streaming_window.test\n@@ -413,3 +413,36 @@ select\n [{'key': A}]\n [{'key': A}, {'key': B}]\n [{'key': A}, {'key': B}, {'key': C}]\n+\n+# Reset argument test (fails in debug)\n+statement ok\n+CREATE TABLE issue17621(i INT, j INT, k INT);\n+\n+statement ok\n+INSERT INTO issue17621 VALUES (1,1,1),\n+\t(1,4,1),\n+\t(1,2,1),\n+\t(1,4,1),\n+\t(1,4,1),\n+\t(1,1,2),\n+\t(1,4,2),\n+\t(1,2,2),\n+\t(1,4,2),\n+\t(1,1,3),\n+\t(1,4,3),\n+\t(1,2,3),\n+\t(1,4,3),\n+\t(1,1,4),\n+\t(1,4,4),\n+\t(1,2,4),\n+\t(1,4,4);\n+\n+# No data because scan order is non-deterministic.\n+statement ok\n+SELECT \n+\tk, \n+\tSTDDEV_POP(i), \n+\tSUM(j), \n+\tSTDDEV_SAMP(k) OVER (ROWS UNBOUNDED PRECEDING) std_wf \n+FROM issue17621 \n+GROUP BY ROLLUP(k)\n---\n"}], "ideal": "Title: Issue #17621: Streaming Window Reset\n\nBody: * Call Reset before evaluation\r\n* Remove sort to detect hash scan order changes.\r\n* Remove output because repro query is non-deterministic.\r\n\r\n\r\nfixes: duckdb/duckdb#17621\r\nfixes: duckdblabs/duckdb-internal#4955"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/cast/vector_cast_helpers.cpp\n@@ -124,7 +124,8 @@ static string_t HandleString(Vector &vec, const char *buf, idx_t start, idx_t en\n \tbool escaped = false;\n \n \tbool quoted = false;\n-\tchar quote_char;\n+\t// Satisfy GCC warning about uninitialized variable\n+\tchar quote_char = '\\0';\n \tstack<char> scopes;\n \tfor (idx_t i = 0; i < length; i++) {\n \t\tauto current_char = buf[start + i];\n---\n"}], "ideal": "Title: chore: Fix initialization\n\nBody: Needed for duckdb-r 1.3.0."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types/row/tuple_data_segment.cpp\n@@ -26,7 +26,7 @@ static inline void SwapTupleDataChunk(TupleDataChunk &a, TupleDataChunk &b) noex\n \tstd::swap(a.lock, b.lock);\n }\n \n-TupleDataChunk::TupleDataChunk(TupleDataChunk &&other) noexcept {\n+TupleDataChunk::TupleDataChunk(TupleDataChunk &&other) noexcept : count(0) {\n \tSwapTupleDataChunk(*this, other);\n }\n \n---\n"}], "ideal": "Title: chore: Fix initialization\n\nBody: Needed for duckdb-r 1.3.0."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/parquet_float16.cpp\n@@ -11,7 +11,9 @@ float Float16ToFloat32(const uint16_t &float16_value) {\n \tuint32_t sign = float16_value >> 15;\n \tuint32_t exponent = (float16_value >> 10) & 0x1F;\n \tuint32_t fraction = (float16_value & 0x3FF);\n-\tuint32_t float32_value;\n+\t// Avoid strict aliasing issues and compiler warnings\n+\tuint32_t float32_value = 0;\n+\n \tif (exponent == 0) {\n \t\tif (fraction == 0) {\n \t\t\t// zero\n@@ -39,7 +41,7 @@ float Float16ToFloat32(const uint16_t &float16_value) {\n \t\tfloat32_value = (sign << 31) | ((exponent + (127 - 15)) << 23) | (fraction << 13);\n \t}\n \n-\treturn *reinterpret_cast<float *>(&float32_value);\n+\treturn Load<float>(const_data_ptr_cast(&float32_value));\n }\n \n } // namespace duckdb\n---\n"}], "ideal": "Title: chore: Fix strict aliasing warning on GCC\n\nBody: Needed for duckdb-r 1.3.0."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/shell/shell.cpp\n@@ -5032,7 +5032,8 @@ int SQLITE_CDECL wmain(int argc, wchar_t **wargv) {\n \t\t} else if (strcmp(z, \"-bail\") == 0) {\n \t\t\tbail_on_error = true;\n \t\t} else if (strcmp(z, \"-version\") == 0) {\n-\t\t\tprintf(\"%s %s\\n\", sqlite3_libversion(), sqlite3_sourceid());\n+\t\t\tprintf(\"%s (%s) %s\\n\", duckdb::DuckDB::LibraryVersion(), duckdb::DuckDB::ReleaseCodename(),\n+\t\t\t       duckdb::DuckDB::SourceID());\n \t\t\tfree(azCmd);\n \t\t\treturn 0;\n \t\t} else if (strcmp(z, \"-interactive\") == 0) {\n---\n"}], "ideal": "Title: CLI: Print codename for '-version'\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/4958"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/persistent/physical_batch_copy_to_file.cpp\n@@ -118,6 +118,7 @@ class FixedBatchCopyGlobalState : public GlobalSinkState {\n \t\t\top.function.copy_to_get_written_statistics(context, *op.bind_data, *global_state,\n \t\t\t                                           *written_file_info->file_stats);\n \t\t}\n+\t\tinitialized = true;\n \t}\n \n \tvoid AddBatchData(idx_t batch_index, unique_ptr<PreparedBatchData> new_batch, idx_t memory_usage) {\n---\n"}], "ideal": "Title: Actually initialize in batch copy to file\n\nBody: Otherwise the Parquet writer opens multiple file handles when `WRITE_EMPTY_FILE` is set to true"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/local_file_system.cpp\n@@ -838,21 +838,21 @@ static string AdditionalLockInfo(const std::wstring path) {\n \n \tstatus = RmStartSession(&session, 0, session_key);\n \tif (status != ERROR_SUCCESS) {\n-\t\treturn \"\";\n+\t\treturn string();\n \t}\n \n \tPCWSTR path_ptr = path.c_str();\n \tstatus = RmRegisterResources(session, 1, &path_ptr, 0, NULL, 0, NULL);\n \tif (status != ERROR_SUCCESS) {\n-\t\treturn \"\";\n+\t\treturn string();\n \t}\n \tUINT process_info_size_needed, process_info_size;\n \n \t// we first call with nProcInfo = 0 to find out how much to allocate\n \tprocess_info_size = 0;\n \tstatus = RmGetList(session, &process_info_size_needed, &process_info_size, NULL, &reason);\n \tif (status != ERROR_MORE_DATA || process_info_size_needed == 0) {\n-\t\treturn \"\";\n+\t\treturn string();\n \t}\n \n \t// allocate\n@@ -866,8 +866,7 @@ static string AdditionalLockInfo(const std::wstring path) {\n \t\treturn \"\";\n \t}\n \n-\tstring conflict_string = \"File is already open in \";\n-\n+\tstring conflict_string;\n \tfor (UINT process_idx = 0; process_idx < process_info_size; process_idx++) {\n \t\tstring process_name = WindowsUtil::UnicodeToUTF8(process_info[process_idx].strAppName);\n \t\tauto pid = process_info[process_idx].Process.dwProcessId;\n@@ -886,7 +885,10 @@ static string AdditionalLockInfo(const std::wstring path) {\n \t}\n \n \tRmEndSession(session);\n-\treturn conflict_string;\n+\tif (conflict_string.empty()) {\n+\t\treturn string();\n+\t}\n+\treturn \"File is already open in \" + conflict_string;\n }\n \n bool LocalFileSystem::IsPrivateFile(const string &path_p, FileOpener *opener) {\n@@ -950,12 +952,11 @@ unique_ptr<FileHandle> LocalFileSystem::OpenFile(const string &path_p, FileOpenF\n \t\t}\n \t\tauto error = LocalFileSystem::GetLastErrorAsString();\n \n-\t\tauto better_error = AdditionalLockInfo(unicode_path);\n-\t\tif (!better_error.empty()) {\n-\t\t\tthrow IOException(better_error);\n-\t\t} else {\n-\t\t\tthrow IOException(\"Cannot open file \\\"%s\\\": %s\", path.c_str(), error);\n+\t\tauto extended_error = AdditionalLockInfo(unicode_path);\n+\t\tif (!extended_error.empty()) {\n+\t\t\textended_error = \"\\n\" + extended_error;\n \t\t}\n+\t\tthrow IOException(\"Cannot open file \\\"%s\\\": %s%s\", path.c_str(), error, extended_error);\n \t}\n \tauto handle = make_uniq<WindowsFileHandle>(*this, path.c_str(), hFile, flags);\n \tif (flags.OpenForAppending()) {\n---\n"}], "ideal": "Title: Improve Windows lock conflict error\n\nBody: Previously the `AdditionalLockInfo` would *override* the original error, instead of providing extra info. In addition, when no PIDs were found, the error was replaced with `File is already open in `. This would cause the lock error to be rather non-descriptive:\r\n\r\n```\r\nIOException: File is already open in\r\n```\r\n\r\nThis PR fixes it so this is actually used as extra information in the error, and we only report PIDs if any are found."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/extension/extension_install.cpp\n@@ -52,7 +52,7 @@ string ExtensionHelper::ExtensionInstallDocumentationLink(const string &extensio\n \tstring link = \"https://duckdb.org/docs/stable/extensions/troubleshooting\";\n \n \tif (components.size() >= 2) {\n-\t\tlink += \"/?version=\" + components[0] + \"&platform=\" + components[1] + \"&extension=\" + extension_name;\n+\t\tlink += \"?version=\" + components[0] + \"&platform=\" + components[1] + \"&extension=\" + extension_name;\n \t}\n \n \treturn link;\n---\n"}], "ideal": "Title: fix extension troubleshooting link\n\nBody: Fix extensions troubleshooting link which had an extra `/` which resulted in a 404.\r\n\r\nBefore: https://duckdb.org/docs/stable/extensions/troubleshooting/?version=1f0067f1a5&platform=osx_arm64&extension=aws\r\n\r\nAfter: https://duckdb.org/docs/stable/extensions/troubleshooting?version=1f0067f1a5&platform=osx_arm64&extension=aws"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/icu/icu-timezone.cpp\n@@ -247,6 +247,10 @@ struct ICUToNaiveTimestamp : public ICUDateFunc {\n \t\tif (!input.context) {\n \t\t\tthrow InternalException(\"Missing context for TIMESTAMPTZ to TIMESTAMP cast.\");\n \t\t}\n+\t\tif (input.context->config.disable_timestamptz_casts) {\n+\t\t\tthrow BinderException(\"Casting from TIMESTAMP WITH TIME ZONE to TIMESTAMP without an explicit time zone \"\n+\t\t\t                      \"has been disabled  - use \\\"AT TIME ZONE ...\\\"\");\n+\t\t}\n \n \t\tauto cast_data = make_uniq<CastData>(make_uniq<BindData>(*input.context));\n \n---\ntest/sql/timezone/disable_timestamptz_casts.test\n@@ -2,11 +2,19 @@\n # description: Test for disabling timestamp => timestamptz casts\n # group: [timezone]\n \n+# Setting has no effect when ICU is not loaded as the two types are equivalent\n statement ok\n set disable_timestamptz_casts=false;\n \n-statement ok\n+query I\n select cast('2020-01-01T00:00:00'::timestamp as timestamptz)\n+----\n+2020-01-01 00:00:00+00\n+\n+query I\n+select cast('2020-01-01T15:00:00+0000'::timestamptz as timestamp)\n+----\n+2020-01-01 15:00:00\n \n statement ok\n set disable_timestamptz_casts=true;\n@@ -16,6 +24,12 @@ select cast('2020-01-01T00:00:00'::timestamp as timestamptz)\n ----\n 2020-01-01 00:00:00+00\n \n+query I\n+select cast('2020-01-01T15:00:00+0000'::timestamptz as timestamp)\n+----\n+2020-01-01 15:00:00\n+\n+# With ICU loaded, the casts are disabled in both directions when the flag is set.\n require icu\n \n statement ok\n@@ -24,12 +38,13 @@ set calendar='gregorian';\n statement ok\n set TimeZone='America/Los_Angeles';\n \n-\n statement ok\n set disable_timestamptz_casts=false;\n \n-statement ok\n+query I\n select cast('2020-01-01T00:00:00'::timestamp as timestamptz)\n+----\n+2020-01-01 00:00:00-08\n \n statement ok\n set disable_timestamptz_casts=true;\n@@ -38,3 +53,8 @@ statement error\n select cast('2020-01-01T00:00:00'::timestamp as timestamptz)\n ----\n Casting from TIMESTAMP to TIMESTAMP WITH TIME ZONE without an explicit time zone has been disabled\n+\n+statement error\n+select cast('2020-01-01T15:00:00+0000'::timestamptz as timestamp)\n+----\n+Casting from TIMESTAMP WITH TIME ZONE to TIMESTAMP without an explicit time zone has been disabled\n---\n"}], "ideal": "Title: Issue #17606: Disable TIMESTAMPTZ Casts\n\nBody: * Check flag in both directions\r\n* Clean up and extend tests\r\n\r\nfixes: duckdb#17606\r\nfixes: duckdblabs/duckdb-internal#4948"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/scalar/random/random.cpp\n@@ -29,7 +29,9 @@ struct ExtractTimestampUuidOperator {\n \t\t}\n \n \t\t// UUID v7 begins with a 48 bit big-endian Unix Epoch timestamp with millisecond granularity.\n-\t\tconst int64_t upper = input.upper;\n+\t\tint64_t upper = input.upper;\n+\t\t// flip the top byte\n+\t\tupper ^= NumericLimits<int64_t>::Minimum();\n \t\tint64_t unix_ts_milli = upper;\n \t\tunix_ts_milli = unix_ts_milli >> 16;\n \n---\nsrc/common/types/uuid.cpp\n@@ -210,7 +210,10 @@ hugeint_t UUIDv7::GenerateRandomUUID(RandomEngine &engine) {\n \t// Fill in variant field.\n \tbytes[8] = (bytes[8] & 0x3f) | 0x80;\n \n-\treturn Convert(bytes);\n+\t// Flip the top byte\n+\tauto result = Convert(bytes);\n+\tresult.upper ^= NumericLimits<int64_t>::Minimum();\n+\treturn result;\n }\n \n hugeint_t UUIDv7::GenerateRandomUUID() {\n---\ntest/sql/function/uuid/test_uuid_function.test\n@@ -34,9 +34,19 @@ SELECT uuid_extract_timestamp(uuidv4());\n ----\n Given UUID is with version 4, not version 7.\n \n-# Test timestamp extraction with UUID v7.\n-statement ok\n-SELECT uuid_extract_timestamp('81948644-15b2-7170-839a-7a01b9fb5951');\n+require icu\n \n statement ok\n-SELECT uuid_extract_timestamp(uuidv7());\n+SET TimeZone='UTC'\n+\n+# Test timestamp extraction with UUID v7.\n+query I\n+SELECT uuid_extract_timestamp('0196f97a-db14-71c3-9132-9f0b1334466f');\n+----\n+2025-05-22 19:31:40.436+00\n+\n+# verify the generated timestamp is consistent with NOW()\n+query I\n+SELECT datediff('month', uuid_extract_timestamp(uuidv7()), now());\n+----\n+0\n---\n"}], "ideal": "Title: Generate correct UUID v7\n\nBody: Fixes https://github.com/duckdb/duckdb/issues/17611\r\n\r\nWe were one bit off, unfortunately it was the most significant bit :)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "CMakeLists.txt\n@@ -241,13 +241,15 @@ endif()\n \n set(VERSIONING_TAG_MATCH \"v*.*.*\")\n ######\n+# MAIN_BRANCH_VERSIONING default should be 'TRUE' for main branch and feature branches\n+# MAIN_BRANCH_VERSIONING default should be 'FALSE' for release branches\n # MAIN_BRANCH_VERSIONING default value needs to keep in sync between:\n # - CMakeLists.txt\n # - scripts/amalgamation.py\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-option(MAIN_BRANCH_VERSIONING \"Versioning scheme for main branch\" TRUE)\n+option(MAIN_BRANCH_VERSIONING \"Versioning scheme for main branch\" FALSE)\n if(${MAIN_BRANCH_VERSIONING})\n   set(VERSIONING_TAG_MATCH \"v*.*.0\")\n endif()\n---\nscripts/amalgamation.py\n@@ -271,13 +271,19 @@ def git_commit_hash():\n \n \n ######\n+# MAIN_BRANCH_VERSIONING default should be 'True' for main branch and feature branches\n+# MAIN_BRANCH_VERSIONING default should be 'False' for release branches\n # MAIN_BRANCH_VERSIONING default value needs to keep in sync between:\n # - CMakeLists.txt\n # - scripts/amalgamation.py\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-main_branch_versioning = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n+    MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n+    MAIN_BRANCH_VERSIONING = True\n \n \n def git_dev_version():\n@@ -291,7 +297,7 @@ def git_dev_version():\n         else:\n             # not on a tag: increment the version by one and add a -devX suffix\n             # this needs to keep in sync with changes to CMakeLists.txt\n-            if main_branch_versioning == True:\n+            if MAIN_BRANCH_VERSIONING == True:\n                 # increment minor version\n                 version_splits[1] = str(int(version_splits[1]) + 1)\n             else:\n---\nscripts/package_build.py\n@@ -137,19 +137,25 @@ def get_relative_path(source_dir, target_file):\n \n \n ######\n+# MAIN_BRANCH_VERSIONING default should be 'True' for main branch and feature branches\n+# MAIN_BRANCH_VERSIONING default should be 'False' for release branches\n # MAIN_BRANCH_VERSIONING default value needs to keep in sync between:\n # - CMakeLists.txt\n # - scripts/amalgamation.py\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-main_branch_versioning = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n+    MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n+    MAIN_BRANCH_VERSIONING = True\n \n \n def get_git_describe():\n     override_git_describe = os.getenv('OVERRIDE_GIT_DESCRIBE') or ''\n     versioning_tag_match = 'v*.*.*'\n-    if main_branch_versioning:\n+    if MAIN_BRANCH_VERSIONING:\n         versioning_tag_match = 'v*.*.0'\n     # empty override_git_describe, either since env was empty string or not existing\n     # -> ask git (that can fail, so except in place)\n@@ -210,7 +216,7 @@ def git_dev_version():\n         else:\n             # not on a tag: increment the version by one and add a -devX suffix\n             # this needs to keep in sync with changes to CMakeLists.txt\n-            if main_branch_versioning == True:\n+            if MAIN_BRANCH_VERSIONING == True:\n                 # increment minor version\n                 version_splits[1] = str(int(version_splits[1]) + 1)\n             else:\n---\ntools/pythonpkg/setup.py\n@@ -381,15 +381,19 @@ def setup_data_files(data_files):\n import os\n \n ######\n+# MAIN_BRANCH_VERSIONING default should be 'True' for main branch and feature branches\n+# MAIN_BRANCH_VERSIONING default should be 'False' for release branches\n # MAIN_BRANCH_VERSIONING default value needs to keep in sync between:\n # - CMakeLists.txt\n # - scripts/amalgamation.py\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-\n-# Whether to use main branch versioning logic, defaults to True\n-MAIN_BRANCH_VERSIONING = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n+    MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n+    MAIN_BRANCH_VERSIONING = True\n \n \n def parse(root: str | Path, config: Configuration) -> ScmVersion | None:\n---\n"}], "ideal": "Title: Main branch versioning set to false\n\nBody: Set default value for `MAIN_BRANCH_VERSIONING` to false.\r\nNOTE: needs to be reverted on main branch:\r\n```\r\ngit revert 7cd2e2439017200753d120ec74b5575639b3844b\r\n```\r\n\r\nThis PR is part of a 2 step code change:\r\n1. minor resturcture, no logical change (https://github.com/duckdb/duckdb/pull/17601)\r\n2. set default for MAIN_BRANCH_VERSIONING to false (revert on main branch) **<----- this PR**"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "CMakeLists.txt\n@@ -241,6 +241,8 @@ endif()\n \n set(VERSIONING_TAG_MATCH \"v*.*.*\")\n ######\n+# MAIN_BRANCH_VERSIONING default should be 'TRUE' for main branch and feature branches\n+# MAIN_BRANCH_VERSIONING default should be 'FALSE' for release branches\n # MAIN_BRANCH_VERSIONING default value needs to keep in sync between:\n # - CMakeLists.txt\n # - scripts/amalgamation.py\n---\nscripts/amalgamation.py\n@@ -271,13 +271,19 @@ def git_commit_hash():\n \n \n ######\n+# MAIN_BRANCH_VERSIONING default should be 'True' for main branch and feature branches\n+# MAIN_BRANCH_VERSIONING default should be 'False' for release branches\n # MAIN_BRANCH_VERSIONING default value needs to keep in sync between:\n # - CMakeLists.txt\n # - scripts/amalgamation.py\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-main_branch_versioning = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+MAIN_BRANCH_VERSIONING = True\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n+    MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n+    MAIN_BRANCH_VERSIONING = True\n \n \n def git_dev_version():\n@@ -291,7 +297,7 @@ def git_dev_version():\n         else:\n             # not on a tag: increment the version by one and add a -devX suffix\n             # this needs to keep in sync with changes to CMakeLists.txt\n-            if main_branch_versioning == True:\n+            if MAIN_BRANCH_VERSIONING == True:\n                 # increment minor version\n                 version_splits[1] = str(int(version_splits[1]) + 1)\n             else:\n---\nscripts/package_build.py\n@@ -137,19 +137,25 @@ def get_relative_path(source_dir, target_file):\n \n \n ######\n+# MAIN_BRANCH_VERSIONING default should be 'True' for main branch and feature branches\n+# MAIN_BRANCH_VERSIONING default should be 'False' for release branches\n # MAIN_BRANCH_VERSIONING default value needs to keep in sync between:\n # - CMakeLists.txt\n # - scripts/amalgamation.py\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-main_branch_versioning = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+MAIN_BRANCH_VERSIONING = True\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n+    MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n+    MAIN_BRANCH_VERSIONING = True\n \n \n def get_git_describe():\n     override_git_describe = os.getenv('OVERRIDE_GIT_DESCRIBE') or ''\n     versioning_tag_match = 'v*.*.*'\n-    if main_branch_versioning:\n+    if MAIN_BRANCH_VERSIONING:\n         versioning_tag_match = 'v*.*.0'\n     # empty override_git_describe, either since env was empty string or not existing\n     # -> ask git (that can fail, so except in place)\n@@ -210,7 +216,7 @@ def git_dev_version():\n         else:\n             # not on a tag: increment the version by one and add a -devX suffix\n             # this needs to keep in sync with changes to CMakeLists.txt\n-            if main_branch_versioning == True:\n+            if MAIN_BRANCH_VERSIONING == True:\n                 # increment minor version\n                 version_splits[1] = str(int(version_splits[1]) + 1)\n             else:\n---\ntools/pythonpkg/setup.py\n@@ -381,15 +381,19 @@ def setup_data_files(data_files):\n import os\n \n ######\n+# MAIN_BRANCH_VERSIONING default should be 'True' for main branch and feature branches\n+# MAIN_BRANCH_VERSIONING default should be 'False' for release branches\n # MAIN_BRANCH_VERSIONING default value needs to keep in sync between:\n # - CMakeLists.txt\n # - scripts/amalgamation.py\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-\n-# Whether to use main branch versioning logic, defaults to True\n-MAIN_BRANCH_VERSIONING = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+MAIN_BRANCH_VERSIONING = True\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\":\n+    MAIN_BRANCH_VERSIONING = False\n+if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\":\n+    MAIN_BRANCH_VERSIONING = True\n \n \n def parse(root: str | Path, config: Configuration) -> ScmVersion | None:\n---\n"}], "ideal": "Title: minor restructure MAIN_BRANCH_VERSIONING\n\nBody: minor restructure, allows for easier adjustment of default for MAIN_BRANCH_VERSIONING\r\n\r\nThis PR is part of a 2 step code change:\r\n1. minor resturcture, no logical change <----- **this PR**\r\n2. set default for MAIN_BRANCH_VERSIONING to false (revert on main branch) https://github.com/duckdb/duckdb/pull/17602"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/sqlite3_api_wrapper/include/sqlite3.h\n@@ -579,7 +579,7 @@ SQLITE_API int sqlite3_exec(\n // we need to somehow communicate this through open_v2\n #define DUCKDB_UNSIGNED_EXTENSIONS   0x10000000\n #define DUCKDB_UNREDACTED_SECRETS    0x20000000\n-#define DUCKDB_LATEST_STORAGE_VERSION 0x30000000\n+#define DUCKDB_LATEST_STORAGE_VERSION 0x40000000\n \n /*\n ** CAPI3REF: Device Characteristics\n---\n"}], "ideal": "Title: [Dev][CLI] Use an unused bit for `DUCKDB_LATEST_STORAGE_VERSION`\n\nBody: This PR fixes https://github.com/duckdblabs/duckdb-internal/issues/4938"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/reader/struct_column_reader.cpp\n@@ -118,12 +118,21 @@ static bool TypeHasExactRowCount(const LogicalType &type) {\n }\n \n idx_t StructColumnReader::GroupRowsAvailable() {\n-\tfor (idx_t i = 0; i < child_readers.size(); i++) {\n-\t\tif (TypeHasExactRowCount(child_readers[i]->Type())) {\n-\t\t\treturn child_readers[i]->GroupRowsAvailable();\n+\tfor (auto &child : child_readers) {\n+\t\tif (!child) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tif (TypeHasExactRowCount(child->Type())) {\n+\t\t\treturn child->GroupRowsAvailable();\n+\t\t}\n+\t}\n+\tfor (auto &child : child_readers) {\n+\t\tif (!child) {\n+\t\t\tcontinue;\n \t\t}\n+\t\treturn child->GroupRowsAvailable();\n \t}\n-\treturn child_readers[0]->GroupRowsAvailable();\n+\tthrow InternalException(\"No projected columns in struct?\");\n }\n \n } // namespace duckdb\n---\nsrc/common/multi_file/multi_file_column_mapper.cpp\n@@ -297,6 +297,19 @@ ColumnMapResult MapColumnList(ClientContext &context, const MultiFileColumnDefin\n \t\t\tresult.column_map = Value::STRUCT(std::move(child_list));\n \t\t}\n \t}\n+\tif (is_selected && child_map.default_value) {\n+\t\t// we have default values at a previous level wrap it in a \"list\"\n+\t\tchild_list_t<LogicalType> default_type_list;\n+\t\tdefault_type_list.emplace_back(\"list\", child_map.default_value->return_type);\n+\t\tvector<unique_ptr<Expression>> default_expressions;\n+\t\tchild_map.default_value->alias = \"list\";\n+\t\tdefault_expressions.push_back(std::move(child_map.default_value));\n+\t\tauto default_type = LogicalType::STRUCT(std::move(default_type_list));\n+\t\tauto struct_pack_fun = StructPackFun::GetFunction();\n+\t\tauto bind_data = make_uniq<VariableReturnBindData>(default_type);\n+\t\tresult.default_value = make_uniq<BoundFunctionExpression>(std::move(default_type), std::move(struct_pack_fun),\n+\t\t                                                          std::move(default_expressions), std::move(bind_data));\n+\t}\n \tresult.column_index = make_uniq<ColumnIndex>(local_id.GetId(), std::move(child_indexes));\n \tresult.mapping = std::move(mapping);\n \treturn result;\n@@ -347,7 +360,7 @@ ColumnMapResult MapColumnMap(ClientContext &context, const MultiFileColumnDefini\n \n \tauto nested_mapper = mapper.Create(local_key_value.children);\n \tchild_list_t<Value> column_mapping;\n-\tunique_ptr<Expression> default_expression;\n+\tvector<unique_ptr<Expression>> default_expressions;\n \tunordered_map<idx_t, const_reference<ColumnIndex>> selected_children;\n \tif (global_index.HasChildren()) {\n \t\t//! FIXME: is this expected for maps??\n@@ -378,6 +391,10 @@ ColumnMapResult MapColumnMap(ClientContext &context, const MultiFileColumnDefini\n \t\t\t// found a column mapping for the component - emplace it\n \t\t\tcolumn_mapping.emplace_back(name, std::move(map_result.column_map));\n \t\t}\n+\t\tif (map_result.default_value) {\n+\t\t\tmap_result.default_value->alias = name;\n+\t\t\tdefault_expressions.push_back(std::move(map_result.default_value));\n+\t\t}\n \t}\n \n \tColumnMapResult result;\n@@ -393,6 +410,18 @@ ColumnMapResult MapColumnMap(ClientContext &context, const MultiFileColumnDefini\n \t\t\tresult.column_map = Value::STRUCT(std::move(child_list));\n \t\t}\n \t}\n+\tif (!default_expressions.empty()) {\n+\t\t// we have default values at a previous level wrap it in a \"list\"\n+\t\tchild_list_t<LogicalType> default_type_list;\n+\t\tfor (auto &expr : default_expressions) {\n+\t\t\tdefault_type_list.emplace_back(expr->GetAlias(), expr->return_type);\n+\t\t}\n+\t\tauto default_type = LogicalType::STRUCT(std::move(default_type_list));\n+\t\tauto struct_pack_fun = StructPackFun::GetFunction();\n+\t\tauto bind_data = make_uniq<VariableReturnBindData>(default_type);\n+\t\tresult.default_value = make_uniq<BoundFunctionExpression>(std::move(default_type), std::move(struct_pack_fun),\n+\t\t                                                          std::move(default_expressions), std::move(bind_data));\n+\t}\n \tvector<ColumnIndex> map_indexes;\n \tmap_indexes.emplace_back(0, std::move(child_indexes));\n \n@@ -563,11 +592,6 @@ unique_ptr<Expression> ConstructMapExpression(ClientContext &context, idx_t loca\n \t\t}\n \t\treturn expr;\n \t}\n-\t// struct column - generate a remap_struct - but only if we have any columns to remap\n-\tif (mapping.column_map.IsNull()) {\n-\t\t// no columns to map - emit the default value directly\n-\t\treturn std::move(mapping.default_value);\n-\t}\n \t// generate the remap_struct function call\n \tvector<unique_ptr<Expression>> children;\n \tchildren.push_back(std::move(expr));\n---\nsrc/function/scalar/struct/remap_struct.cpp\n@@ -401,6 +401,9 @@ struct RemapEntry {\n \t\t\tauto &child_types = StructType::GetChildTypes(default_type);\n \t\t\tfor (idx_t child_idx = 0; child_idx < child_types.size(); child_idx++) {\n \t\t\t\tauto &child_default = child_types[child_idx];\n+\t\t\t\tif (!result_entry->second.child_remaps || !entry->second.child_map) {\n+\t\t\t\t\tthrow BinderException(\"No child remaps found\");\n+\t\t\t\t}\n \t\t\t\tHandleDefault(child_idx, child_default.first, child_default.second, *entry->second.child_map,\n \t\t\t\t              *result_entry->second.child_remaps);\n \t\t\t}\n@@ -542,6 +545,10 @@ static unique_ptr<FunctionData> RemapStructBind(ClientContext &context, ScalarFu\n \t\tif (arg->return_type.id() == LogicalTypeId::UNKNOWN) {\n \t\t\tthrow ParameterNotResolvedException();\n \t\t}\n+\t\tif (arg->return_type.id() == LogicalTypeId::SQLNULL && arg_idx == 2) {\n+\t\t\t// remap target can be NULL\n+\t\t\tcontinue;\n+\t\t}\n \t\tif (!arg->return_type.IsNested()) {\n \t\t\tthrow BinderException(\"Struct remap can only remap nested types, not '%s'\", arg->return_type.ToString());\n \t\t} else if (arg->return_type.id() == LogicalTypeId::STRUCT && StructType::IsUnnamed(arg->return_type)) {\n@@ -571,11 +578,11 @@ static unique_ptr<FunctionData> RemapStructBind(ClientContext &context, ScalarFu\n \tauto target_map = RemapIndex::GetMap(to_type);\n \n \tValue remap_val = ExpressionExecutor::EvaluateScalar(context, *arguments[2]);\n-\tauto &remap_types = StructType::GetChildTypes(arguments[2]->return_type);\n \n \t// (recursively) generate the remap entries\n \tcase_insensitive_map_t<RemapEntry> remap_map;\n \tif (!remap_val.IsNull()) {\n+\t\tauto &remap_types = StructType::GetChildTypes(arguments[2]->return_type);\n \t\tauto &remap_values = StructValue::GetChildren(remap_val);\n \t\tfor (idx_t remap_idx = 0; remap_idx < remap_values.size(); remap_idx++) {\n \t\t\tauto &remap_val = remap_values[remap_idx];\n---\ntest/sql/types/struct/remap_struct.test\n@@ -5,6 +5,12 @@\n statement ok\n PRAGMA enable_verification\n \n+# remap without targets\n+query I\n+SELECT remap_struct({'i': 1, 'j': 2}, NULL::ROW(v2 INT), NULL, {'v2': NULL::INTEGER});\n+----\n+{'v2': NULL}\n+\n # basic binding error\n statement error\n select remap_struct(42, NULL::ROW(v1 INT, v2 INT, v3 INT), {'v1': 'j', 'v3': 'i'}, {'v2': NULL::INTEGER})\n@@ -195,3 +201,21 @@ SELECT remap_struct(\n  {'v2': NULL, 'v3': NULL::VARCHAR});\n ----\n Binder Error: Default key v2 does not match target type STRUCT(x INTEGER, y INTEGER) - add a cast\n+\n+statement error\n+SELECT remap_struct(\n+      [\n+          {\n+              'i': 1,\n+              'j': 42\n+          }\n+      ],\n+      NULL::STRUCT(k INT)[],\n+      {'list': 'list'},\n+      {\n+          'list': {\n+              'k': NULL\n+          }\n+      }\n+  );\n+----\n---\n"}], "ideal": "Title: MultiFileReader: Fix for handling nested list/map default values\n\nBody: Follow-up fixes from https://github.com/duckdb/duckdb/pull/17448\r\n\r\nNested default values were ignored, which caused this code not to correctly handle missing fields.\r\n\r\nIn addition, we can no longer directly emit the default value as the result since the default value is no longer equivalent to the actual value (e.g. the default value might be `{'list': {'k': NULL}}` when the actual value should be `[{'k': NULL}]`.) We fix this by allowing `remap_struct` to take `NULL` as remap target, in case there are zero columns used from the source struct."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "CMakeLists.txt\n@@ -247,7 +247,7 @@ set(VERSIONING_TAG_MATCH \"v*.*.*\")\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-option(MAIN_BRANCH_VERSIONING \"Versioning scheme for main branch\" TRUE)\n+option(MAIN_BRANCH_VERSIONING \"Versioning scheme for main branch\" FALSE)\n if(${MAIN_BRANCH_VERSIONING})\n   set(VERSIONING_TAG_MATCH \"v*.*.0\")\n endif()\n---\nscripts/amalgamation.py\n@@ -277,7 +277,7 @@ def git_commit_hash():\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-main_branch_versioning = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+main_branch_versioning = True if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\" else False\n \n \n def git_dev_version():\n---\nscripts/package_build.py\n@@ -143,7 +143,7 @@ def get_relative_path(source_dir, target_file):\n # - scripts/package_build.py\n # - tools/pythonpkg/setup.py\n ######\n-main_branch_versioning = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+main_branch_versioning = True if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\" else False\n \n \n def get_git_describe():\n---\ntools/pythonpkg/setup.py\n@@ -389,7 +389,7 @@ def setup_data_files(data_files):\n ######\n \n # Whether to use main branch versioning logic, defaults to True\n-MAIN_BRANCH_VERSIONING = False if os.getenv('MAIN_BRANCH_VERSIONING') == \"0\" else True\n+main_branch_versioning = True if os.getenv('MAIN_BRANCH_VERSIONING') == \"1\" else False\n \n \n def parse(root: str | Path, config: Configuration) -> ScmVersion | None:\n@@ -414,7 +414,7 @@ def parse(root: str | Path, config: Configuration) -> ScmVersion | None:\n             raise ValueError(f\"Invalid OVERRIDE_GIT_DESCRIBE format: {override}\")\n         return meta(tag=tag, distance=int(distance), config=config)\n \n-    versioning_tag_match = 'v*.*.0' if MAIN_BRANCH_VERSIONING else 'v*.*.*'\n+    versioning_tag_match = 'v*.*.0' if main_branch_versioning else 'v*.*.*'\n     git_describe_command = f\"git describe --tags --long --debug --match {versioning_tag_match}\"\n \n     try:\n@@ -449,7 +449,7 @@ def prefix_version(version):\n     major, minor, patch = [int(x) for x in str(version.tag).split('.')]\n     # Increment minor version if main_branch_versioning is enabled (default),\n     # otherwise increment patch version\n-    if MAIN_BRANCH_VERSIONING == True:\n+    if main_branch_versioning == True:\n         minor += 1\n         patch = 0\n     else:\n---\n"}], "ideal": "Title: Deactivate main branch versioning for release branch\n\nBody: set default value for MAIN_BRANCH_VERSIONING to false for the release branch\r\nhttps://github.com/duckdblabs/duckdb-internal/issues/4892"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/setup.py\n@@ -430,9 +430,21 @@ def prefix_version(version):\n             return version\n         return 'v' + version\n \n+    print(\"Version is\", version)\n+\n+    override = os.getenv('OVERRIDE_GIT_DESCRIBE')\n+    if override:\n+        formatted_version = version.format_with(\"{tag}\")\n+        print(\"formatted_version = \", formatted_version)\n+        print(\"Early return due to OVERRIDE_GIT_DESCRIBE\")\n+        return formatted_version\n+\n     # If we're exactly on a tag (dev_iteration = 0, dirty=False)\n     if version.exact:\n-        return version.format_with(\"{tag}\")\n+        formatted_version = version.format_with(\"{tag}\")\n+        print(\"formatted_version = \", formatted_version)\n+        print(\"Early return due to version.exact\")\n+        return formatted_version\n \n     major, minor, patch = [int(x) for x in str(version.tag).split('.')]\n     # Increment minor version if main_branch_versioning is enabled (default),\n---\n"}], "ideal": "Title: Fix setup.py to correctly handle OVERRIDE_GIT_DESCRIBE\n\nBody: With @Maxxen and @Tishj\r\n\r\nThis is currently tested at https://github.com/carlopi/duckdb/actions/runs/15160608765/job/42625694082"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/table/arrow_conversion.cpp\n@@ -349,26 +349,29 @@ static void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, const ArrowScan\n \t\t//! Have to check validity mask before setting this up\n \t\tidx_t offset = GetEffectiveOffset(array, parent_offset, scan_state, nested_offset) * fixed_size;\n \t\tauto cdata = ArrowBufferData<char>(array, 1);\n+\t\tauto blob_len = fixed_size;\n+\t\tauto result = FlatVector::GetData<string_t>(vector);\n \t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n \t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n+\t\t\t\toffset += blob_len;\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t\tauto bptr = cdata + offset;\n-\t\t\tauto blob_len = fixed_size;\n-\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n+\t\t\tresult[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n \t\t\toffset += blob_len;\n \t\t}\n \t} else if (size_type == ArrowVariableSizeType::NORMAL) {\n \t\tauto offsets =\n \t\t    ArrowBufferData<uint32_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\tauto cdata = ArrowBufferData<char>(array, 2);\n+\t\tauto result = FlatVector::GetData<string_t>(vector);\n \t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n \t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t\tauto bptr = cdata + offsets[row_idx];\n \t\t\tauto blob_len = offsets[row_idx + 1] - offsets[row_idx];\n-\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n+\t\t\tresult[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n \t\t}\n \t} else {\n \t\t//! Check if last offset is higher than max uint32\n@@ -378,13 +381,14 @@ static void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, const ArrowScan\n \t\tauto offsets =\n \t\t    ArrowBufferData<uint64_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\tauto cdata = ArrowBufferData<char>(array, 2);\n+\t\tauto result = FlatVector::GetData<string_t>(vector);\n \t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n \t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t\tauto bptr = cdata + offsets[row_idx];\n \t\t\tauto blob_len = offsets[row_idx + 1] - offsets[row_idx];\n-\t\t\tFlatVector::GetData<string_t>(vector)[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n+\t\t\tresult[row_idx] = StringVector::AddStringOrBlob(vector, bptr, blob_len);\n \t\t}\n \t}\n }\n---\ntools/pythonpkg/tests/fast/arrow/test_arrow_fixed_binary.py\n@@ -0,0 +1,21 @@\n+import pytest\n+\n+pa = pytest.importorskip(\"pyarrow\")\n+\n+\n+class TestArrowFixedBinary(object):\n+    def test_arrow_fixed_binary(self, duckdb_cursor):\n+        ids = [\n+            None,\n+            b'\\x66\\x4d\\xf4\\xae\\xb1\\x5c\\xb0\\x4a\\xdd\\x5d\\x1d\\x54',\n+            b'\\x66\\x4d\\xf4\\xf0\\xa3\\xfc\\xec\\x5b\\x26\\x81\\x4e\\x1d',\n+        ]\n+\n+        id_array = pa.array(ids, type=pa.binary(12))\n+        arrow_table = pa.Table.from_arrays([id_array], names=[\"id\"])\n+        res = duckdb_cursor.sql(\n+            \"\"\"\n+\t\t\tSELECT lower(hex(id)) as id FROM arrow_table\n+\t\t\"\"\"\n+        ).fetchall()\n+        assert res == [(None,), ('664df4aeb15cb04add5d1d54',), ('664df4f0a3fcec5b26814e1d',)]\n---\n"}], "ideal": "Title: [Dev] Fix Arrow fixed size binary reading\n\nBody: This PR fixes the issue reported by https://github.com/duckdb/duckdb/discussions/17570\r\n\r\nWhen a null is encountered, the offset wasn't being incremented before continuing."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types.cpp\n@@ -648,6 +648,34 @@ bool LogicalType::IsIntegral() const {\n \t}\n }\n \n+bool LogicalType::IsSigned() const {\n+\tswitch (id_) {\n+\tcase LogicalTypeId::TINYINT:\n+\tcase LogicalTypeId::SMALLINT:\n+\tcase LogicalTypeId::INTEGER:\n+\tcase LogicalTypeId::BIGINT:\n+\tcase LogicalTypeId::HUGEINT:\n+\t\treturn true;\n+\tdefault:\n+\t\tbreak;\n+\t}\n+\treturn false;\n+}\n+\n+bool LogicalType::IsUnsigned() const {\n+\tswitch (id_) {\n+\tcase LogicalTypeId::UTINYINT:\n+\tcase LogicalTypeId::USMALLINT:\n+\tcase LogicalTypeId::UINTEGER:\n+\tcase LogicalTypeId::UBIGINT:\n+\tcase LogicalTypeId::UHUGEINT:\n+\t\treturn true;\n+\tdefault:\n+\t\tbreak;\n+\t}\n+\treturn false;\n+}\n+\n bool LogicalType::IsFloating() const {\n \tswitch (id_) {\n \tcase LogicalTypeId::FLOAT:\n---\nsrc/include/duckdb/common/types.hpp\n@@ -349,6 +349,9 @@ struct LogicalType {\n \n \tDUCKDB_API void Verify() const;\n \n+\tDUCKDB_API bool IsSigned() const;\n+\tDUCKDB_API bool IsUnsigned() const;\n+\n \tDUCKDB_API bool IsValid() const;\n \tDUCKDB_API bool IsComplete() const;\n \n---\nsrc/planner/expression/bound_cast_expression.cpp\n@@ -191,6 +191,9 @@ bool BoundCastExpression::CastIsInvertible(const LogicalType &source_type, const\n \t\t\treturn false;\n \t\t}\n \t}\n+\tif (source_type.IsSigned() && target_type.IsUnsigned()) {\n+\t\treturn false;\n+\t}\n \treturn true;\n }\n \n---\ntest/optimizer/optimize_unsigned_vs_signed_ints.test\n@@ -0,0 +1,19 @@\n+# name: test/optimizer/optimize_unsigned_vs_signed_ints.test\n+# description: Test move constants\n+# group: [optimizer]\n+\n+statement ok\n+pragma enable_verification;\n+\n+query II\n+SELECT i, TRY_CAST(i - 4 AS UINT32) as b from range(10) tbl(i) where b < 2;\n+----\n+4\t0\n+5\t1\n+\n+\n+query III\n+SELECT *, b<2 FROM (SELECT i, TRY_CAST(i - 4 AS UINT32) as b from range(10) tbl(i) where b < 2);\n+----\n+4\t0\ttrue\n+5\t1\ttrue\n\\ No newline at end of file\n---\n"}], "ideal": "Title: Signed to Unsigned is not reversible\n\nBody: fixes https://github.com/duckdblabs/duckdb-internal/issues/4848 see issue for detailed explanation.\r\n\r\nMight not be the best fix I have other ideas. \r\nCasting from signed to unsiged theoretically works, but below gives an error\r\n```\r\nD select (i - 5)::UINT32 from range(10) tbl(i);\r\nConversion Error:\r\nType INT64 with value -5 can't be cast because the value is out of range for the destination type UINT32\r\n\r\nLINE 1: select (i - 5)::UINT32 from range(10) tbl(i);\r\n```\r\n\r\nThis means casting from INT32 to UINT32 should not be considered Invertible since it throws an error."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/include/thrift_tools.hpp\n@@ -119,6 +119,7 @@ struct ReadAheadBuffer {\n \t\t\t\tthrow std::runtime_error(\"Prefetch registered requested for bytes outside file\");\n \t\t\t}\n \t\t\tread_head.buffer_handle = file_handle.Read(read_head.buffer_ptr, read_head.size, read_head.location);\n+\t\t\tD_ASSERT(read_head.buffer_handle.IsValid());\n \t\t\tread_head.data_isset = true;\n \t\t}\n \t}\n@@ -141,8 +142,10 @@ class ThriftFileTransport : public duckdb_apache::thrift::transport::TVirtualTra\n \t\t\tif (!prefetch_buffer->data_isset) {\n \t\t\t\tprefetch_buffer->buffer_handle =\n \t\t\t\t    file_handle.Read(prefetch_buffer->buffer_ptr, prefetch_buffer->size, prefetch_buffer->location);\n+\t\t\t\tD_ASSERT(prefetch_buffer->buffer_handle.IsValid());\n \t\t\t\tprefetch_buffer->data_isset = true;\n \t\t\t}\n+\t\t\tD_ASSERT(prefetch_buffer->buffer_handle.IsValid());\n \t\t\tmemcpy(buf, prefetch_buffer->buffer_ptr + location - prefetch_buffer->location, len);\n \t\t} else if (prefetch_mode && len < PREFETCH_FALLBACK_BUFFERSIZE && len > 0) {\n \t\t\tPrefetch(location, MinValue<uint64_t>(PREFETCH_FALLBACK_BUFFERSIZE, file_handle.GetFileSize() - location));\n---\nsrc/storage/caching_file_system.cpp\n@@ -236,7 +236,7 @@ BufferHandle CachingFileHandle::TryReadFromCache(data_ptr_t &buffer, idx_t nr_by\n \tif (it != ranges.begin()) {\n \t\t--it;\n \t}\n-\tfor (it = ranges.begin(); it != ranges.end();) {\n+\twhile (it != ranges.end()) {\n \t\tif (it->second->location >= this_end) {\n \t\t\t// We're past the requested location\n \t\t\tbreak;\n@@ -285,12 +285,16 @@ BufferHandle CachingFileHandle::TryInsertFileRange(BufferHandle &pin, data_ptr_t\n \tauto &ranges = cached_file.Ranges(guard);\n \n \t// Start at lower_bound (first range with location not less than location of newly created range)\n-\tfor (auto it = ranges.lower_bound(location); it != ranges.end();) {\n+\tauto it = ranges.lower_bound(location);\n+\tif (it != ranges.begin()) {\n+\t\t--it;\n+\t}\n+\twhile (it != ranges.end()) {\n \t\tif (it->second->GetOverlap(*new_file_range) == CachedFileRangeOverlap::FULL) {\n \t\t\t// Another thread has read a range that fully contains the requested range in the meantime\n-\t\t\tpin = TryReadFromFileRange(guard, *it->second, buffer, nr_bytes, location);\n-\t\t\tif (pin.IsValid()) {\n-\t\t\t\treturn std::move(pin);\n+\t\t\tauto other_pin = TryReadFromFileRange(guard, *it->second, buffer, nr_bytes, location);\n+\t\t\tif (other_pin.IsValid()) {\n+\t\t\t\treturn other_pin;\n \t\t\t}\n \t\t\tit = ranges.erase(it);\n \t\t\tcontinue;\n@@ -314,8 +318,10 @@ BufferHandle CachingFileHandle::TryInsertFileRange(BufferHandle &pin, data_ptr_t\n \t\tif (break_loop) {\n \t\t\tbreak;\n \t\t}\n+\n \t\t++it;\n \t}\n+\tD_ASSERT(pin.IsValid());\n \n \t// Finally, insert newly created buffer into the map\n \tnew_file_range->AddCheckSum();\n---\nsrc/storage/external_file_cache.cpp\n@@ -50,6 +50,9 @@ void ExternalFileCache::CachedFileRange::VerifyCheckSum() {\n \t\treturn;\n \t}\n \tauto buffer_handle = block_handle->block_manager.buffer_manager.Pin(block_handle);\n+\tif (!buffer_handle.IsValid()) {\n+\t\treturn;\n+\t}\n \tD_ASSERT(checksum == Checksum(buffer_handle.Ptr(), nr_bytes));\n #endif\n }\n@@ -61,7 +64,7 @@ void ExternalFileCache::CachedFile::Verify(const unique_ptr<StorageLockKey> &gua\n #ifdef DEBUG\n \tfor (const auto &range1 : ranges) {\n \t\tfor (const auto &range2 : ranges) {\n-\t\t\tif (range1 == range2) {\n+\t\t\tif (range1.first == range2.first) {\n \t\t\t\tcontinue;\n \t\t\t}\n \t\t\tD_ASSERT(range1.second->GetOverlap(*range2.second) != CachedFileRangeOverlap::FULL);\n---\ntest/sql/storage/external_file_cache/external_file_cache_low_mem.test_slow\n@@ -0,0 +1,38 @@\n+# name: test/sql/storage/external_file_cache/external_file_cache_low_mem.test_slow\n+# description: Test the external file cache with low memory limit\n+# group: [external_file_cache]\n+\n+require tpch\n+\n+require parquet\n+\n+statement ok\n+call dbgen(sf=1);\n+\n+statement ok\n+export database '__TEST_DIR__/efc_tpch_sf1' (format parquet)\n+\n+foreach table customer lineitem nation orders part partsupp region supplier\n+\n+statement ok\n+drop table ${table};\n+\n+statement ok\n+CREATE VIEW ${table} AS FROM '__TEST_DIR__/efc_tpch_sf1/${table}.parquet';\n+\n+endloop\n+\n+statement ok\n+set prefetch_all_parquet_files=true;\n+\n+statement ok\n+pragma tpch(9);\n+\n+statement ok\n+set threads=1;\n+\n+statement ok\n+set memory_limit='100mb';\n+\n+statement ok\n+pragma tpch(9);\n---\n"}], "ideal": "Title: Fix issue with `ExternalFileCache` when data is evicted\n\nBody: Issue found internally while stress-testing"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/scan/physical_table_scan.cpp\n@@ -18,7 +18,7 @@ PhysicalTableScan::PhysicalTableScan(vector<LogicalType> types, TableFunction fu\n     : PhysicalOperator(PhysicalOperatorType::TABLE_SCAN, std::move(types), estimated_cardinality),\n       function(std::move(function_p)), bind_data(std::move(bind_data_p)), returned_types(std::move(returned_types_p)),\n       column_ids(std::move(column_ids_p)), projection_ids(std::move(projection_ids_p)), names(std::move(names_p)),\n-      table_filters(std::move(table_filters_p)), extra_info(extra_info), parameters(std::move(parameters_p)),\n+      table_filters(std::move(table_filters_p)), extra_info(std::move(extra_info)), parameters(std::move(parameters_p)),\n       virtual_columns(std::move(virtual_columns_p)) {\n }\n \n---\nsrc/execution/physical_plan/plan_get.cpp\n@@ -139,10 +139,10 @@ PhysicalOperator &PhysicalPlanGenerator::CreatePlan(LogicalGet &op) {\n \t// create the table scan node\n \tif (!op.function.projection_pushdown) {\n \t\t// function does not support projection pushdown\n-\t\tauto &table_scan = Make<PhysicalTableScan>(op.returned_types, op.function, std::move(op.bind_data),\n-\t\t                                           op.returned_types, column_ids, vector<column_t>(), op.names,\n-\t\t                                           std::move(table_filters), op.estimated_cardinality, op.extra_info,\n-\t\t                                           std::move(op.parameters), std::move(op.virtual_columns));\n+\t\tauto &table_scan = Make<PhysicalTableScan>(\n+\t\t    op.returned_types, op.function, std::move(op.bind_data), op.returned_types, column_ids, vector<column_t>(),\n+\t\t    op.names, std::move(table_filters), op.estimated_cardinality, std::move(op.extra_info),\n+\t\t    std::move(op.parameters), std::move(op.virtual_columns));\n \t\t// first check if an additional projection is necessary\n \t\tif (column_ids.size() == op.returned_types.size()) {\n \t\t\tbool projection_necessary = false;\n@@ -188,7 +188,7 @@ PhysicalOperator &PhysicalPlanGenerator::CreatePlan(LogicalGet &op) {\n \tauto &table_scan =\n \t    Make<PhysicalTableScan>(op.types, op.function, std::move(op.bind_data), op.returned_types, column_ids,\n \t                            op.projection_ids, op.names, std::move(table_filters), op.estimated_cardinality,\n-\t                            op.extra_info, std::move(op.parameters), std::move(op.virtual_columns));\n+\t                            std::move(op.extra_info), std::move(op.parameters), std::move(op.virtual_columns));\n \tauto &cast_table_scan = table_scan.Cast<PhysicalTableScan>();\n \tcast_table_scan.dynamic_filters = op.dynamic_filters;\n \tif (filter) {\n---\nsrc/include/duckdb/common/extra_operator_info.hpp\n@@ -21,15 +21,33 @@ class ExtraOperatorInfo {\n public:\n \tExtraOperatorInfo() : file_filters(\"\"), sample_options(nullptr) {\n \t}\n-\tExtraOperatorInfo(ExtraOperatorInfo &extra_info)\n-\t    : file_filters(extra_info.file_filters), sample_options(std::move(extra_info.sample_options)) {\n+\tExtraOperatorInfo(ExtraOperatorInfo &&extra_info) noexcept\n+\t    : file_filters(std::move(extra_info.file_filters)), sample_options(std::move(extra_info.sample_options)) {\n \t\tif (extra_info.total_files.IsValid()) {\n \t\t\ttotal_files = extra_info.total_files.GetIndex();\n \t\t}\n \t\tif (extra_info.filtered_files.IsValid()) {\n \t\t\tfiltered_files = extra_info.filtered_files.GetIndex();\n \t\t}\n \t}\n+\tExtraOperatorInfo &operator=(ExtraOperatorInfo &&extra_info) noexcept {\n+\t\tif (this != &extra_info) {\n+\t\t\tfile_filters = extra_info.file_filters;\n+\t\t\tif (extra_info.total_files.IsValid()) {\n+\t\t\t\ttotal_files = extra_info.total_files.GetIndex();\n+\t\t\t}\n+\t\t\tif (extra_info.filtered_files.IsValid()) {\n+\t\t\t\tfiltered_files = extra_info.filtered_files.GetIndex();\n+\t\t\t}\n+\t\t\tsample_options = std::move(extra_info.sample_options);\n+\t\t}\n+\t\treturn *this;\n+\t}\n+\n+\tbool operator==(const ExtraOperatorInfo &other) const {\n+\t\treturn file_filters == other.file_filters && total_files == other.total_files &&\n+\t\t       filtered_files == other.filtered_files && sample_options == other.sample_options;\n+\t}\n \n \t//! Filters that have been pushed down into the main file list\n \tstring file_filters;\n@@ -39,6 +57,9 @@ class ExtraOperatorInfo {\n \toptional_idx filtered_files;\n \t//! Sample options that have been pushed down into the table scan\n \tunique_ptr<SampleOptions> sample_options;\n+\n+\tvoid Serialize(Serializer &serializer) const;\n+\tstatic ExtraOperatorInfo Deserialize(Deserializer &deserializer);\n };\n \n } // namespace duckdb\n---\nsrc/include/duckdb/storage/serialization/nodes.json\n@@ -1106,5 +1106,34 @@\n     ],\n     \"pointer_type\": \"none\",\n     \"constructor\": [\"name\", \"type\"]\n+  },\n+  {\n+    \"class\": \"ExtraOperatorInfo\",\n+    \"includes\": [\n+      \"duckdb/common/extra_operator_info.hpp\"\n+    ],\n+    \"members\": [\n+      {\n+        \"id\": 100,\n+        \"name\": \"file_filters\",\n+        \"type\": \"string\"\n+      },\n+      {\n+        \"id\": 101,\n+        \"name\": \"total_files\",\n+        \"type\": \"optional_idx\"\n+      },\n+      {\n+        \"id\": 102,\n+        \"name\": \"filtered_files\",\n+        \"type\": \"optional_idx\"\n+      },\n+      {\n+        \"id\": 103,\n+        \"name\": \"sample_options\",\n+        \"type\": \"unique_ptr<SampleOptions>\"\n+      }\n+    ],\n+    \"pointer_type\": \"none\"\n   }\n ]\n---\nsrc/planner/operator/logical_get.cpp\n@@ -219,6 +219,7 @@ void LogicalGet::Serialize(Serializer &serializer) const {\n \t}\n \tserializer.WriteProperty(210, \"projected_input\", projected_input);\n \tserializer.WritePropertyWithDefault(211, \"column_indexes\", column_ids);\n+\tserializer.WritePropertyWithDefault(212, \"extra_info\", extra_info, ExtraOperatorInfo {});\n }\n \n unique_ptr<LogicalOperator> LogicalGet::Deserialize(Deserializer &deserializer) {\n@@ -256,6 +257,7 @@ unique_ptr<LogicalOperator> LogicalGet::Deserialize(Deserializer &deserializer)\n \t\t\tresult->column_ids.emplace_back(col_id);\n \t\t}\n \t}\n+\tresult->extra_info = deserializer.ReadPropertyWithExplicitDefault<ExtraOperatorInfo>(212, \"extra_info\", {});\n \tauto &context = deserializer.Get<ClientContext &>();\n \tvirtual_column_map_t virtual_columns;\n \tif (!has_serialize) {\n---\nsrc/storage/serialization/serialize_nodes.cpp\n@@ -35,6 +35,7 @@\n #include \"duckdb/parser/parsed_data/exported_table_data.hpp\"\n #include \"duckdb/common/column_index.hpp\"\n #include \"duckdb/common/table_column.hpp\"\n+#include \"duckdb/common/extra_operator_info.hpp\"\n \n namespace duckdb {\n \n@@ -302,6 +303,22 @@ ExportedTableInfo ExportedTableInfo::Deserialize(Deserializer &deserializer) {\n \treturn result;\n }\n \n+void ExtraOperatorInfo::Serialize(Serializer &serializer) const {\n+\tserializer.WritePropertyWithDefault<string>(100, \"file_filters\", file_filters);\n+\tserializer.WriteProperty<optional_idx>(101, \"total_files\", total_files);\n+\tserializer.WriteProperty<optional_idx>(102, \"filtered_files\", filtered_files);\n+\tserializer.WritePropertyWithDefault<unique_ptr<SampleOptions>>(103, \"sample_options\", sample_options);\n+}\n+\n+ExtraOperatorInfo ExtraOperatorInfo::Deserialize(Deserializer &deserializer) {\n+\tExtraOperatorInfo result;\n+\tdeserializer.ReadPropertyWithDefault<string>(100, \"file_filters\", result.file_filters);\n+\tdeserializer.ReadProperty<optional_idx>(101, \"total_files\", result.total_files);\n+\tdeserializer.ReadProperty<optional_idx>(102, \"filtered_files\", result.filtered_files);\n+\tdeserializer.ReadPropertyWithDefault<unique_ptr<SampleOptions>>(103, \"sample_options\", result.sample_options);\n+\treturn result;\n+}\n+\n void HivePartitioningIndex::Serialize(Serializer &serializer) const {\n \tserializer.WritePropertyWithDefault<string>(100, \"value\", value);\n \tserializer.WritePropertyWithDefault<idx_t>(101, \"index\", index);\n---\n"}], "ideal": "Title: Add (de)serialization for ExtraOperatorInfo\n\nBody: Takes over from https://github.com/duckdb/duckdb/pull/17518\r\n\r\n---\r\n\r\nIn the sample pushdown, the SampleOptions are added to the extra_infos of the LogicalGet, but go missing when serializing the operator. This PR fixes this by adding serialization and deserialization for ExtraOperatorInfo. I hope that nodes.json is the right place to define the serialization for this type.\r\n\r\nFurthermore, I changed the constructor of ExtraOperatorInfo to be a move constructor because it is moving a unique_ptr."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types/conflict_manager.cpp\n@@ -1,7 +1,7 @@\n #include \"duckdb/common/types/conflict_manager.hpp\"\n-#include \"duckdb/storage/index.hpp\"\n-#include \"duckdb/execution/index/art/art.hpp\"\n+\n #include \"duckdb/common/types/constraint_conflict_info.hpp\"\n+#include \"duckdb/execution/index/art/art.hpp\"\n \n namespace duckdb {\n \n@@ -57,10 +57,10 @@ void ConflictManager::FinishLookup() {\n \t}\n }\n \n-void ConflictManager::SetMode(ConflictManagerMode mode) {\n-\t// Only allow SCAN when we have conflict info\n-\tD_ASSERT(mode != ConflictManagerMode::SCAN || conflict_info != nullptr);\n-\tthis->mode = mode;\n+void ConflictManager::SetMode(const ConflictManagerMode mode_p) {\n+\t// Scanning requires conflict_info.\n+\tD_ASSERT(mode_p != ConflictManagerMode::SCAN || conflict_info != nullptr);\n+\tmode = mode_p;\n }\n \n void ConflictManager::AddToConflictSet(idx_t chunk_index) {\n@@ -86,20 +86,22 @@ void ConflictManager::AddConflictInternal(idx_t chunk_index, row_t row_id) {\n \n \t\t// We can be more efficient because we don't need to merge conflicts of multiple indexes\n \t\tauto &selection = InternalSelection();\n-\t\tauto &row_ids = InternalRowIds();\n-\t\tauto data = FlatVector::GetData<row_t>(row_ids);\n+\t\tauto &internal_row_ids = InternalRowIds();\n+\t\tauto data = FlatVector::GetData<row_t>(internal_row_ids);\n \t\tdata[selection.Count()] = row_id;\n \t\tselection.Append(chunk_index);\n-\t} else {\n-\t\tauto &intermediate = InternalIntermediate();\n-\t\tauto data = FlatVector::GetData<bool>(intermediate);\n-\t\t// Mark this index in the chunk as producing a conflict\n-\t\tdata[chunk_index] = true;\n-\t\tif (row_id_map.empty()) {\n-\t\t\trow_id_map.resize(input_size);\n-\t\t}\n-\t\trow_id_map[chunk_index] = row_id;\n+\t\treturn;\n \t}\n+\n+\tauto &intermediate = InternalIntermediate();\n+\tauto data = FlatVector::GetData<bool>(intermediate);\n+\t// Mark this index in the chunk as producing a conflict\n+\tdata[chunk_index] = true;\n+\tif (row_id_map.empty()) {\n+\t\trow_id_map.resize(input_size);\n+\t}\n+\t// FIXME: do we need to limit calls to AddConflictInternal to one per chunk?\n+\trow_id_map[chunk_index] = row_id;\n }\n \n bool ConflictManager::IsConflict(LookupResultType type) {\n@@ -128,11 +130,6 @@ bool ConflictManager::IsConflict(LookupResultType type) {\n \n bool ConflictManager::AddHit(idx_t chunk_index, row_t row_id) {\n \tD_ASSERT(chunk_index < input_size);\n-\t// First check if this causes a conflict\n-\tif (!IsConflict(LookupResultType::LOOKUP_HIT)) {\n-\t\treturn false;\n-\t}\n-\n \t// Then check if we should throw on a conflict\n \tif (ShouldThrow(chunk_index)) {\n \t\treturn true;\n@@ -149,11 +146,6 @@ bool ConflictManager::AddHit(idx_t chunk_index, row_t row_id) {\n \treturn false;\n }\n \n-bool ConflictManager::AddMiss(idx_t chunk_index) {\n-\tD_ASSERT(chunk_index < input_size);\n-\treturn IsConflict(LookupResultType::LOOKUP_MISS);\n-}\n-\n bool ConflictManager::AddNull(idx_t chunk_index) {\n \tD_ASSERT(chunk_index < input_size);\n \tif (!IsConflict(LookupResultType::LOOKUP_NULL)) {\n---\nsrc/execution/index/art/art.cpp\n@@ -937,9 +937,6 @@ void ART::VerifyLeaf(const Node &leaf, const ARTKey &key, optional_ptr<ART> dele\n \t\tauto this_row_id = leaf.GetRowId();\n \n \t\tif (deleted_row_id == this_row_id) {\n-\t\t\tif (manager.AddMiss(i)) {\n-\t\t\t\tconflict_idx = i;\n-\t\t\t}\n \t\t\treturn;\n \t\t}\n \n@@ -980,9 +977,6 @@ void ART::VerifyLeaf(const Node &leaf, const ARTKey &key, optional_ptr<ART> dele\n \n \tauto deleted_row_id = deleted_leaf->GetRowId();\n \tif (deleted_row_id == row_ids[0] || deleted_row_id == row_ids[1]) {\n-\t\tif (manager.AddMiss(i)) {\n-\t\t\tconflict_idx = i;\n-\t\t}\n \t\treturn;\n \t}\n \n@@ -1019,9 +1013,6 @@ void ART::VerifyConstraint(DataChunk &chunk, IndexAppendInfo &info, ConflictMana\n \n \t\tauto leaf = ARTOperator::Lookup(*this, tree, keys[i], 0);\n \t\tif (!leaf) {\n-\t\t\tif (manager.AddMiss(i)) {\n-\t\t\t\tconflict_idx = i;\n-\t\t\t}\n \t\t\tcontinue;\n \t\t}\n \t\tVerifyLeaf(*leaf, keys[i], delete_art, manager, conflict_idx, i);\n---\nsrc/include/duckdb/common/types/conflict_manager.hpp\n@@ -32,7 +32,6 @@ class ConflictManager {\n \n public:\n \t// These methods return a boolean indicating whether we should throw or not\n-\tbool AddMiss(idx_t chunk_index);\n \tbool AddHit(idx_t chunk_index, row_t row_id);\n \tbool AddNull(idx_t chunk_index);\n \tVerifyExistenceType LookupType() const;\n@@ -42,7 +41,7 @@ class ConflictManager {\n \tVector &RowIds();\n \tconst ConflictInfo &GetConflictInfo() const;\n \tvoid FinishLookup();\n-\tvoid SetMode(ConflictManagerMode mode);\n+\tvoid SetMode(const ConflictManagerMode mode_p);\n \n \t//! Returns a reference to all conflicts in this conflict manager.\n \tconst ManagedSelection &Conflicts() const;\n---\n"}], "ideal": "Title: Remove redundant code path in the ConflictManager\n\nBody: Related to https://github.com/duckdblabs/duckdb-internal/issues/4924"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/subquery/flatten_dependent_join.cpp\n@@ -657,6 +657,8 @@ unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal\n \t\tplan->children[0] =\n \t\t    PushDownDependentJoinInternal(std::move(plan->children[0]), parent_propagate_null_values, lateral_depth);\n \t\tauto left_binding = this->base_binding;\n+\t\tauto left_delim_offset = delim_offset;\n+\n \t\tplan->children[1] =\n \t\t    PushDownDependentJoinInternal(std::move(plan->children[1]), parent_propagate_null_values, lateral_depth);\n \t\tauto right_binding = this->base_binding;\n@@ -665,6 +667,7 @@ unique_ptr<LogicalOperator> FlattenDependentJoins::PushDownDependentJoinInternal\n \t\t// because the RIGHT binding might contain NULL values\n \t\tif (join.join_type == JoinType::LEFT) {\n \t\t\tthis->base_binding = left_binding;\n+\t\t\tdelim_offset = left_delim_offset;\n \t\t} else if (join.join_type == JoinType::RIGHT) {\n \t\t\tthis->base_binding = right_binding;\n \t\t\tdelim_offset += plan->children[0]->GetColumnBindings().size();\n---\ntest/fuzzer/public/correlate_left_join.test\n@@ -0,0 +1,24 @@\n+# name: test/fuzzer/public/correlate_left_join.test\n+# description: correlate subquery followed by a right join\n+# group: [public]\n+\n+statement ok\n+pragma enable_verification\n+\n+statement ok\n+CREATE TABLE t1(c0 BOOLEAN);\n+\n+statement ok\n+CREATE TABLE t2(c0 DOUBLE);\n+\n+statement ok\n+INSERT INTO t1(c0) VALUES (true);\n+\n+statement ok\n+INSERT INTO t2(c0) VALUES (0.2);\n+\n+# The build_side_probe_side optimizer will turn the right join into left join\n+query IIIII\n+SELECT * FROM t2, (SELECT 0) RIGHT JOIN ( SELECT 0 AS col_0, t2.c0 AS col_2) as subQuery1 ON ((subQuery1.col_0)>(subQuery1.col_2)) RIGHT JOIN t1 ON true;\n+----\n+0.2\tNULL\t0\t0.2\ttrue\n---\n"}], "ideal": "Title: Issue 17335\n\nBody: This PR fixes #17335. The issue occurs because the `delim_offset` is updated by the right child operator for left join, causing join conditions to obtain incorrect bindings."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/config/out_of_tree_extensions.cmake\n@@ -19,9 +19,8 @@\n duckdb_extension_load(httpfs\n     LOAD_TESTS\n     GIT_URL https://github.com/duckdb/duckdb-httpfs\n-    GIT_TAG eb1b04907c419d576f5fa4b34303810e8802e2f8\n+    GIT_TAG 217ec8e04f6ed419c866a6d2496aa15aace4382f\n     INCLUDE_DIR extension/httpfs/include\n-    APPLY_PATCHES\n     )\n \n ################# AVRO\n---\n.github/patches/extensions/httpfs/no_follow_no_keep_alive.patch\n@@ -1,13 +0,0 @@\n-diff --git a/extension/httpfs/httpfs_client.cpp b/extension/httpfs/httpfs_client.cpp\n-index 7a779ef..84eb457 100644\n---- a/extension/httpfs/httpfs_client.cpp\n-+++ b/extension/httpfs/httpfs_client.cpp\n-@@ -10,7 +10,7 @@ class HTTPFSClient : public HTTPClient {\n- public:\n- \tHTTPFSClient(HTTPFSParams &http_params, const string &proto_host_port) {\n- \t\tclient = make_uniq<duckdb_httplib_openssl::Client>(proto_host_port);\n--\t\tclient->set_follow_location(true);\n-+\t\tclient->set_follow_location(http_params.follow_location);\n- \t\tclient->set_keep_alive(http_params.keep_alive);\n- \t\tif (!http_params.ca_cert_file.empty()) {\n- \t\t\tclient->set_ca_cert_path(http_params.ca_cert_file.c_str());\n---\ntest/sql/secrets/secret_types_function.test\n@@ -14,7 +14,7 @@ require no_extension_autoloading \"EXPECTED: The duckdb_secret_types() function d\n query III\n FROM duckdb_secret_types() WHERE type IN ['s3', 'r2', 'gcs', 'http'] ORDER BY type\n ----\n-gcs\tconfig\t(empty)\n+gcs\tconfig\thttpfs\n http\tconfig\t(empty)\n-r2\tconfig\t(empty)\n-s3\tconfig\t\t(empty)\n\\ No newline at end of file\n+r2\tconfig\thttpfs\n+s3\tconfig\thttpfs\n---\n"}], "ideal": "Title: Remove patch and bump httpfs\n\nBody: This includes both removing patch from https://github.com/duckdb/duckdb/pull/17527 and fix from https://github.com/duckdb/duckdb-httpfs/pull/57"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/scalar/strftime_format.cpp\n@@ -921,11 +921,13 @@ bool StrpTimeFormat::Parse(const char *data, size_t size, ParseResult &result, b\n \t\tif (numeric_width[i] > 0) {\n \t\t\t// numeric specifier: parse a number\n \t\t\tuint64_t number = 0;\n+\t\t\tint digits = 0;\n \t\t\tsize_t start_pos = pos;\n \t\t\tsize_t end_pos = start_pos + UnsafeNumericCast<size_t>(numeric_width[i]);\n \t\t\twhile (pos < size && pos < end_pos && StringUtil::CharacterIsDigit(data[pos])) {\n \t\t\t\tnumber = number * 10 + UnsafeNumericCast<uint64_t>(data[pos]) - '0';\n \t\t\t\tpos++;\n+\t\t\t\t++digits;\n \t\t\t}\n \t\t\tif (pos == start_pos) {\n \t\t\t\t// expected a number here\n@@ -1083,16 +1085,25 @@ bool StrpTimeFormat::Parse(const char *data, size_t size, ParseResult &result, b\n \t\t\t\tresult_data[5] = UnsafeNumericCast<int32_t>(number);\n \t\t\t\tbreak;\n \t\t\tcase StrTimeSpecifier::NANOSECOND_PADDED:\n+\t\t\t\tfor (; digits < numeric_width[i]; ++digits) {\n+\t\t\t\t\tnumber *= 10;\n+\t\t\t\t}\n \t\t\t\tD_ASSERT(number < Interval::NANOS_PER_SEC); // enforced by the length of the number\n \t\t\t\t// nanoseconds\n \t\t\t\tresult_data[6] = UnsafeNumericCast<int32_t>(number);\n \t\t\t\tbreak;\n \t\t\tcase StrTimeSpecifier::MICROSECOND_PADDED:\n+\t\t\t\tfor (; digits < numeric_width[i]; ++digits) {\n+\t\t\t\t\tnumber *= 10;\n+\t\t\t\t}\n \t\t\t\tD_ASSERT(number < Interval::MICROS_PER_SEC); // enforced by the length of the number\n \t\t\t\t// nanoseconds\n \t\t\t\tresult_data[6] = UnsafeNumericCast<int32_t>(number * Interval::NANOS_PER_MICRO);\n \t\t\t\tbreak;\n \t\t\tcase StrTimeSpecifier::MILLISECOND_PADDED:\n+\t\t\t\tfor (; digits < numeric_width[i]; ++digits) {\n+\t\t\t\t\tnumber *= 10;\n+\t\t\t\t}\n \t\t\t\tD_ASSERT(number < Interval::MSECS_PER_SEC); // enforced by the length of the number\n \t\t\t\t// nanoseconds\n \t\t\t\tresult_data[6] = UnsafeNumericCast<int32_t>(number * Interval::NANOS_PER_MSEC);\n---\ntest/sql/function/timestamp/test_strptime.test\n@@ -331,6 +331,22 @@ select strptime('2020-12-30 23:25:58.745232-04', '%Y-%m-%d %H:%M:%S.%f%z');\n ----\n 2020-12-31 03:25:58.745232+00\n \n+# Right side padding of fractional seconds\n+query I\n+select strptime('2024-11-28 23:59:00.3', '%Y-%m-%d %H:%M:%S.%g');\n+----\n+2024-11-28 23:59:00.3\n+\n+query I\n+select strptime('2024-11-28 23:59:00.312', '%Y-%m-%d %H:%M:%S.%f');\n+----\n+2024-11-28 23:59:00.312\n+\n+query I\n+select strptime('2024-11-28 23:59:00.312457', '%Y-%m-%d %H:%M:%S.%n');\n+----\n+2024-11-28 23:59:00.312457\n+\n # infinities should always parse\n \n query I\n---\n"}], "ideal": "Title: Issue #17537: Fractional Second Padding\n\nBody: * Right fill %g, %f and %n with there are fewer digits than the field width\r\n\r\nfixes: duckdb#17537\r\nfixes: duckdblabs/duckdb-internal#4914"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/secret/secret_manager.cpp\n@@ -75,14 +75,15 @@ void SecretManager::LoadSecretStorage(unique_ptr<SecretStorage> storage) {\n \n void SecretManager::LoadSecretStorageInternal(unique_ptr<SecretStorage> storage) {\n \tif (secret_storages.find(storage->GetName()) != secret_storages.end()) {\n-\t\tthrow InternalException(\"Secret Storage with name '%s' already registered!\", storage->GetName());\n+\t\tthrow InvalidConfigurationException(\"Secret Storage with name '%s' already registered!\", storage->GetName());\n \t}\n \n \t// Check for tie-break offset collisions to ensure we can always tie-break cleanly\n \tfor (const auto &storage_ptr : secret_storages) {\n \t\tif (storage_ptr.second->tie_break_offset == storage->tie_break_offset) {\n-\t\t\tthrow InternalException(\"Failed to load secret storage '%s', tie break score collides with '%s'\",\n-\t\t\t                        storage->GetName(), storage_ptr.second->GetName());\n+\t\t\tthrow InvalidConfigurationException(\n+\t\t\t    \"Failed to load secret storage '%s', tie break score collides with '%s'\", storage->GetName(),\n+\t\t\t    storage_ptr.second->GetName());\n \t\t}\n \t}\n \n@@ -118,7 +119,7 @@ unique_ptr<BaseSecret> SecretManager::DeserializeSecret(Deserializer &deserializ\n \t}\n \n \tif (!deserialized_type.deserializer) {\n-\t\tthrow InternalException(\n+\t\tthrow InvalidConfigurationException(\n \t\t    \"Attempted to deserialize secret type '%s' which does not have a deserialization method\", type);\n \t}\n \n@@ -351,7 +352,7 @@ unique_ptr<SecretEntry> SecretManager::GetSecretByName(CatalogTransaction transa\n \t\tauto lookup = storage_ref.get().GetSecretByName(name, &transaction);\n \t\tif (lookup) {\n \t\t\tif (found) {\n-\t\t\t\tthrow InternalException(\n+\t\t\t\tthrow InvalidConfigurationException(\n \t\t\t\t    \"Ambiguity detected for secret name '%s', secret occurs in multiple storage backends.\", name);\n \t\t\t}\n \n---\n"}], "ideal": "Title: change exception type to not be an internal exception\n\nBody: Fixes nightly CI failures when running `relassert` build with `CRASH_ON_ASSERT`"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/settings/custom_settings.cpp\n@@ -196,6 +196,9 @@ void AllowedDirectoriesSetting::SetGlobal(DatabaseInstance *db, DBConfig &config\n \tif (!config.options.enable_external_access) {\n \t\tthrow InvalidInputException(\"Cannot change allowed_directories when enable_external_access is disabled\");\n \t}\n+\tif (!config.file_system) {\n+\t\tthrow InvalidInputException(\"Cannot change/set allowed_directories before the database is started\");\n+\t}\n \tconfig.options.allowed_directories.clear();\n \tauto &list = ListValue::GetChildren(input);\n \tfor (auto &val : list) {\n@@ -226,6 +229,10 @@ void AllowedPathsSetting::SetGlobal(DatabaseInstance *db, DBConfig &config, cons\n \tif (!config.options.enable_external_access) {\n \t\tthrow InvalidInputException(\"Cannot change allowed_paths when enable_external_access is disabled\");\n \t}\n+\tif (!config.file_system) {\n+\t\tthrow InvalidInputException(\"Cannot change/set allowed_paths before the database is started\");\n+\t}\n+\n \tconfig.options.allowed_paths.clear();\n \tauto &list = ListValue::GetChildren(input);\n \tfor (auto &val : list) {\n---\ntest/api/test_config.cpp\n@@ -38,6 +38,29 @@ TEST_CASE(\"Test DB config configuration\", \"[api]\") {\n \t}\n }\n \n+TEST_CASE(\"Test allowed options\", \"[api]\") {\n+\tcase_insensitive_map_t<Value> config_dict;\n+\tstring option;\n+\n+\tSECTION(\"allowed_directories\") {\n+\t\tconfig_dict.emplace(\"allowed_directories\", Value::LIST({Value(\"test\")}));\n+\t\toption = \"allowed_directories\";\n+\t}\n+\tSECTION(\"allowed_paths\") {\n+\t\tconfig_dict.emplace(\"allowed_paths\", Value::LIST({Value(\"test\")}));\n+\t\toption = \"allowed_paths\";\n+\t}\n+\n+\ttry {\n+\t\tDBConfig config(config_dict, false);\n+\t} catch (std::exception &ex) {\n+\t\tErrorData error_data(ex);\n+\t\tREQUIRE(error_data.Type() == ExceptionType::INVALID_INPUT);\n+\t\tREQUIRE(error_data.RawMessage() ==\n+\t\t        StringUtil::Format(\"Cannot change/set %s before the database is started\", option));\n+\t}\n+}\n+\n TEST_CASE(\"Test user_agent\", \"[api]\") {\n \t{\n \t\t// Default duckdb_api is cpp\n---\n"}], "ideal": "Title: [Dev] Fix `allowed_directories` crash\n\nBody: This PR fixes https://github.com/duckdb/duckdb/issues/17128\r\n\r\nProblem is that we were assuming the `config.file_system` to be set, which is not the case when the DBConfig is not associated with a database yet."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/table/table_scan.cpp\n@@ -344,6 +344,18 @@ unique_ptr<GlobalTableFunctionState> DuckIndexScanInitGlobal(ClientContext &cont\n                                                              unsafe_vector<row_t> &row_ids) {\n \tauto g_state = make_uniq<DuckIndexScanState>(context, input.bind_data.get());\n \tif (!row_ids.empty()) {\n+\t\t// Duplicate-eliminate row IDs.\n+\t\tunordered_set<row_t> row_id_set;\n+\t\tauto it = row_ids.begin();\n+\t\twhile (it != row_ids.end()) {\n+\t\t\tif (row_id_set.find(*it) == row_id_set.end()) {\n+\t\t\t\trow_id_set.insert(*it++);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t// Found a duplicate.\n+\t\t\tit = row_ids.erase(it);\n+\t\t}\n+\n \t\tstd::sort(row_ids.begin(), row_ids.end());\n \t\tg_state->row_ids = std::move(row_ids);\n \t}\n---\ntest/sql/index/art/scan/test_art_scan_duplicate_filters.test\n@@ -0,0 +1,61 @@\n+# name: test/sql/index/art/scan/test_art_scan_duplicate_filters.test\n+# description: Test index scans with duplicate filters.\n+# group: [scan]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE t_1 (fIdx VARCHAR, sIdx UUID,);\n+\n+statement ok\n+CREATE TABLE t_3 (fIdx VARCHAR, sIdx UUID);\n+\n+statement ok\n+CREATE TABLE t_4 (sIdx UUID);\n+\n+statement ok\n+CREATE TABLE t_5 (sIdx UUID);\n+\n+statement ok\n+CREATE UNIQUE INDEX _pk_idx_t_5 ON t_5 (sIdx);\n+\n+statement ok\n+INSERT INTO t_4 (sIdx) VALUES\n+('1381e0ce-6b3e-43f5-9536-5e7af3a512a5'::UUID),\n+('6880cdba-09f5-3c4f-8eb8-391aefdd8052'::UUID),\n+('a3e876dd-5e50-3af7-9649-689fd938daeb'::UUID),\n+('e0abc0d3-63be-41d8-99ca-b1269ed153a8'::UUID);\n+\n+statement ok\n+INSERT INTO t_5 (sIdx) VALUES\n+('a3e876dd-5e50-3af7-9649-689fd938daeb'::UUID),\n+('e0abc0d3-63be-41d8-99ca-b1269ed153a8'::UUID),\n+('91c79790-5828-45f2-ad88-50e9b541cc05'::UUID),\n+('1381e0ce-6b3e-43f5-9536-5e7af3a512a5'::UUID),\n+('d9f29fa5-2051-3ea4-9d9e-15c3698b4bb8'::UUID),\n+('6880cdba-09f5-3c4f-8eb8-391aefdd8052'::UUID),\n+('3239280c-2204-3e60-b3a1-3ad3acc8fd59'::UUID),\n+('2fced822-342d-344a-aa34-2707e593be52'::UUID);\n+\n+query I\n+WITH\n+\tcte_5 AS (\n+\t\tSELECT sIdx FROM t_4 ANTI JOIN t_3 USING (sIdx)\n+\t),\n+\tcte_6 AS MATERIALIZED (\n+\t\tSELECT\n+\t\t\tCOALESCE(cte_5.sIdx, t_1.sIdx) AS sIdx,\n+\t\t\tCOALESCE(t_1.fIdx, cte_5.sIdx::VARCHAR) AS fIdx,\n+\t\tFROM cte_5 FULL JOIN t_1 USING (sIdx)\n+\t),\n+\tcte_7 AS (\n+\t\tSELECT t_5.sIdx, FROM t_5\n+\t\tWHERE sIdx IN (SELECT sIdx FROM cte_6)\n+\t)\n+SELECT fIdx, FROM cte_6 JOIN cte_7 USING (sIdx) ORDER BY fIdx;\n+----\n+1381e0ce-6b3e-43f5-9536-5e7af3a512a5\n+6880cdba-09f5-3c4f-8eb8-391aefdd8052\n+a3e876dd-5e50-3af7-9649-689fd938daeb\n+e0abc0d3-63be-41d8-99ca-b1269ed153a8\n\\ No newline at end of file\n---\n"}], "ideal": "Title: [Fix] duplicate filters during index scans\n\nBody: See https://github.com/duckdb/duckdb/pull/17540\r\n\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/attached_database.cpp\n@@ -119,14 +119,14 @@ AttachedDatabase::AttachedDatabase(DatabaseInstance &db, Catalog &catalog_p, Sto\n                                    ClientContext &context, string name_p, AttachInfo &info, AttachOptions &options)\n     : CatalogEntry(CatalogType::DATABASE_ENTRY, catalog_p, std::move(name_p)), db(db), parent_catalog(&catalog_p),\n       storage_extension(&storage_extension_p) {\n-\tStorageExtensionInfo *storage_info = storage_extension->storage_info.get();\n-\tcatalog = storage_extension->attach(storage_info, context, *this, name, info, options.access_mode);\n-\n \tif (options.access_mode == AccessMode::READ_ONLY) {\n \t\ttype = AttachedDatabaseType::READ_ONLY_DATABASE;\n \t} else {\n \t\ttype = AttachedDatabaseType::READ_WRITE_DATABASE;\n \t}\n+\n+\tStorageExtensionInfo *storage_info = storage_extension->storage_info.get();\n+\tcatalog = storage_extension->attach(storage_info, context, *this, name, info, options.access_mode);\n \tif (!catalog) {\n \t\tthrow InternalException(\"AttachedDatabase - attach function did not return a catalog\");\n \t}\n---\n"}], "ideal": "Title: Attach after setting database type\n\nBody: This allows attach to change the read properties of the attached database"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/http/http_util.cpp\n@@ -264,10 +264,73 @@ void HTTPUtil::ParseHTTPProxyHost(string &proxy_value, string &hostname_out, idx\n \t}\n }\n \n-void HTTPUtil::DecomposeURL(const string &url, string &path_out, string &proto_host_port_out) {\n-\tif (url.rfind(\"http://\", 0) != 0 && url.rfind(\"https://\", 0) != 0) {\n-\t\tthrow IOException(\"URL needs to start with http:// or https://\");\n+namespace {\n+\n+enum class URISchemeType { HTTP, HTTPS, NONE, OTHER };\n+\n+struct URISchemeDetectionResult {\n+\tstring lower_scheme;\n+\tURISchemeType scheme_type = URISchemeType::NONE;\n+};\n+\n+bool IsValidSchemeChar(char c) {\n+\treturn std::isalnum(c) || c == '+' || c == '.' || c == '-';\n+}\n+\n+//! See https://datatracker.ietf.org/doc/html/rfc3986#section-3.1\n+URISchemeDetectionResult DetectURIScheme(const string &uri) {\n+\tURISchemeDetectionResult result;\n+\tauto colon_pos = uri.find(':');\n+\n+\t// No colon or it's before any non-scheme content\n+\tif (colon_pos == string::npos || colon_pos == 0) {\n+\t\tresult.lower_scheme = \"\";\n+\t\tresult.scheme_type = URISchemeType::NONE;\n+\t\treturn result;\n+\t}\n+\n+\tif (!std::isalpha(uri[0])) {\n+\t\t//! Scheme names consist of a sequence of characters beginning with a letter\n+\t\tresult.lower_scheme = \"\";\n+\t\tresult.scheme_type = URISchemeType::NONE;\n+\t\treturn result;\n \t}\n+\n+\t// Validate scheme characters\n+\tfor (size_t i = 1; i < colon_pos; ++i) {\n+\t\tif (!IsValidSchemeChar(uri[i])) {\n+\t\t\t//! Scheme can't contain this character, assume the URI has no scheme\n+\t\t\tresult.lower_scheme = \"\";\n+\t\t\tresult.scheme_type = URISchemeType::NONE;\n+\t\t\treturn result;\n+\t\t}\n+\t}\n+\n+\tstring scheme = uri.substr(0, colon_pos);\n+\tresult.lower_scheme = StringUtil::Lower(scheme);\n+\n+\tif (result.lower_scheme == \"http\") {\n+\t\tresult.scheme_type = URISchemeType::HTTP;\n+\t\treturn result;\n+\t}\n+\tif (result.lower_scheme == \"https\") {\n+\t\tresult.scheme_type = URISchemeType::HTTPS;\n+\t\treturn result;\n+\t}\n+\tresult.scheme_type = URISchemeType::OTHER;\n+\treturn result;\n+}\n+\n+} // namespace\n+\n+void HTTPUtil::DecomposeURL(const string &input, string &path_out, string &proto_host_port_out) {\n+\tauto detection_result = DetectURIScheme(input);\n+\tauto url = input;\n+\tif (detection_result.scheme_type == URISchemeType::NONE) {\n+\t\t//! Assume it's HTTP\n+\t\turl = \"http://\" + url;\n+\t}\n+\n \tauto slash_pos = url.find('/', 8);\n \tif (slash_pos == string::npos) {\n \t\tthrow IOException(\"URL needs to contain a '/' after the host\");\n---\n"}], "ideal": "Title: [HTTPUtil] Let requests made through the `HTTPUtil` interface accept URI's without a scheme.\n\nBody: This PR fixes https://github.com/duckdblabs/duckdb-internal/issues/4902\r\n\r\nWhen a URI without a scheme is passed, we default to the  `http` scheme.\r\nUnknown schemes are let through, so paths like `localhost` and other defined hosts continue to be allowed."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/binder/tableref/bind_showref.cpp\n@@ -13,8 +13,8 @@\n namespace duckdb {\n \n struct BaseTableColumnInfo {\n-\toptional_ptr<TableCatalogEntry> table;\n-\toptional_ptr<const ColumnDefinition> column;\n+\toptional_ptr<TableCatalogEntry> table = nullptr;\n+\toptional_ptr<const ColumnDefinition> column = nullptr;\n };\n \n BaseTableColumnInfo FindBaseTableColumn(LogicalOperator &op, ColumnBinding binding) {\n@@ -32,8 +32,12 @@ BaseTableColumnInfo FindBaseTableColumn(LogicalOperator &op, ColumnBinding bindi\n \t\tif (!get.projection_ids.empty()) {\n \t\t\tthrow InternalException(\"Projection ids should not exist here\");\n \t\t}\n-\t\tresult.table = table;\n \t\tauto base_column_id = get.GetColumnIds()[binding.column_index];\n+\t\tif (base_column_id.IsVirtualColumn()) {\n+\t\t\t//! Virtual column (like ROW_ID) does not have a ColumnDefinition entry in the TableCatalogEntry\n+\t\t\treturn result;\n+\t\t}\n+\t\tresult.table = table;\n \t\tresult.column = &table->GetColumn(LogicalIndex(base_column_id.GetPrimaryIndex()));\n \t\treturn result;\n \t}\n---\ntest/sql/show_select/describe_rowid.test\n@@ -0,0 +1,26 @@\n+# name: test/sql/show_select/describe_rowid.test\n+# group: [show_select]\n+\n+statement ok\n+create table sometable (\n+\tcolumn1 varchar\n+);\n+\n+statement ok\n+insert into sometable values\n+\t('abc');\n+\n+statement ok\n+create view someview as select\n+\trowid as table_rowid,\n+\t*\n+from sometable;\n+\n+statement ok\n+select * from someview;\n+\n+query II\n+select column_name, column_type from (describe someview);\n+----\n+table_rowid\tBIGINT\n+column1\tVARCHAR\n---\n"}], "ideal": "Title: [Dev] Fix crash when describing a table with a virtual column\n\nBody: This PR fixes https://github.com/duckdblabs/duckdb-internal/issues/4680\r\n\r\nThis issue arose because of https://github.com/duckdb/duckdb/pull/16248"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/storage/storage_manager.hpp\n@@ -110,7 +110,11 @@ class StorageManager {\n \tvoid SetStorageVersion(idx_t version) {\n \t\tstorage_version = version;\n \t}\n+\tbool HasStorageVersion() const {\n+\t\treturn storage_version.IsValid();\n+\t}\n \tidx_t GetStorageVersion() const {\n+\t\tD_ASSERT(HasStorageVersion());\n \t\treturn storage_version.GetIndex();\n \t}\n \n---\nsrc/planner/binder/statement/bind_create_table.cpp\n@@ -21,6 +21,7 @@\n #include \"duckdb/catalog/catalog_entry/schema_catalog_entry.hpp\"\n #include \"duckdb/parser/parsed_expression_iterator.hpp\"\n #include \"duckdb/storage/data_table.hpp\"\n+#include \"duckdb/storage/storage_manager.hpp\"\n \n namespace duckdb {\n \n@@ -34,6 +35,30 @@ static void CreateColumnDependencyManager(BoundCreateTableInfo &info) {\n \t}\n }\n \n+static void VerifyCompressionType(optional_ptr<StorageManager> storage_manager, DBConfig &config,\n+                                  BoundCreateTableInfo &info) {\n+\tauto &base = info.base->Cast<CreateTableInfo>();\n+\tfor (auto &col : base.columns.Logical()) {\n+\t\tauto compression_type = col.CompressionType();\n+\t\tif (CompressionTypeIsDeprecated(compression_type, storage_manager)) {\n+\t\t\tthrow BinderException(\"Can't compress using user-provided compression type '%s', that type is deprecated \"\n+\t\t\t                      \"and only has decompress support\",\n+\t\t\t                      CompressionTypeToString(compression_type));\n+\t\t}\n+\t\tconst auto &logical_type = col.GetType();\n+\t\tauto physical_type = logical_type.InternalType();\n+\t\tif (compression_type == CompressionType::COMPRESSION_AUTO) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tauto compression_method = config.GetCompressionFunction(compression_type, physical_type);\n+\t\tif (!compression_method) {\n+\t\t\tthrow BinderException(\n+\t\t\t    \"Can't compress column \\\"%s\\\" with type '%s' (physical: %s) using compression type '%s'\", col.Name(),\n+\t\t\t    logical_type.ToString(), EnumUtil::ToString(physical_type), CompressionTypeToString(compression_type));\n+\t\t}\n+\t}\n+}\n+\n vector<unique_ptr<BoundConstraint>> Binder::BindConstraints(ClientContext &context,\n                                                             const vector<unique_ptr<Constraint>> &constraints,\n                                                             const string &table_name, const ColumnList &columns) {\n@@ -551,6 +576,11 @@ unique_ptr<BoundCreateTableInfo> Binder::BindCreateTableInfo(unique_ptr<CreateIn\n \tauto &base = info->Cast<CreateTableInfo>();\n \tauto result = make_uniq<BoundCreateTableInfo>(schema, std::move(info));\n \tauto &dependencies = result->dependencies;\n+\tauto &catalog = schema.ParentCatalog();\n+\toptional_ptr<StorageManager> storage_manager;\n+\tif (catalog.IsDuckCatalog() && !catalog.InMemory()) {\n+\t\tstorage_manager = StorageManager::Get(catalog);\n+\t}\n \n \tvector<unique_ptr<BoundConstraint>> bound_constraints;\n \tif (base.query) {\n@@ -603,6 +633,9 @@ unique_ptr<BoundCreateTableInfo> Binder::BindCreateTableInfo(unique_ptr<CreateIn\n \t\t\t}\n \t\t\tdependencies.AddDependency(entry);\n \t\t});\n+\n+\t\tauto &config = DBConfig::Get(catalog.GetAttached());\n+\t\tVerifyCompressionType(storage_manager, config, *result);\n \t\tCreateColumnDependencyManager(*result);\n \t\t// bind the generated column expressions\n \t\tBindGeneratedColumns(*result);\n---\nsrc/storage/table/row_group.cpp\n@@ -1012,14 +1012,16 @@ bool RowGroup::HasUnloadedDeletes() const {\n RowGroupWriteData RowGroup::WriteToDisk(RowGroupWriter &writer) {\n \tvector<CompressionType> compression_types;\n \tcompression_types.reserve(columns.size());\n+\n \tfor (idx_t column_idx = 0; column_idx < GetColumnCount(); column_idx++) {\n \t\tauto &column = GetColumn(column_idx);\n \t\tif (column.count != this->count) {\n \t\t\tthrow InternalException(\"Corrupted in-memory column - column with index %llu has misaligned count (row \"\n \t\t\t                        \"group has %llu rows, column has %llu)\",\n \t\t\t                        column_idx, this->count.load(), column.count.load());\n \t\t}\n-\t\tcompression_types.push_back(writer.GetColumnCompressionType(column_idx));\n+\t\tauto compression_type = writer.GetColumnCompressionType(column_idx);\n+\t\tcompression_types.push_back(compression_type);\n \t}\n \n \tRowGroupWriteInfo info(writer.GetPartialBlockManager(), compression_types, writer.GetCheckpointType());\n---\ntest/sql/create/create_table_compression.test\n@@ -39,12 +39,6 @@ CREATE TABLE T (a INTEGER NOT NULL USING COMPRESSION RLE)\n statement ok\n DROP TABLE T\n \n-statement ok\n-CREATE TABLE T (a INTEGER USING COMPRESSION RLE, b VARCHAR USING COMPRESSION RLE )\n-\n-statement ok\n-DROP TABLE T\n-\n statement ok\n CREATE TABLE T (a INTEGER USING COMPRESSION RLE, b VARCHAR )\n \n---\ntest/sql/storage/compact_block_size/create_table_compression.test\n@@ -37,12 +37,6 @@ CREATE TABLE T (a INTEGER NOT NULL USING COMPRESSION RLE)\n statement ok\n DROP TABLE T\n \n-statement ok\n-CREATE TABLE T (a INTEGER USING COMPRESSION RLE, b VARCHAR USING COMPRESSION RLE )\n-\n-statement ok\n-DROP TABLE T\n-\n statement ok\n CREATE TABLE T (a INTEGER USING COMPRESSION RLE, b VARCHAR )\n \n---\ntest/sql/storage/compression/test_using_compression.test\n@@ -0,0 +1,26 @@\n+# name: test/sql/storage/compression/test_using_compression.test\n+# group: [compression]\n+\n+load __TEST_DIR__/using_compression_test readwrite v1.0.0\n+\n+# Can't use CHIMP because it's deprecated\n+statement error\n+CREATE OR REPLACE TABLE t(\n+\tx VARCHAR USING COMPRESSION chimp\n+);\n+----\n+Binder Error: Can't compress using user-provided compression type 'Chimp', that type is deprecated and only has decompress support\n+\n+# Can't use Dictionary because the column is not VARCHAR\n+statement error\n+CREATE OR REPLACE TABLE t(\n+\tx BIGINT USING COMPRESSION Dictionary\n+);\n+----\n+Binder Error: Can't compress column \"x\" with type 'BIGINT' (physical: INT64) using compression type 'Dictionary'\n+\n+# Can use Dictionary for a VARCHAR column\n+statement ok\n+CREATE OR REPLACE TABLE t(\n+\tx VARCHAR USING COMPRESSION Dictionary\n+);\n---\n"}], "ideal": "Title: [Dev] Can't use `USING COMPRESSION` with a deprecated compression type\n\nBody: This PR fixes https://github.com/duckdblabs/duckdb-internal/issues/4740\r\n\r\nBefore we would silently fall back to uncompressed, now we throw instead."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/actions/build_extensions_dockerized/action.yml\n@@ -68,11 +68,10 @@ runs:\n           echo \"CXX=${{ inputs.duckdb_arch == 'linux_arm64' && 'aarch64-linux-gnu-g++' || '' }}\" >> docker_env.txt \n \n       - name: Generate timestamp for Ccache entry\n-        shell: cmake -P {0}\n+        shell: bash\n         id: ccache_timestamp\n         run: |\n-          string(TIMESTAMP current_date \"%Y-%m-%d-%H;%M;%S\" UTC)\n-          message(\"::set-output name=timestamp::${current_date}\")\n+          date --utc +'timestamp=%Y-%m-%d-%H;%M;%S' >> \"$GITHUB_OUTPUT\"\n \n       - name: Create Ccache directory\n         shell: bash\n---\nsrc/function/table/table_scan.cpp\n@@ -344,6 +344,18 @@ unique_ptr<GlobalTableFunctionState> DuckIndexScanInitGlobal(ClientContext &cont\n                                                              unsafe_vector<row_t> &row_ids) {\n \tauto g_state = make_uniq<DuckIndexScanState>(context, input.bind_data.get());\n \tif (!row_ids.empty()) {\n+\t\t// Duplicate-eliminate row IDs.\n+\t\tunordered_set<row_t> row_id_set;\n+\t\tauto it = row_ids.begin();\n+\t\twhile (it != row_ids.end()) {\n+\t\t\tif (row_id_set.find(*it) == row_id_set.end()) {\n+\t\t\t\trow_id_set.insert(*it++);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t// Found a duplicate.\n+\t\t\tit = row_ids.erase(it);\n+\t\t}\n+\n \t\tstd::sort(row_ids.begin(), row_ids.end());\n \t\tg_state->row_ids = std::move(row_ids);\n \t}\n---\ntest/sql/index/art/scan/test_art_scan_duplicate_filters.test\n@@ -0,0 +1,61 @@\n+# name: test/sql/index/art/scan/test_art_scan_duplicate_filters.test\n+# description: Test index scans with duplicate filters.\n+# group: [scan]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE t_1 (fIdx VARCHAR, sIdx UUID,);\n+\n+statement ok\n+CREATE TABLE t_3 (fIdx VARCHAR, sIdx UUID);\n+\n+statement ok\n+CREATE TABLE t_4 (sIdx UUID);\n+\n+statement ok\n+CREATE TABLE t_5 (sIdx UUID);\n+\n+statement ok\n+CREATE UNIQUE INDEX _pk_idx_t_5 ON t_5 (sIdx);\n+\n+statement ok\n+INSERT INTO t_4 (sIdx) VALUES\n+('1381e0ce-6b3e-43f5-9536-5e7af3a512a5'::UUID),\n+('6880cdba-09f5-3c4f-8eb8-391aefdd8052'::UUID),\n+('a3e876dd-5e50-3af7-9649-689fd938daeb'::UUID),\n+('e0abc0d3-63be-41d8-99ca-b1269ed153a8'::UUID);\n+\n+statement ok\n+INSERT INTO t_5 (sIdx) VALUES\n+('a3e876dd-5e50-3af7-9649-689fd938daeb'::UUID),\n+('e0abc0d3-63be-41d8-99ca-b1269ed153a8'::UUID),\n+('91c79790-5828-45f2-ad88-50e9b541cc05'::UUID),\n+('1381e0ce-6b3e-43f5-9536-5e7af3a512a5'::UUID),\n+('d9f29fa5-2051-3ea4-9d9e-15c3698b4bb8'::UUID),\n+('6880cdba-09f5-3c4f-8eb8-391aefdd8052'::UUID),\n+('3239280c-2204-3e60-b3a1-3ad3acc8fd59'::UUID),\n+('2fced822-342d-344a-aa34-2707e593be52'::UUID);\n+\n+query I\n+WITH\n+\tcte_5 AS (\n+\t\tSELECT sIdx FROM t_4 ANTI JOIN t_3 USING (sIdx)\n+\t),\n+\tcte_6 AS MATERIALIZED (\n+\t\tSELECT\n+\t\t\tCOALESCE(cte_5.sIdx, t_1.sIdx) AS sIdx,\n+\t\t\tCOALESCE(t_1.fIdx, cte_5.sIdx::VARCHAR) AS fIdx,\n+\t\tFROM cte_5 FULL JOIN t_1 USING (sIdx)\n+\t),\n+\tcte_7 AS (\n+\t\tSELECT t_5.sIdx, FROM t_5\n+\t\tWHERE sIdx IN (SELECT sIdx FROM cte_6)\n+\t)\n+SELECT fIdx, FROM cte_6 JOIN cte_7 USING (sIdx) ORDER BY fIdx;\n+----\n+1381e0ce-6b3e-43f5-9536-5e7af3a512a5\n+6880cdba-09f5-3c4f-8eb8-391aefdd8052\n+a3e876dd-5e50-3af7-9649-689fd938daeb\n+e0abc0d3-63be-41d8-99ca-b1269ed153a8\n\\ No newline at end of file\n---\n"}], "ideal": "Title: [Fix] duplicate filters during index scans\n\nBody: We're seeing this regression/incorrect data because there are duplicate filters in the table scan. We invoke an index scan on every filter of the conjunction (containing the duplicates), leading to duplicate row IDs in the `row_id` **vector**.\r\n\r\nThis PR changes the container for row IDs to `unordered_set` to duplicate-eliminate row IDs on insertion. This change causes on extra copy of the row IDs (from the `unordered_set`) into the row ID vector of the global state.\r\n\r\nClose https://github.com/duckdblabs/duckdb-internal/issues/4903."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "benchmark/benchmark_runner.cpp\n@@ -87,7 +87,7 @@ void sleep_thread(Benchmark *benchmark, BenchmarkRunner *runner, BenchmarkState\n \t\t\tif (!hotrun) {\n \t\t\t\trunner->Log(StringUtil::Format(\"%s\\t%d\\t\", benchmark->name, 0));\n \t\t\t}\n-\t\t\trunner->LogResult(\"KILLED\");\n+\t\t\trunner->LogResult(\"Benchmark timeout reached; Interrupt failed. Benchmark killed by benchmark runner\");\n \t\t\texit(1);\n \t\t}\n \t}\n---\nbenchmark/large/tpch-sf100/load.sql\n@@ -1 +1,10 @@\n-CALL dbgen(sf=100);\n+CALL dbgen(sf=100, children=10, step=1);\n+CALL dbgen(sf=100, children=10, step=2);\n+CALL dbgen(sf=100, children=10, step=3);\n+CALL dbgen(sf=100, children=10, step=4);\n+CALL dbgen(sf=100, children=10, step=5);\n+CALL dbgen(sf=100, children=10, step=6);\n+CALL dbgen(sf=100, children=10, step=7);\n+CALL dbgen(sf=100, children=10, step=8);\n+CALL dbgen(sf=100, children=10, step=9);\n+CALL dbgen(sf=100, children=10, step=10);\n\\ No newline at end of file\n---\n"}], "ideal": "Title: Generate data for tpch sf100 in steps\n\nBody: fixes https://github.com/duckdblabs/duckdb-internal/issues/4905\r\nAlso changes \"KILLED\" message to more DuckDB-specific action instead of something that sounds like the OS is killing the benchmark runner"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "CMakeLists.txt\n@@ -289,7 +289,6 @@ if(${CMAKE_SYSTEM_NAME} STREQUAL \"SunOS\")\n endif()\n \n if (OVERRIDE_GIT_DESCRIBE MATCHES \"^v[0-9]+\\.[0-9]+\\.[0-9]+\\-rc[0-9]+$\")\n-  message(STATUS \"HI FROM ME\")\n   if (DUCKDB_EXPLICIT_VERSION)\n     if (DUCKDB_EXPLICIT_PLATFORM STREQUAL DUCKDB_EXPLICIT_PLATFORM)\n       message(FATAL_ERROR \"Provided OVERRIDE_GIT_DESCRIBE '${OVERRIDE_GIT_DESCRIBE}' and DUCKDB_EXPLICIT_PLATFORM '${DUCKDB_EXPLICIT_PLATFORM}' are both set and different\")\n---\nscripts/run_extension_medata_tests.sh\n@@ -120,7 +120,7 @@ EOL\n EOL\n \n   # Build the extensions using the incorrect platform\n-  EXTRA_CMAKE_VARIABLES=-DDUCKDB_EXPLICIT_VERSION=v1337 EXTENSION_CONFIGS=$TEST_DIR/extension_config_before.cmake BUILD_EXTENSIONS_ONLY=1 make debug\n+  DUCKDB_EXPLICIT_VERSION=v1337 EXTENSION_CONFIGS=$TEST_DIR/extension_config_before.cmake BUILD_EXTENSIONS_ONLY=1 make debug\n \n   cp $DUCKDB_BUILD_DIR/extension/json/json.duckdb_extension $DIRECT_INSTALL_DIR/json_incorrect_version.duckdb_extension\n \n@@ -134,7 +134,7 @@ EOL\n EOL\n \n   # Build the extensions using the incorrect platform\n-  DUCKDB_PLATFORM=test_platform EXTRA_CMAKE_VARIABLES=-DDUCKDB_EXPLICIT_VERSION=v1337 EXTENSION_CONFIGS=$TEST_DIR/extension_config_before.cmake BUILD_EXTENSIONS_ONLY=1 make debug\n+  DUCKDB_PLATFORM=test_platform DUCKDB_EXPLICIT_VERSION=v1337 EXTENSION_CONFIGS=$TEST_DIR/extension_config_before.cmake BUILD_EXTENSIONS_ONLY=1 make debug\n \n   cp $DUCKDB_BUILD_DIR/extension/json/json.duckdb_extension $DIRECT_INSTALL_DIR/json_incorrect_version_and_platform.duckdb_extension\n \n---\n"}], "ideal": "Title: Fix extension test\n\nBody: This fixes only a failing test in CI.\r\nChanges are removing a `message` line and changing slightly test setup to account for the changes in https://github.com/duckdb/duckdb/pull/17501.\r\n\r\nI double checked since it's the test checking that extensions install works properly, and wanted to be extra sure."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "scripts/sqllogictest/parser/parser.py\n@@ -356,6 +356,8 @@ def statement_load(self, header: Token) -> Optional[BaseStatement]:\n         statement = Load(header, self.current_line + 1)\n         if len(header.parameters) > 1 and header.parameters[1] == \"readonly\":\n             statement.set_readonly()\n+        if len(header.parameters) > 2:\n+            statement.set_version(header.parameters[2])\n         return statement\n \n     def statement_loop(self, header: Token) -> Optional[BaseStatement]:\n---\nscripts/sqllogictest/result.py\n@@ -63,9 +63,6 @@ def __init__(self, type: \"ExecuteResult.Type\"):\n \n BUILTIN_EXTENSIONS = [\n     'json',\n-    'fts',\n-    'tpcds',\n-    'tpch',\n     'parquet',\n     'icu',\n ]\n@@ -823,6 +820,9 @@ def execute_load(self, load: Load):\n         else:\n             additional_config['access_mode'] = 'automatic'\n \n+        if load.version:\n+            additional_config['storage_compatibility_version'] = str(load.version)\n+\n         self.pool = None\n         self.runner.database = None\n         self.runner.database = SQLLogicDatabase(dbpath, self, additional_config)\n@@ -958,13 +958,20 @@ def execute_restart(self, statement: Restart):\n \n     def execute_set(self, statement: Set):\n         option = statement.header.parameters[0]\n-        string_set = (\n-            self.runner.ignore_error_messages\n-            if option == \"ignore_error_messages\"\n-            else self.runner.always_fail_error_messages\n-        )\n-        string_set.clear()\n-        string_set = statement.error_messages\n+        if option == 'ignore_error_messages':\n+            string_set = (\n+                self.runner.ignore_error_messages\n+                if option == \"ignore_error_messages\"\n+                else self.runner.always_fail_error_messages\n+            )\n+            string_set.clear()\n+            string_set = statement.error_messages\n+        elif option == 'seed':\n+            con = self.get_connection()\n+            con.execute(f\"SELECT SETSEED({statement.header.parameters[1]})\")\n+            self.runner.skip_reload = True\n+        else:\n+            self.skiptest(f\"SET '{option}' is not implemented!\")\n \n     def execute_hash_threshold(self, statement: HashThreshold):\n         self.runner.hash_threshold = statement.threshold\n---\nscripts/sqllogictest/statement/load.py\n@@ -6,6 +6,10 @@ class Load(BaseStatement):\n     def __init__(self, header: Token, line: int):\n         super().__init__(header, line)\n         self.readonly: bool = False\n+        self.version: Optional[int] = None\n \n     def set_readonly(self):\n         self.readonly = True\n+\n+    def set_version(self, version: str):\n+        self.version = version\n---\ntools/pythonpkg/sqllogic/conftest.py\n@@ -5,6 +5,7 @@\n import re\n import typing\n import warnings\n+import glob\n from .skipped_tests import SKIPPED_TESTS\n \n SQLLOGIC_TEST_CASE_NAME = \"test_sqllogic\"\n@@ -22,6 +23,13 @@ def pytest_addoption(parser: pytest.Parser):\n         dest=\"test_dirs\",\n         help=\"Path to one or more directories containing SQLLogic test scripts\",\n     )\n+    parser.addoption(\n+        \"--path\",\n+        type=str,\n+        default=None,\n+        dest=\"path\",\n+        help=\"Path (or glob) of the tests to run\",\n+    )\n     parser.addoption(\n         \"--build-dir\",\n         type=str,\n@@ -96,6 +104,15 @@ def get_test_marks(path: pathlib.Path, root_dir: pathlib.Path, config: pytest.Co\n     return marks\n \n \n+def create_parameters_from_paths(paths, root_dir: pathlib.Path, config: pytest.Config) -> typing.Iterator[typing.Any]:\n+    return map(\n+        lambda path: pytest.param(\n+            path.absolute(), id=get_test_id(path, root_dir, config), marks=get_test_marks(path, root_dir, config)\n+        ),\n+        paths,\n+    )\n+\n+\n def scan_for_test_scripts(root_dir: pathlib.Path, config: pytest.Config) -> typing.Iterator[typing.Any]:\n     \"\"\"\n     Scans for .test files in the given directory and its subdirectories.\n@@ -105,30 +122,35 @@ def scan_for_test_scripts(root_dir: pathlib.Path, config: pytest.Config) -> typi\n     # TODO: Add tests from extensions\n     test_script_extensions = [\".test\", \".test_slow\", \".test_coverage\"]\n     it = itertools.chain.from_iterable(root_dir.rglob(f\"*{ext}\") for ext in test_script_extensions)\n-    return map(\n-        lambda path: pytest.param(\n-            path.absolute(), id=get_test_id(path, root_dir, config), marks=get_test_marks(path, root_dir, config)\n-        ),\n-        it,\n-    )\n+    return create_parameters_from_paths(it, root_dir, config)\n \n \n def pytest_generate_tests(metafunc: pytest.Metafunc):\n     # test_sqllogic (a.k.a SQLLOGIC_TEST_CASE_NAME) is defined in test_sqllogic.py\n-    if metafunc.definition.name == SQLLOGIC_TEST_CASE_NAME:\n-        test_dirs: typing.List[pathlib.Path] = metafunc.config.getoption(\"test_dirs\")\n-        if len(test_dirs) == 0:\n-            # Use DuckDB's test directory as the default when --test-dir not specified\n-            test_dirs = [DUCKDB_ROOT_DIR / \"test\"]\n+    if metafunc.definition.name != SQLLOGIC_TEST_CASE_NAME:\n+        return\n \n-        parameters = []\n-        for test_dir in test_dirs:\n-            # Create absolute & normalized path\n-            test_dir = test_dir.resolve()\n-            assert test_dir.is_dir()\n-            parameters.extend(scan_for_test_scripts(test_dir, metafunc.config))\n+    test_dirs: typing.List[pathlib.Path] = metafunc.config.getoption(\"test_dirs\")\n+    test_glob: typing.Optional[pathlib.Path] = metafunc.config.getoption(\"path\")\n+\n+    parameters = []\n+\n+    if test_glob:\n+        test_paths = DUCKDB_ROOT_DIR.rglob(test_glob)\n+        parameters.extend(create_parameters_from_paths(test_paths, DUCKDB_ROOT_DIR, metafunc.config))\n+\n+    for test_dir in test_dirs:\n+        # Create absolute & normalized path\n+        test_dir = test_dir.resolve()\n+        assert test_dir.is_dir()\n+        parameters.extend(scan_for_test_scripts(test_dir, metafunc.config))\n+\n+    if parameters == []:\n+        if len(test_dirs) == 0:\n+            # Use DuckDB's test directory as the default when no paths are provided\n+            parameters.extend(scan_for_test_scripts(DUCKDB_ROOT_DIR / \"test\", metafunc.config))\n \n-        metafunc.parametrize(SQLLOGIC_TEST_PARAMETER, parameters)\n+    metafunc.parametrize(SQLLOGIC_TEST_PARAMETER, parameters)\n \n \n # Execute last, after pytest has already deselected tests based on -k and -m parameters\n---\n"}], "ideal": "Title: [Python Dev] Fix failing tests for the Python SQLLogicTester\n\nBody: This PR fixes https://github.com/duckdblabs/duckdb-internal/issues/4900"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/extension.cpp\n@@ -75,7 +75,7 @@ string ParsedExtensionMetaData::GetInvalidMetadataError() {\n \t\t\t                       DUCKDB_EXTENSION_API_VERSION_MINOR, DUCKDB_EXTENSION_API_VERSION_PATCH);\n \t\t}\n \t} else {\n-\t\tthrow InternalException(\"Unknown ABI type for extension: \" + extension_abi_metadata);\n+\t\tthrow InternalException(\"Unknown ABI type for extension: '%s'\", extension_abi_metadata);\n \t}\n \n \tif (engine_platform != platform) {\n---\nsrc/parallel/executor_task.cpp\n@@ -55,7 +55,7 @@ TaskExecutionResult ExecutorTask::Execute(TaskExecutionMode mode) {\n \t} catch (std::exception &ex) {\n \t\texecutor.PushError(ErrorData(ex));\n \t} catch (...) { // LCOV_EXCL_START\n-\t\texecutor.PushError(ErrorData(\"Unknown exception in Finalize!\"));\n+\t\texecutor.PushError(ErrorData(\"Unknown exception in ExecutorTask::Execute\"));\n \t} // LCOV_EXCL_STOP\n \treturn TaskExecutionResult::TASK_ERROR;\n }\n---\nsrc/parallel/pipeline_event.cpp\n@@ -15,7 +15,7 @@ void PipelineEvent::Schedule() {\n \t} catch (std::exception &ex) {\n \t\texecutor.PushError(ErrorData(ex));\n \t} catch (...) { // LCOV_EXCL_START\n-\t\texecutor.PushError(ErrorData(\"Unknown exception in Finalize!\"));\n+\t\texecutor.PushError(ErrorData(\"Unknown exception while calling pipeline->Schedule(event)!\"));\n \t} // LCOV_EXCL_STOP\n }\n \n---\n"}], "ideal": "Title: Minor fixes to exception error messages\n\nBody: Minor, I bumped on those, they can be as well be fixed.\r\n\r\nThe fix at [src/main/extension.cpp](https://github.com/duckdb/duckdb/compare/v1.3-ossivalis...carlopi:duckdb:minor_fixes_exceptions?expand=1#diff-d3d8bf6f19341657a048550f00e3bb1cc0f8d75c9d66d29296999d63d3e25696) would need to be generalized, there might be other places where we construct exception messages from unsanitized strings, and while that is handled by Format function, then error that is then re-throw bumps to InternalError and it's not super clear.\r\nMy proposal would be to change exceptions to take a `const char *` instead of a string, so that string concatenation is forbidden while constant strings are still allowed.\r\nBut this requires changes in many places, and not a point fix.\r\n\r\n---\r\n\r\nEither `v1.3-ossivalis` or `main`, both are fine, this is minor."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/config/out_of_tree_extensions.cmake\n@@ -21,6 +21,7 @@ duckdb_extension_load(httpfs\n     GIT_URL https://github.com/duckdb/duckdb-httpfs\n     GIT_TAG eb1b04907c419d576f5fa4b34303810e8802e2f8\n     INCLUDE_DIR extension/httpfs/include\n+    APPLY_PATCHES\n     )\n \n ################# AVRO\n---\n.github/patches/extensions/httpfs/no_follow_no_keep_alive.patch\n@@ -0,0 +1,13 @@\n+diff --git a/extension/httpfs/httpfs_client.cpp b/extension/httpfs/httpfs_client.cpp\n+index 7a779ef..84eb457 100644\n+--- a/extension/httpfs/httpfs_client.cpp\n++++ b/extension/httpfs/httpfs_client.cpp\n+@@ -10,7 +10,7 @@ class HTTPFSClient : public HTTPClient {\n+ public:\n+ \tHTTPFSClient(HTTPFSParams &http_params, const string &proto_host_port) {\n+ \t\tclient = make_uniq<duckdb_httplib_openssl::Client>(proto_host_port);\n+-\t\tclient->set_follow_location(true);\n++\t\tclient->set_follow_location(http_params.follow_location);\n+ \t\tclient->set_keep_alive(http_params.keep_alive);\n+ \t\tif (!http_params.ca_cert_file.empty()) {\n+ \t\t\tclient->set_ca_cert_path(http_params.ca_cert_file.c_str());\n---\nsrc/include/duckdb/common/http_util.hpp\n@@ -39,6 +39,7 @@ struct HTTPParams {\n \tuint64_t retry_wait_ms = DEFAULT_RETRY_WAIT_MS;\n \tfloat retry_backoff = DEFAULT_RETRY_BACKOFF;\n \tbool keep_alive = DEFAULT_KEEP_ALIVE;\n+\tbool follow_location = true;\n \n \tstring http_proxy;\n \tidx_t http_proxy_port;\n---\nsrc/main/extension/extension_install.cpp\n@@ -367,6 +367,11 @@ static unique_ptr<ExtensionInstallInfo> InstallFromHttpUrl(DatabaseInstance &db,\n \t\tparams = http_util.InitializeParameters(db, url);\n \t}\n \n+\t// Unclear what's peculiar about extension install flow, but those two parameters are needed\n+\t// to avoid lengthy retry on 304\n+\tparams->follow_location = false;\n+\tparams->keep_alive = false;\n+\n \tGetRequestInfo get_request(url, headers, *params, nullptr, nullptr);\n \tget_request.try_request = true;\n \n---\nsrc/main/http/http_util.cpp\n@@ -130,7 +130,7 @@ class HTTPLibClient : public HTTPClient {\n \t\tauto sec = static_cast<time_t>(http_params.timeout);\n \t\tauto usec = static_cast<time_t>(http_params.timeout_usec);\n \t\tclient = make_uniq<duckdb_httplib::Client>(proto_host_port);\n-\t\tclient->set_follow_location(true);\n+\t\tclient->set_follow_location(http_params.follow_location);\n \t\tclient->set_keep_alive(http_params.keep_alive);\n \t\tclient->set_write_timeout(sec, usec);\n \t\tclient->set_read_timeout(sec, usec);\n---\n"}], "ideal": "Title: Fix update extensions\n\nBody: This restore the following workflows to work as intended:\r\n```\r\nFORCE INSTALL spatial;\r\n--- not relevant which extension, this should trigger a full download\r\nUPDATE EXTENSIONS (spatial);\r\n--- this should be a single GET request with \"If-Not-Match\" header\r\n```\r\nand the same with `httpfs` loaded:\r\n```\r\nFORCE INSTALL httpfs;\r\nLOAD httpfs;\r\nFORCE INSTALL spatial;\r\n--- not relevant which extension, this should trigger a full download\r\nUPDATE EXTENSIONS (spatial);\r\n--- this should be a single GET request with \"If-Not-Match\" header\r\n```\r\n\r\nNote that this is relevant only for UPDATE EXTENSIONS or for the INSTALL codepath for an already installed extension.\r\n\r\nAlso, without this PR the behaviour would be long waits on `UPDATE EXTENSIONS`, that would eventually timeout and resume normal execution.\r\n\r\nIt's not completely clear to me why both `follow_location` and `keep_alive` needs to be set to false, whether this has something to do with our current setup to serve extensions or else, but this restore to the defaults in place before the HTTPUtil rework."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/src/typing/pytype.cpp\n@@ -272,7 +272,7 @@ static LogicalType FromGenericAlias(const py::object &obj) {\n }\n \n static LogicalType FromDictionary(const py::object &obj) {\n-\tauto dict = py::reinterpret_steal<py::dict>(obj);\n+\tauto dict = py::reinterpret_borrow<py::dict>(obj);\n \tchild_list_t<LogicalType> children;\n \tif (dict.size() == 0) {\n \t\tthrow InvalidInputException(\"Could not convert empty dictionary to a duckdb STRUCT type\");\n---\n"}], "ideal": "Title: [Python Dev] Using `reinterpret_steal` breaks the refcount of the passed-in object\n\nBody: Fix #17523 \r\n\r\nFixes the corruption of the refcount of the passed-in object"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/expression_executor.cpp\n@@ -25,7 +25,6 @@ ExpressionExecutor::ExpressionExecutor(ClientContext &context, const Expression\n \n ExpressionExecutor::ExpressionExecutor(ClientContext &context, const vector<unique_ptr<Expression>> &exprs)\n     : ExpressionExecutor(context) {\n-\tD_ASSERT(exprs.size() > 0);\n \tfor (auto &expr : exprs) {\n \t\tAddExpression(*expr);\n \t}\n---\n"}], "ideal": "Title: fix: remove this assertion that size > 0 for ExpressionExecutor construction\n\nBody: Hi DuckDB Team,\r\n\r\nI came across this assertion and was wondering if it's strictly necessary\u2014it seems to prevent the use of an ExpressionExecutor when there are no expressions, which I was hoping to support in my use case.\r\n\r\nIf it's alright with you, I\u2019d like you to merge this PR to remove it.\r\n\r\nThanks!\r\nRusty"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "scripts/generate_extensions_function.py\n@@ -882,6 +882,7 @@ def write_header(data: ExtensionData):\n     \"autocomplete\",\n     \"core_functions\",\n     \"delta\",\n+    \"encodings\",\n     \"excel\",\n     \"fts\",\n     \"httpfs\",\n---\nsrc/execution/operator/csv_scanner/encode/csv_encoder.cpp\n@@ -43,13 +43,10 @@ CSVEncoder::CSVEncoder(ClientContext &context, const string &encoding_name_to_fi\n \tencoding_name = StringUtil::Lower(encoding_name_to_find);\n \tauto function = config.GetEncodeFunction(encoding_name_to_find);\n \tif (!function) {\n-\t\tif (!context.db->ExtensionIsLoaded(\"encodings\")) {\n-\t\t\t// Maybe we can try to auto-load from our encodings extension, if this somehow fails, we just error.\n-\t\t\tauto extension_loaded = ExtensionHelper::TryAutoLoadExtension(context, \"encodings\");\n-\t\t\tif (extension_loaded) {\n-\t\t\t\t// If it successfully loaded, we can try to get our function again\n-\t\t\t\tfunction = config.GetEncodeFunction(encoding_name_to_find);\n-\t\t\t}\n+\t\t// Maybe we can try to auto-load from our encodings extension, if this somehow fails, we just error.\n+\t\tif (Catalog::TryAutoLoad(context, \"encodings\")) {\n+\t\t\t// If it successfully loaded, we can try to get our function again\n+\t\t\tfunction = config.GetEncodeFunction(encoding_name_to_find);\n \t\t}\n \t}\n \tif (!function) {\n---\nsrc/include/duckdb/main/extension_entries.hpp\n@@ -1119,6 +1119,7 @@ static constexpr const char *AUTOLOADABLE_EXTENSIONS[] = {\"avro\",\n                                                           \"autocomplete\",\n                                                           \"core_functions\",\n                                                           \"delta\",\n+                                                          \"encodings\",\n                                                           \"excel\",\n                                                           \"fts\",\n                                                           \"httpfs\",\n---\n"}], "ideal": "Title: Use Catalog::TryAutoLoad for encodings extension\n\nBody: This should check the additional logic to handle : \r\n\r\n```\r\nstatement ok\r\nset autoload_known_extensions=false\r\n\r\nstatement ok\r\nset autoinstall_known_extensions=false\r\n``` "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/scan/physical_table_scan.cpp\n@@ -18,7 +18,7 @@ PhysicalTableScan::PhysicalTableScan(vector<LogicalType> types, TableFunction fu\n     : PhysicalOperator(PhysicalOperatorType::TABLE_SCAN, std::move(types), estimated_cardinality),\n       function(std::move(function_p)), bind_data(std::move(bind_data_p)), returned_types(std::move(returned_types_p)),\n       column_ids(std::move(column_ids_p)), projection_ids(std::move(projection_ids_p)), names(std::move(names_p)),\n-      table_filters(std::move(table_filters_p)), extra_info(extra_info), parameters(std::move(parameters_p)),\n+      table_filters(std::move(table_filters_p)), extra_info(std::move(extra_info)), parameters(std::move(parameters_p)),\n       virtual_columns(std::move(virtual_columns_p)) {\n }\n \n---\nsrc/execution/physical_plan/plan_get.cpp\n@@ -139,10 +139,10 @@ PhysicalOperator &PhysicalPlanGenerator::CreatePlan(LogicalGet &op) {\n \t// create the table scan node\n \tif (!op.function.projection_pushdown) {\n \t\t// function does not support projection pushdown\n-\t\tauto &table_scan = Make<PhysicalTableScan>(op.returned_types, op.function, std::move(op.bind_data),\n-\t\t                                           op.returned_types, column_ids, vector<column_t>(), op.names,\n-\t\t                                           std::move(table_filters), op.estimated_cardinality, op.extra_info,\n-\t\t                                           std::move(op.parameters), std::move(op.virtual_columns));\n+\t\tauto &table_scan = Make<PhysicalTableScan>(\n+\t\t    op.returned_types, op.function, std::move(op.bind_data), op.returned_types, column_ids, vector<column_t>(),\n+\t\t    op.names, std::move(table_filters), op.estimated_cardinality, std::move(op.extra_info),\n+\t\t    std::move(op.parameters), std::move(op.virtual_columns));\n \t\t// first check if an additional projection is necessary\n \t\tif (column_ids.size() == op.returned_types.size()) {\n \t\t\tbool projection_necessary = false;\n@@ -188,7 +188,7 @@ PhysicalOperator &PhysicalPlanGenerator::CreatePlan(LogicalGet &op) {\n \tauto &table_scan =\n \t    Make<PhysicalTableScan>(op.types, op.function, std::move(op.bind_data), op.returned_types, column_ids,\n \t                            op.projection_ids, op.names, std::move(table_filters), op.estimated_cardinality,\n-\t                            op.extra_info, std::move(op.parameters), std::move(op.virtual_columns));\n+\t                            std::move(op.extra_info), std::move(op.parameters), std::move(op.virtual_columns));\n \tauto &cast_table_scan = table_scan.Cast<PhysicalTableScan>();\n \tcast_table_scan.dynamic_filters = op.dynamic_filters;\n \tif (filter) {\n---\nsrc/include/duckdb/common/extra_operator_info.hpp\n@@ -21,7 +21,7 @@ class ExtraOperatorInfo {\n public:\n \tExtraOperatorInfo() : file_filters(\"\"), sample_options(nullptr) {\n \t}\n-\tExtraOperatorInfo(ExtraOperatorInfo &extra_info)\n+\tExtraOperatorInfo(ExtraOperatorInfo &&extra_info) noexcept\n \t    : file_filters(extra_info.file_filters), sample_options(std::move(extra_info.sample_options)) {\n \t\tif (extra_info.total_files.IsValid()) {\n \t\t\ttotal_files = extra_info.total_files.GetIndex();\n@@ -30,6 +30,19 @@ class ExtraOperatorInfo {\n \t\t\tfiltered_files = extra_info.filtered_files.GetIndex();\n \t\t}\n \t}\n+\tExtraOperatorInfo &operator=(ExtraOperatorInfo &&extra_info) noexcept {\n+\t\tif (this != &extra_info) {\n+\t\t\tfile_filters = extra_info.file_filters;\n+\t\t\tif (extra_info.total_files.IsValid()) {\n+\t\t\t\ttotal_files = extra_info.total_files.GetIndex();\n+\t\t\t}\n+\t\t\tif (extra_info.filtered_files.IsValid()) {\n+\t\t\t\tfiltered_files = extra_info.filtered_files.GetIndex();\n+\t\t\t}\n+\t\t\tsample_options = std::move(extra_info.sample_options);\n+\t\t}\n+\t\treturn *this;\n+\t}\n \n \t//! Filters that have been pushed down into the main file list\n \tstring file_filters;\n@@ -39,6 +52,9 @@ class ExtraOperatorInfo {\n \toptional_idx filtered_files;\n \t//! Sample options that have been pushed down into the table scan\n \tunique_ptr<SampleOptions> sample_options;\n+\n+\tvoid Serialize(Serializer &serializer) const;\n+\tstatic ExtraOperatorInfo Deserialize(Deserializer &deserializer);\n };\n \n } // namespace duckdb\n---\nsrc/include/duckdb/storage/serialization/nodes.json\n@@ -1106,5 +1106,34 @@\n     ],\n     \"pointer_type\": \"none\",\n     \"constructor\": [\"name\", \"type\"]\n+  },\n+  {\n+    \"class\": \"ExtraOperatorInfo\",\n+    \"includes\": [\n+      \"duckdb/common/extra_operator_info.hpp\"\n+    ],\n+    \"members\": [\n+      {\n+        \"id\": 100,\n+        \"name\": \"file_filters\",\n+        \"type\": \"string\"\n+      },\n+      {\n+        \"id\": 101,\n+        \"name\": \"total_files\",\n+        \"type\": \"optional_idx\"\n+      },\n+      {\n+        \"id\": 102,\n+        \"name\": \"filtered_files\",\n+        \"type\": \"optional_idx\"\n+      },\n+      {\n+        \"id\": 103,\n+        \"name\": \"sample_options\",\n+        \"type\": \"unique_ptr<SampleOptions>\"\n+      }\n+    ],\n+    \"pointer_type\": \"none\"\n   }\n ]\n---\nsrc/planner/operator/logical_get.cpp\n@@ -219,6 +219,7 @@ void LogicalGet::Serialize(Serializer &serializer) const {\n \t}\n \tserializer.WriteProperty(210, \"projected_input\", projected_input);\n \tserializer.WritePropertyWithDefault(211, \"column_indexes\", column_ids);\n+\tserializer.WriteProperty(212, \"extra_info\", extra_info);\n }\n \n unique_ptr<LogicalOperator> LogicalGet::Deserialize(Deserializer &deserializer) {\n@@ -256,6 +257,7 @@ unique_ptr<LogicalOperator> LogicalGet::Deserialize(Deserializer &deserializer)\n \t\t\tresult->column_ids.emplace_back(col_id);\n \t\t}\n \t}\n+\tresult->extra_info = deserializer.ReadProperty<ExtraOperatorInfo>(212, \"extra_info\");\n \tauto &context = deserializer.Get<ClientContext &>();\n \tvirtual_column_map_t virtual_columns;\n \tif (!has_serialize) {\n---\nsrc/storage/serialization/serialize_nodes.cpp\n@@ -35,6 +35,7 @@\n #include \"duckdb/parser/parsed_data/exported_table_data.hpp\"\n #include \"duckdb/common/column_index.hpp\"\n #include \"duckdb/common/table_column.hpp\"\n+#include \"duckdb/common/extra_operator_info.hpp\"\n \n namespace duckdb {\n \n@@ -302,6 +303,22 @@ ExportedTableInfo ExportedTableInfo::Deserialize(Deserializer &deserializer) {\n \treturn result;\n }\n \n+void ExtraOperatorInfo::Serialize(Serializer &serializer) const {\n+\tserializer.WritePropertyWithDefault<string>(100, \"file_filters\", file_filters);\n+\tserializer.WriteProperty<optional_idx>(101, \"total_files\", total_files);\n+\tserializer.WriteProperty<optional_idx>(102, \"filtered_files\", filtered_files);\n+\tserializer.WritePropertyWithDefault<unique_ptr<SampleOptions>>(103, \"sample_options\", sample_options);\n+}\n+\n+ExtraOperatorInfo ExtraOperatorInfo::Deserialize(Deserializer &deserializer) {\n+\tExtraOperatorInfo result;\n+\tdeserializer.ReadPropertyWithDefault<string>(100, \"file_filters\", result.file_filters);\n+\tdeserializer.ReadProperty<optional_idx>(101, \"total_files\", result.total_files);\n+\tdeserializer.ReadProperty<optional_idx>(102, \"filtered_files\", result.filtered_files);\n+\tdeserializer.ReadPropertyWithDefault<unique_ptr<SampleOptions>>(103, \"sample_options\", result.sample_options);\n+\treturn result;\n+}\n+\n void HivePartitioningIndex::Serialize(Serializer &serializer) const {\n \tserializer.WritePropertyWithDefault<string>(100, \"value\", value);\n \tserializer.WritePropertyWithDefault<idx_t>(101, \"index\", index);\n---\n"}], "ideal": "Title: Add (de)serialization for `ExtraOperatorInfo`\n\nBody: In the sample pushdown, the `SampleOptions` are added to the `extra_infos` of the `LogicalGet`, but go missing when serializing the operator. This PR fixes this by adding serialization and deserialization for `ExtraOperatorInfo`. I hope that _nodes.json_ is the right place to define the serialization for this type.\r\n\r\nFurthermore, I changed the constructor of `ExtraOperatorInfo` to be a move constructor because it is moving a `unique_ptr`."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/actions/build_extensions_dockerized/action.yml\n@@ -68,11 +68,10 @@ runs:\n           echo \"CXX=${{ inputs.duckdb_arch == 'linux_arm64' && 'aarch64-linux-gnu-g++' || '' }}\" >> docker_env.txt \n \n       - name: Generate timestamp for Ccache entry\n-        shell: cmake -P {0}\n+        shell: bash\n         id: ccache_timestamp\n         run: |\n-          string(TIMESTAMP current_date \"%Y-%m-%d-%H;%M;%S\" UTC)\n-          message(\"::set-output name=timestamp::${current_date}\")\n+          date --utc +'timestamp=%Y-%m-%d-%H;%M;%S' >> \"$GITHUB_OUTPUT\"\n \n       - name: Create Ccache directory\n         shell: bash\n---\n"}], "ideal": "Title: Resolve GitHub workflow `set-output` deprecation warnings\n\nBody: In 2022 GitHub deprecated the `set-output` commands:\r\n\r\nhttps://github.blog/changelog/2022-10-10-github-actions-deprecating-save-state-and-set-output-commands/\r\n\r\n`set-output` is used in the `build_extensions_dockerized` workflow and triggers warnings when the `LinuxRelease.yml` workflow runs [[recent example](https://github.com/duckdb/duckdb/actions/runs/15063088936)]:\r\n\r\n> ![image](https://github.com/user-attachments/assets/9cc1d25e-c9de-4c26-b387-1748b5c6bc3b)\r\n\r\n\r\n\r\nThis change resolves the warning."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/pyproject.toml\n@@ -1,7 +1,7 @@\n [build-system]\n requires = [\n \t\"setuptools>=60\",\n-\t\"setuptools_scm>=6.4\",\n+\t\"setuptools_scm>=6.4,<8.0\",\n \t\"pybind11>=2.6.0\",\n ]\n build-backend = \"setuptools.build_meta\"\n@@ -28,9 +28,6 @@ Source = \"https://github.com/duckdb/duckdb/blob/main/tools/pythonpkg\"\n Issues = \"https://github.com/duckdb/duckdb/issues\"\n Changelog = \"https://github.com/duckdb/duckdb/releases\"\n \n-[tool.setuptools_scm]\n-root = \"../..\"\n-\n ### CI Builwheel configurations ###\n \n # Default config runs all tests and requires at least one extension to be tested against\n---\n"}], "ideal": "Title: Fix python nightly build\n\nBody: Revert #17443 \r\n\r\nThey (setuptool_scm) have not fixed their problem, all they've done is upgrade the warning into an error, so we won't use those newer versions until they fix it \ud83d\udc4d "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/config/out_of_tree_extensions.cmake\n@@ -169,10 +169,12 @@ duckdb_extension_load(fts\n )\n \n ################# ENCODINGS\n+if (NOT ${WASM_ENABLED})\n duckdb_extension_load(encodings\n         LOAD_TESTS\n         DONT_LINK\n         GIT_URL https://github.com/duckdb/duckdb-encodings\n         GIT_TAG dc3c206e237b517abcdd95ebe40d02dcd0f71084\n         TEST_DIR test/sql\n-)\n\\ No newline at end of file\n+)\n+endif()\n---\ntest/extension/autoloading_encodings.test\n@@ -2,6 +2,8 @@\n # description: Test autoloading of encodings.\n # group: [extension]\n \n+mode skip\n+\n require-env LOCAL_EXTENSION_REPO\n \n statement ok\n---\n"}], "ideal": "Title: Fix encodings\n\nBody: Unclear, working around autoloading problem and wasm linking timing out.\r\n\r\nFYI @pdet"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types/value.cpp\n@@ -1637,6 +1637,15 @@ string Value::ToSQLString() const {\n \t\tret += \"]\";\n \t\treturn ret;\n \t}\n+\tcase LogicalTypeId::UNION: {\n+\t\tstring ret = \"union_value(\";\n+\t\tauto union_tag = UnionValue::GetTag(*this);\n+\t\tauto &tag_name = UnionType::GetMemberName(type(), union_tag);\n+\t\tret += tag_name + \" := \";\n+\t\tret += UnionValue::GetValue(*this).ToSQLString();\n+\t\tret += \")\";\n+\t\treturn ret;\n+\t}\n \tdefault:\n \t\treturn ToString();\n \t}\n---\n"}], "ideal": "Title: Add support for ToSqlString for union types\n\nBody: This adds support for converting union expressions / values to valid sql strings.\r\n\r\nThis can be used by custom table functions performing filter pushdown into other duckdb tables."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/NightlyTests.yml\n@@ -105,7 +105,7 @@ jobs:\n     needs: linux-memory-leaks\n     env:\n       GEN: ninja\n-      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet;httpfs;spatial\"\n+      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet;httpfs\"\n       DISABLE_SANITIZER: 1\n       CRASH_ON_ASSERT: 1\n       RUN_SLOW_VERIFIERS: 1\n---\n"}], "ideal": "Title: Remove spatial from OSX Relassert\n\nBody: This was added recently, but has never worked"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/join_hashtable.cpp\n@@ -158,19 +158,19 @@ static void ApplyBitmaskAndGetSaltBuild(Vector &hashes_v, Vector &salt_v, const\n }\n \n template <bool HAS_SEL>\n-idx_t GetOptionalIndex(const SelectionVector *sel, idx_t idx) {\n+idx_t GetOptionalIndex(const SelectionVector *sel, const idx_t idx) {\n \treturn HAS_SEL ? sel->get_index(idx) : idx;\n }\n \n static void AddPointerToCompare(JoinHashTable::ProbeState &state, const ht_entry_t &entry, Vector &pointers_result_v,\n-                                idx_t ht_offset, idx_t &keys_to_compare_count, const idx_t &row_index) {\n+                                idx_t row_ht_offset, idx_t &keys_to_compare_count, const idx_t &row_index) {\n \n \tconst auto row_ptr_insert_to = FlatVector::GetData<data_ptr_t>(pointers_result_v);\n \tconst auto ht_offsets = FlatVector::GetData<idx_t>(state.ht_offsets_v);\n \n \tstate.keys_to_compare_sel.set_index(keys_to_compare_count, row_index);\n \trow_ptr_insert_to[row_index] = entry.GetPointer();\n-\tht_offsets[row_index] = ht_offset;\n+\tht_offsets[row_index] = row_ht_offset;\n \tkeys_to_compare_count += 1;\n }\n \n@@ -251,17 +251,24 @@ static void GetRowPointersInternal(DataChunk &keys, TupleDataChunkState &key_sta\n                                    Vector &hashes_v, const SelectionVector *row_sel, idx_t &count, JoinHashTable &ht,\n                                    ht_entry_t *entries, Vector &pointers_result_v, SelectionVector &match_sel,\n                                    bool has_row_sel) {\n-\t// densify hashes: If there is no sel, simply flatten the hashes, else densify via UnifiedVectorFormat\n+\n+\t// in case of a hash collision, we need this information to correctly retrieve the salt of this hash\n+\tbool uses_unified = false;\n+\tUnifiedVectorFormat hashes_unified_v;\n+\n+\t// densify hashes: If there is no sel, flatten the hashes, else densify via UnifiedVectorFormat\n \tif (has_row_sel) {\n-\t\tUnifiedVectorFormat hashes_v_unified;\n-\t\thashes_v.ToUnifiedFormat(count, hashes_v_unified);\n-\t\tauto hashes = UnifiedVectorFormat::GetData<hash_t>(hashes_v_unified);\n+\n+\t\thashes_v.ToUnifiedFormat(count, hashes_unified_v);\n+\t\tuses_unified = true;\n+\n+\t\tauto hashes_unified = UnifiedVectorFormat::GetData<hash_t>(hashes_unified_v);\n \t\tauto hashes_dense = FlatVector::GetData<idx_t>(state.hashes_dense_v);\n \n \t\tfor (idx_t i = 0; i < count; i++) {\n \t\t\tconst auto row_index = row_sel->get_index(i);\n-\t\t\tconst auto uvf_index = hashes_v_unified.sel->get_index(row_index);\n-\t\t\thashes_dense[i] = hashes[uvf_index];\n+\t\t\tconst auto uvf_index = hashes_unified_v.sel->get_index(row_index);\n+\t\t\thashes_dense[i] = hashes_unified[uvf_index];\n \t\t}\n \t} else {\n \t\thashes_v.Flatten(count);\n@@ -284,7 +291,7 @@ static void GetRowPointersInternal(DataChunk &keys, TupleDataChunkState &key_sta\n \t\t\tbreak;\n \t\t}\n \n-\t\t// Perform row comparisons, after function call salt_match_sel will point to the keys that match\n+\t\t// Perform row comparisons, after Match function call salt_match_sel will point to the keys that match\n \t\tkeys_no_match_count = 0;\n \t\tconst idx_t keys_match_count = ht.row_matcher_build.Match(\n \t\t    keys, key_state.vector_data, state.keys_to_compare_sel, keys_to_compare_count, *ht.layout_ptr,\n@@ -300,19 +307,28 @@ static void GetRowPointersInternal(DataChunk &keys, TupleDataChunkState &key_sta\n \t\t}\n \n \t\t// Linear probing for collisions: Move to the next entry in the HT\n+\t\tauto hashes_unified = UnifiedVectorFormat::GetData<hash_t>(hashes_unified_v);\n \t\tauto hashes_dense = FlatVector::GetData<hash_t>(state.hashes_dense_v);\n \t\tauto ht_offsets = FlatVector::GetData<idx_t>(state.ht_offsets_v);\n \n \t\tfor (idx_t i = 0; i < keys_no_match_count; i++) {\n \t\t\tconst auto row_index = state.keys_no_match_sel.get_index(i);\n-\t\t\t// the ProbeForPointers function calculates the ht_offset from the hash, therefore we have to write the\n-\t\t\t// new offset into the hashes_v, otherwise the next iteration will start at the old position. This might\n+\t\t\t// The ProbeForPointers function calculates the ht_offset from the hash; therefore, we have to write the\n+\t\t\t// new offset into the hashes_v; otherwise the next iteration will start at the old position. This might\n \t\t\t// seem as an overhead but assures that the first call of ProbeForPointers is optimized as conceding\n \t\t\t// calls are unlikely (Max 1-(65535/65536)^VectorSize = 3.1%)\n \t\t\tauto ht_offset = ht_offsets[row_index];\n \t\t\tIncrementAndWrap(ht_offset, ht.bitmask);\n \n-\t\t\tconst auto hash = hashes_dense[row_index];\n+\t\t\t// Get original hash from unified vector format to extract the salt if hashes_dense was populated that way\n+\t\t\thash_t hash;\n+\t\t\tif (uses_unified) {\n+\t\t\t\tconst auto uvf_index = hashes_unified_v.sel->get_index(row_index);\n+\t\t\t\thash = hashes_unified[uvf_index];\n+\t\t\t} else {\n+\t\t\t\thash = hashes_dense[row_index];\n+\t\t\t}\n+\n \t\t\tconst auto offset_and_salt = ht_offset | (hash & ht_entry_t::SALT_MASK);\n \n \t\t\thashes_dense[i] = offset_and_salt; // populate dense again\n@@ -672,7 +688,7 @@ static void InsertHashesLoop(atomic<ht_entry_t> entries[], Vector &row_locations\n \t\t\t\tif (PARALLEL) {\n \t\t\t\t\t// if the insertion was not successful, the entry was occupied in the meantime, so we have to\n \t\t\t\t\t// compare the keys and insert the row to the next entry\n-\t\t\t\t\tif (potential_collided_ptr) {\n+\t\t\t\t\tif (DUCKDB_UNLIKELY(potential_collided_ptr != nullptr)) {\n \t\t\t\t\t\t// if the entry was occupied, we need to compare the keys and insert the row to the next entry\n \t\t\t\t\t\t// we need to compare the keys and insert the row to the next entry\n \t\t\t\t\t\tstate.keys_to_compare_sel.set_index(salt_match_count, row_index);\n---\ntest/sql/join/inner/test_join_with_nulls.test_slow\n@@ -0,0 +1,35 @@\n+# name: test/sql/join/inner/test_join_with_nulls.test_slow\n+# description: Test joins on uint64 columns with NULL values\n+# group: [inner]\n+\n+set seed 0.42\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PRAGMA disable_optimizer;\n+\n+statement ok\n+CREATE OR REPLACE TABLE build AS (SELECT if(random() < 0.1,null, CAST(round(random() * 1_000_000) as INT64)) as key FROM range(1_000_000))\n+\n+statement ok\n+CREATE OR REPLACE TABLE probe AS (SELECT if(random() < 0.1,null, CAST(round(random() * 1_000_000) as INT64)) as key FROM range(4_000_000))\n+\n+query II\n+SELECT COUNT(*), SUM(probe.key) FROM probe JOIN build ON probe.key = build.key;\n+----\n+3239239\t1617830750716\n+\n+statement ok\n+CREATE OR REPLACE TABLE build2 AS (SELECT if(CAST(round(random() * 400_000) as INT64) < 10, null, CAST(round(random() * 400_000) as INT64)) as key FROM range(500_000));\n+\n+query II\n+SELECT COUNT(*), SUM(probe.key) FROM probe JOIN build2 ON probe.key = build2.key;\n+----\n+1801809\t360752228491\n+\n+query II\n+SELECT COUNT(*), SUM(probe.key) FROM probe JOIN build2 ON probe.key IS NOT DISTINCT FROM build2.key;\n+----\n+5393349\t360752228491\n---\n"}], "ideal": "Title: Fix: Optional Probe Selection\n\nBody: This fixes a bug introduced in https://github.com/duckdb/duckdb/pull/17062, which occurred during hash table probing when resolving hash collisions. The issue arose specifically when NULL values on the build side were filtered out before probing.\r\n\r\nThe PR also adds a test that can reproduces the issue. Since hash collisions are relatively rare, the test uses a large join to increase the likelihood of collisions. It takes about 40 seconds in debug mode and 0.3 seconds in release mode."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/file_system.cpp\n@@ -551,6 +551,14 @@ void FileSystem::RemoveFile(const string &filename, optional_ptr<FileOpener> ope\n \tthrow NotImplementedException(\"%s: RemoveFile is not implemented!\", GetName());\n }\n \n+bool FileSystem::TryRemoveFile(const string &filename, optional_ptr<FileOpener> opener) {\n+\tif (FileExists(filename, opener)) {\n+\t\tRemoveFile(filename, opener);\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n void FileSystem::FileSync(FileHandle &handle) {\n \tthrow NotImplementedException(\"%s: FileSync is not implemented!\", GetName());\n }\n---\nsrc/common/virtual_file_system.cpp\n@@ -120,6 +120,10 @@ void VirtualFileSystem::RemoveFile(const string &filename, optional_ptr<FileOpen\n \tFindFileSystem(filename).RemoveFile(filename, opener);\n }\n \n+bool VirtualFileSystem::TryRemoveFile(const string &filename, optional_ptr<FileOpener> opener) {\n+\treturn FindFileSystem(filename).TryRemoveFile(filename, opener);\n+}\n+\n string VirtualFileSystem::PathSeparator(const string &path) {\n \treturn FindFileSystem(path).PathSeparator(path);\n }\n---\nsrc/execution/operator/persistent/physical_copy_to_file.cpp\n@@ -458,9 +458,7 @@ unique_ptr<GlobalSinkState> PhysicalCopyToFile::GetGlobalSinkState(ClientContext\n void PhysicalCopyToFile::MoveTmpFile(ClientContext &context, const string &tmp_file_path) {\n \tauto &fs = FileSystem::GetFileSystem(context);\n \tauto file_path = GetNonTmpFile(context, tmp_file_path);\n-\tif (fs.FileExists(file_path)) {\n-\t\tfs.RemoveFile(file_path);\n-\t}\n+\tfs.TryRemoveFile(file_path);\n \tfs.MoveFile(tmp_file_path, file_path);\n }\n \n---\nsrc/include/duckdb/common/file_system.hpp\n@@ -178,6 +178,8 @@ class FileSystem {\n \tDUCKDB_API virtual bool IsPipe(const string &filename, optional_ptr<FileOpener> opener = nullptr);\n \t//! Remove a file from disk\n \tDUCKDB_API virtual void RemoveFile(const string &filename, optional_ptr<FileOpener> opener = nullptr);\n+\t//! Remvoe a file from disk if it exists - if it does not exist, return false\n+\tDUCKDB_API virtual bool TryRemoveFile(const string &filename, optional_ptr<FileOpener> opener = nullptr);\n \t//! Sync a file handle to disk\n \tDUCKDB_API virtual void FileSync(FileHandle &handle);\n \t//! Sets the working directory\n---\nsrc/include/duckdb/common/opener_file_system.hpp\n@@ -107,6 +107,12 @@ class OpenerFileSystem : public FileSystem {\n \t\tGetFileSystem().RemoveFile(filename, GetOpener());\n \t}\n \n+\tbool TryRemoveFile(const string &filename, optional_ptr<FileOpener> opener) override {\n+\t\tVerifyNoOpener(opener);\n+\t\tVerifyCanAccessFile(filename);\n+\t\treturn GetFileSystem().TryRemoveFile(filename, GetOpener());\n+\t}\n+\n \tstring PathSeparator(const string &path) override {\n \t\treturn GetFileSystem().PathSeparator(path);\n \t}\n---\nsrc/include/duckdb/common/virtual_file_system.hpp\n@@ -48,6 +48,7 @@ class VirtualFileSystem : public FileSystem {\n \n \tbool IsPipe(const string &filename, optional_ptr<FileOpener> opener) override;\n \tvoid RemoveFile(const string &filename, optional_ptr<FileOpener> opener) override;\n+\tbool TryRemoveFile(const string &filename, optional_ptr<FileOpener> opener) override;\n \n \tvector<OpenFileInfo> Glob(const string &path, FileOpener *opener = nullptr) override;\n \n---\nsrc/main/extension/extension_install.cpp\n@@ -250,14 +250,10 @@ static void WriteExtensionFiles(FileSystem &fs, const string &temp_path, const s\n \tWriteExtensionMetadataFileToDisk(fs, metadata_tmp_path, info);\n \n \t// First remove the local extension we are about to replace\n-\tif (fs.FileExists(local_extension_path)) {\n-\t\tfs.RemoveFile(local_extension_path);\n-\t}\n+\tfs.TryRemoveFile(local_extension_path);\n \n \t// Then remove the old metadata file\n-\tif (fs.FileExists(metadata_file_path)) {\n-\t\tfs.RemoveFile(metadata_file_path);\n-\t}\n+\tfs.TryRemoveFile(metadata_file_path);\n \n \tfs.MoveFile(metadata_tmp_path, metadata_file_path);\n \tfs.MoveFile(temp_path, local_extension_path);\n@@ -495,9 +491,7 @@ ExtensionHelper::InstallExtensionInternal(DatabaseInstance &db, FileSystem &fs,\n \t\treturn nullptr;\n \t}\n \n-\tif (fs.FileExists(temp_path)) {\n-\t\tfs.RemoveFile(temp_path);\n-\t}\n+\tfs.TryRemoveFile(temp_path);\n \n \tif (ExtensionHelper::IsFullPath(extension) && options.repository) {\n \t\tthrow InvalidInputException(\"Cannot pass both a repository and a full path url\");\n---\nsrc/main/secret/secret_storage.cpp\n@@ -220,13 +220,9 @@ void LocalFileSecretStorage::WriteSecret(const BaseSecret &secret, OnCreateConfl\n \tstring temp_path = file_path + \".tmp-\" + UUID::ToString(UUID::GenerateRandomUUID());\n \n \t// If persistent file already exists remove\n-\tif (fs.FileExists(file_path)) {\n-\t\tfs.RemoveFile(file_path);\n-\t}\n+\tfs.TryRemoveFile(file_path);\n \t// If temporary file already exists remove\n-\tif (fs.FileExists(temp_path)) {\n-\t\tfs.RemoveFile(temp_path);\n-\t}\n+\tfs.TryRemoveFile(temp_path);\n \n \tWriteSecretFileToDisk(fs, temp_path, secret);\n \n---\nsrc/storage/storage_manager.cpp\n@@ -168,13 +168,9 @@ void SingleFileStorageManager::LoadDatabase(StorageOptions storage_options) {\n \t\t// file does not exist and we are in read-write mode\n \t\t// create a new file\n \n-\t\t// check if a WAL file already exists\n \t\tauto wal_path = GetWALPath();\n-\t\tif (fs.FileExists(wal_path)) {\n-\t\t\t// WAL file exists but database file does not\n-\t\t\t// remove the WAL\n-\t\t\tfs.RemoveFile(wal_path);\n-\t\t}\n+\t\t// try to remove the WAL file if it exists\n+\t\tfs.TryRemoveFile(wal_path);\n \n \t\t// Set the block allocation size for the new database file.\n \t\tif (storage_options.block_alloc_size.IsValid()) {\n---\nsrc/storage/write_ahead_log.cpp\n@@ -93,7 +93,7 @@ void WriteAheadLog::Delete() {\n \t}\n \twriter.reset();\n \tauto &fs = FileSystem::Get(database);\n-\tfs.RemoveFile(wal_path);\n+\tfs.TryRemoveFile(wal_path);\n \tinit_state = WALInitState::NO_WAL;\n \twal_size = 0;\n }\n---\ntest/helpers/test_helpers.cpp\n@@ -51,9 +51,7 @@ void TestDeleteDirectory(string path) {\n void TestDeleteFile(string path) {\n \tduckdb::unique_ptr<FileSystem> fs = FileSystem::CreateLocal();\n \ttry {\n-\t\tif (fs->FileExists(path)) {\n-\t\t\tfs->RemoveFile(path);\n-\t\t}\n+\t\tfs->TryRemoveFile(path);\n \t} catch (...) {\n \t}\n }\n---\n"}], "ideal": "Title: Add FileSystem::TryRemoveFile - that only removes a file if it exists\n\nBody: Many places in the code have the following pattern:\r\n\r\n```cpp\r\nif (fs.FileExists(local_extension_path)) {\r\n\tfs.RemoveFile(local_extension_path);\r\n}\r\n```\r\n\r\nThis is both racy (what if the file is removed after the `FileExists` call) and inefficient (need to do two syscalls/round-trips). This PR provides a new method `TryRemoveFile` that only removes files if it exists. Currently it still falls back to the original implementation, but we can pull it through to the various file system layers in follow-up PRs to avoid having to do two round-trips."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "CMakeLists.txt\n@@ -288,6 +288,16 @@ if(${CMAKE_SYSTEM_NAME} STREQUAL \"SunOS\")\n   set(SUN TRUE)\n endif()\n \n+if (OVERRIDE_GIT_DESCRIBE MATCHES \"^v[0-9]+\\.[0-9]+\\.[0-9]+\\-rc[0-9]+$\")\n+  message(STATUS \"HI FROM ME\")\n+  if (DUCKDB_EXPLICIT_VERSION)\n+    if (DUCKDB_EXPLICIT_PLATFORM STREQUAL DUCKDB_EXPLICIT_PLATFORM)\n+      message(FATAL_ERROR \"Provided OVERRIDE_GIT_DESCRIBE '${OVERRIDE_GIT_DESCRIBE}' and DUCKDB_EXPLICIT_PLATFORM '${DUCKDB_EXPLICIT_PLATFORM}' are both set and different\")\n+    endif()\n+  endif()\n+  set (DUCKDB_EXPLICIT_VERSION \"${OVERRIDE_GIT_DESCRIBE}\")\n+  unset (OVERRIDE_GIT_DESCRIBE CACHE)\n+endif()\n \n if (OVERRIDE_GIT_DESCRIBE)\n   if (OVERRIDE_GIT_DESCRIBE MATCHES \"^v[0-9]+\\.[0-9]+\\.[0-9]+\\-[0-9]+\\-g[a-f0-9]+$\")\n@@ -310,7 +320,7 @@ if (OVERRIDE_GIT_DESCRIBE)\n       set(GIT_DESCRIBE \"${OVERRIDE_GIT_DESCRIBE}-0-g0123456789\")\n     endif()\n   else()\n-    message(FATAL_ERROR \"Provided OVERRIDE_GIT_DESCRIBE '${OVERRIDE_GIT_DESCRIBE}' do not match supported versions, either fully specified 'vX.Y.Z-N-gGITHASH123' or version only 'vX.Y.Z'\")\n+    message(FATAL_ERROR \"Provided OVERRIDE_GIT_DESCRIBE '${OVERRIDE_GIT_DESCRIBE}' do not match supported versions, either fully specified 'vX.Y.Z-N-gGITHASH123' or version only 'vX.Y.Z' or rc like 'vX.Y.Z-rcW\")\n   endif()\n else()\n   find_package(Git)\n@@ -362,7 +372,8 @@ endif()\n \n string(SUBSTRING \"${GIT_COMMIT_HASH}\" 0 10 GIT_COMMIT_HASH)\n \n-if(NOT \"${DUCKDB_EXPLICIT_VERSION}\" STREQUAL \"\")\n+if(DUCKDB_EXPLICIT_VERSION)\n+  # Use with care, this forces the version to the provided string, potentially breaking invariants in the process\n   set(DUCKDB_VERSION \"${DUCKDB_EXPLICIT_VERSION}\")\n elseif(DUCKDB_DEV_ITERATION EQUAL 0)\n   # on a tag; directly use the version\n@@ -379,7 +390,9 @@ endif()\n \n string(REGEX MATCH \".*dev.*\" DUCKDB_EXTENSION_FOLDER_IS_VERSION \"${DUCKDB_VERSION}\")\n \n-if(DUCKDB_EXTENSION_FOLDER_IS_VERSION AND NOT GIT_COMMIT_HASH STREQUAL \"\")\n+if(DUCKDB_EXPLICIT_VERSION)\n+  set(DUCKDB_NORMALIZED_VERSION \"${DUCKDB_EXPLICIT_VERSION}\")\n+elseif(DUCKDB_EXTENSION_FOLDER_IS_VERSION AND NOT GIT_COMMIT_HASH STREQUAL \"\")\n   set(DUCKDB_NORMALIZED_VERSION \"${GIT_COMMIT_HASH}\")\n else()\n   set(DUCKDB_NORMALIZED_VERSION \"${DUCKDB_VERSION}\")\n---\nMakefile\n@@ -67,6 +67,13 @@ ifdef OVERRIDE_GIT_DESCRIBE\n else\n         COMMON_CMAKE_VARS:=${COMMON_CMAKE_VARS} -DOVERRIDE_GIT_DESCRIBE=\"\"\n endif\n+\n+ifdef DUCKDB_EXPLICIT_VERSION\n+        COMMON_CMAKE_VARS:=${COMMON_CMAKE_VARS} -DDUCKDB_EXPLICIT_VERSION=\"${DUCKDB_EXPLICIT_VERSION}\"\n+else\n+        COMMON_CMAKE_VARS:=${COMMON_CMAKE_VARS} -DDUCKDB_EXPLICIT_VERSION=\"\"\n+endif\n+\n ifneq (${CXX_STANDARD}, )\n         CMAKE_VARS:=${CMAKE_VARS} -DCMAKE_CXX_STANDARD=\"${CXX_STANDARD}\"\n endif\n---\n"}], "ideal": "Title: Allow passing down rc-style version also via OVERRIDE_GIT_DESCRIBE\n\nBody: Follow up of https://github.com/duckdb/duckdb/pull/17498, also allow OVERRIDE_GIT_DESCRIBE to carry the same information, given that's already wired correctly in all relevant places in our CI.\r\n\r\nRelevant change on top of the other PR is:\r\n```cmake\r\nif (OVERRIDE_GIT_DESCRIBE MATCHES \"^v[0-9]+\\.[0-9]+\\.[0-9]+\\-rc[0-9]+$\")\r\n  message(STATUS \"HI FROM ME\")\r\n  if (DUCKDB_EXPLICIT_VERSION)\r\n    if (DUCKDB_EXPLICIT_PLATFORM STREQUAL DUCKDB_EXPLICIT_PLATFORM)\r\n      message(FATAL_ERROR \"Provided OVERRIDE_GIT_DESCRIBE '${OVERRIDE_GIT_DESCRIBE}' and DUCKDB_EXPLICIT_PLATFORM '${DUCKDB_EXPLICIT_PLATFORM}' are both set and different\")\r\n    endif()\r\n  endif()\r\n  set (DUCKDB_EXPLICIT_VERSION \"${OVERRIDE_GIT_DESCRIBE}\")\r\n  unset (OVERRIDE_GIT_DESCRIBE CACHE)\r\nendif()\r\n```\r\n\r\nthat makes so that:\r\n```\r\nDUCKDB_EXPLICIT_VERSION=v1.3.0-rc0 GEN=ninja make\r\nOVERRIDE_GIT_DESCRIBE=v1.3.0-rc0 GEN=ninja make\r\n```\r\nare equivalent and both work, while providing a not yet understood version works via `DUCKDB_EXPLICIT_VERSION`\r\n```\r\nDUCKDB_EXPLICIT_VERSION=v1.3.0-rc0quack GEN=ninja make\r\n```\r\nbut fails for the more strict OVERRIDE_GIT_DESCRIBE\r\n```\r\nOVERRIDE_GIT_DESCRIBE=v1.3.0-rc0quack GEN=ninja make\r\n-- CMake Error at CMakeLists.txt:323 (message):\r\n--  Provided OVERRIDE_GIT_DESCRIBE 'v1.3.0-rc0quack' do not match supported\r\n--  versions, either fully specified 'vX.Y.Z-N-gGITHASH123' or version only\r\n--  'vX.Y.Z' or rc like 'vX.Y.Z-rcW\r\n```\r\n\r\nMain advantage of this is that the CI side should come for free.\r\n\r\nI am not sure how Python interprets this, that is yet to be tested, and might have to be disable if this is to be used."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/storage/table/column_checkpoint_state.cpp\n@@ -29,7 +29,6 @@ PartialBlockForCheckpoint::PartialBlockForCheckpoint(ColumnData &data, ColumnSeg\n }\n \n PartialBlockForCheckpoint::~PartialBlockForCheckpoint() {\n-\tD_ASSERT(IsFlushed() || Exception::UncaughtException());\n }\n \n bool PartialBlockForCheckpoint::IsFlushed() {\n---\ntest/sql/storage/compression/dict_fsst/dictionary_covers_validity.test\n@@ -1,6 +1,10 @@\n # name: test/sql/storage/compression/dict_fsst/dictionary_covers_validity.test\n # group: [dict_fsst]\n \n+# This test defaults to another compression function for smaller block sizes,\n+# because the bitpacking groups no longer fit the blocks.\n+require block_size 262144\n+\n load __TEST_DIR__/dictionary_covers_validity readwrite v1.3.0\n \n statement ok\n---\n"}], "ideal": "Title: Minor nightly fixes\n\nBody: * Require `block_size` for new DICT_FSST test\r\n* Remove incorrect assertion"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "CMakeLists.txt\n@@ -362,7 +362,8 @@ endif()\n \n string(SUBSTRING \"${GIT_COMMIT_HASH}\" 0 10 GIT_COMMIT_HASH)\n \n-if(NOT \"${DUCKDB_EXPLICIT_VERSION}\" STREQUAL \"\")\n+if(DUCKDB_EXPLICIT_VERSION)\n+  # Use with care, this forces the version to the provided string, potentially breaking invariants in the process\n   set(DUCKDB_VERSION \"${DUCKDB_EXPLICIT_VERSION}\")\n elseif(DUCKDB_DEV_ITERATION EQUAL 0)\n   # on a tag; directly use the version\n@@ -379,7 +380,9 @@ endif()\n \n string(REGEX MATCH \".*dev.*\" DUCKDB_EXTENSION_FOLDER_IS_VERSION \"${DUCKDB_VERSION}\")\n \n-if(DUCKDB_EXTENSION_FOLDER_IS_VERSION AND NOT GIT_COMMIT_HASH STREQUAL \"\")\n+if(DUCKDB_EXPLICIT_VERSION)\n+  set(DUCKDB_NORMALIZED_VERSION \"${DUCKDB_EXPLICIT_VERSION}\")\n+elseif(DUCKDB_EXTENSION_FOLDER_IS_VERSION AND NOT GIT_COMMIT_HASH STREQUAL \"\")\n   set(DUCKDB_NORMALIZED_VERSION \"${GIT_COMMIT_HASH}\")\n else()\n   set(DUCKDB_NORMALIZED_VERSION \"${DUCKDB_VERSION}\")\n---\nMakefile\n@@ -67,6 +67,13 @@ ifdef OVERRIDE_GIT_DESCRIBE\n else\n         COMMON_CMAKE_VARS:=${COMMON_CMAKE_VARS} -DOVERRIDE_GIT_DESCRIBE=\"\"\n endif\n+\n+ifdef DUCKDB_EXPLICIT_VERSION\n+        COMMON_CMAKE_VARS:=${COMMON_CMAKE_VARS} -DDUCKDB_EXPLICIT_VERSION=\"${DUCKDB_EXPLICIT_VERSION}\"\n+else\n+        COMMON_CMAKE_VARS:=${COMMON_CMAKE_VARS} -DDUCKDB_EXPLICIT_VERSION=\"\"\n+endif\n+\n ifneq (${CXX_STANDARD}, )\n         CMAKE_VARS:=${CMAKE_VARS} -DCMAKE_CXX_STANDARD=\"${CXX_STANDARD}\"\n endif\n---\n"}], "ideal": "Title: Allow DUCKDB_EXPLICIT_VERSION to be propagated\n\nBody: This means that for example the extension folder would look like:\r\n```\r\n% DUCKDB_EXPLICIT_VERSION=v1.3.0-rc0 make\r\n% tree build/release/repository\r\nbuild/release/repository\r\n\u2514\u2500\u2500 v1.3.0-rc0\r\n    \u2514\u2500\u2500 osx_arm64\r\n        \u251c\u2500\u2500 core_functions.duckdb_extension\r\n        \u2514\u2500\u2500 parquet.duckdb_extension\r\n```\r\ninstead of having the first 10 char of the sha, same for default extension metadata\r\n\r\nand this is propagated to the CLI initial message and the SQL layer\r\n```\r\n$ ./build/release/duckdb\r\nDuckDB v1.3.0-rc0 (Ossivalis) a1372fe956\r\nEnter \".help\" for usage hints.\r\nConnected to a transient in-memory database.\r\nUse \".open FILENAME\" to reopen on a persistent database.\r\nD PRAGMA version;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 library_version \u2502 source_id  \u2502 codename  \u2502\r\n\u2502     varchar     \u2502  varchar   \u2502  varchar  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 v1.3.0-rc0      \u2502 a1372fe956 \u2502 Ossivalis \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThis has to be used with care, since this effectively forces the version to the one provided, and this might have far reaching unintended consequences.\r\nI have not added a check on the format those needs to have, with the reasoning that this will be likely brittle, although it's probably best to allow only a limited set of regexes, and that if needed more can be added via modification of the CMake layer.\r\n\r\nThis is NOT exposed at the moment to CI, that to be done independently."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/common/http_util.hpp\n@@ -246,6 +246,6 @@ class HTTPUtil {\n public:\n \tstatic duckdb::unique_ptr<HTTPResponse>\n \tRunRequestWithRetry(const std::function<unique_ptr<HTTPResponse>(void)> &on_request, const BaseRequest &request,\n-\t                    const std::function<void(void)> &retry_cb = {});\n+\t                    const std::function<void(void)> &retry_cb);\n };\n } // namespace duckdb\n---\nsrc/main/http/http_util.cpp\n@@ -235,7 +235,7 @@ unique_ptr<HTTPResponse> HTTPUtil::SendRequest(BaseRequest &request, unique_ptr<\n \t// Refresh the client on retries\n \tstd::function<void(void)> on_retry([&]() { client = InitializeClient(request.params, request.proto_host_port); });\n \n-\treturn RunRequestWithRetry(on_request, request);\n+\treturn RunRequestWithRetry(on_request, request, on_retry);\n }\n \n void HTTPUtil::ParseHTTPProxyHost(string &proxy_value, string &hostname_out, idx_t &port_out, idx_t default_port) {\n---\n"}], "ideal": "Title: HTTPUtil Fix: correctly pass in on_retry\n\nBody: This method was accidentally left unused"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/src/python_replacement_scan.cpp\n@@ -241,12 +241,27 @@ static unique_ptr<TableRef> ReplaceInternal(ClientContext &context, const string\n \tauto scan_all_frames = result.GetValue<bool>();\n \n \tpy::gil_scoped_acquire acquire;\n-\tpy::object current_frame = py::module::import(\"inspect\").attr(\"currentframe\")();\n+\tpy::object current_frame;\n+\ttry {\n+\t\tcurrent_frame = py::module::import(\"inspect\").attr(\"currentframe\")();\n+\t} catch (py::error_already_set &e) {\n+\t\t//! Likely no call stack exists, just safely return\n+\t\treturn nullptr;\n+\t}\n \n \tbool has_locals = false;\n \tbool has_globals = false;\n \tdo {\n-\t\tpy::object local_dict_p = current_frame.attr(\"f_locals\");\n+\t\tif (py::none().is(current_frame)) {\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tpy::object local_dict_p;\n+\t\ttry {\n+\t\t\tlocal_dict_p = current_frame.attr(\"f_locals\");\n+\t\t} catch (py::error_already_set &e) {\n+\t\t\treturn nullptr;\n+\t\t}\n \t\thas_locals = !py::none().is(local_dict_p);\n \t\tif (has_locals) {\n \t\t\t// search local dictionary\n@@ -256,7 +271,12 @@ static unique_ptr<TableRef> ReplaceInternal(ClientContext &context, const string\n \t\t\t\treturn result;\n \t\t\t}\n \t\t}\n-\t\tpy::object global_dict_p = current_frame.attr(\"f_globals\");\n+\t\tpy::object global_dict_p;\n+\t\ttry {\n+\t\t\tglobal_dict_p = current_frame.attr(\"f_globals\");\n+\t\t} catch (py::error_already_set &e) {\n+\t\t\treturn nullptr;\n+\t\t}\n \t\thas_globals = !py::none().is(global_dict_p);\n \t\tif (has_globals) {\n \t\t\tauto global_dict = py::cast<py::dict>(global_dict_p);\n@@ -266,9 +286,10 @@ static unique_ptr<TableRef> ReplaceInternal(ClientContext &context, const string\n \t\t\t\treturn result;\n \t\t\t}\n \t\t}\n-\t\tcurrent_frame = current_frame.attr(\"f_back\");\n-\t\tif (py::none().is(current_frame)) {\n-\t\t\tbreak;\n+\t\ttry {\n+\t\t\tcurrent_frame = current_frame.attr(\"f_back\");\n+\t\t} catch (py::error_already_set &e) {\n+\t\t\treturn nullptr;\n \t\t}\n \t} while (scan_all_frames && (has_locals || has_globals));\n \treturn nullptr;\n---\n"}], "ideal": "Title: [Python Dev] Guard against python exceptions when interacting with the `currentframe` object\n\nBody: This PR fixes https://github.com/duckdblabs/duckdb-internal/issues/4835\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "scripts/regression/test_runner.py\n@@ -27,13 +27,6 @@ def geomean(xs):\n     return math.exp(math.fsum(math.log(float(x)) for x in xs) / len(xs))\n \n \n-# how many times we will run the experiment, to be sure of the regression\n-NUMBER_REPETITIONS = 5\n-# the threshold at which we consider something a regression (percentage)\n-REGRESSION_THRESHOLD_PERCENTAGE = 0.1\n-# minimal seconds diff for something to be a regression (for very fast benchmarks)\n-REGRESSION_THRESHOLD_SECONDS = 0.05\n-\n import argparse\n \n # Set up the argument parser\n@@ -50,6 +43,12 @@ def geomean(xs):\n parser.add_argument(\"--max-timeout\", type=int, default=3600, help=\"Set maximum timeout in seconds (default: 3600).\")\n parser.add_argument(\"--root-dir\", type=str, default=\"\", help=\"Root directory.\")\n parser.add_argument(\"--no-summary\", type=str, default=False, help=\"No summary in the end.\")\n+parser.add_argument(\n+    \"--regression-threshold-seconds\",\n+    type=float,\n+    default=0.05,\n+    help=\"REGRESSION_THRESHOLD_SECONDS value for large benchmarks.\",\n+)\n \n # Parse the arguments\n args = parser.parse_args()\n@@ -65,6 +64,15 @@ def geomean(xs):\n max_timeout = args.max_timeout\n root_dir = args.root_dir\n no_summary = args.no_summary\n+regression_threshold_seconds = args.regression_threshold_seconds\n+\n+\n+# how many times we will run the experiment, to be sure of the regression\n+NUMBER_REPETITIONS = 5\n+# the threshold at which we consider something a regression (percentage)\n+REGRESSION_THRESHOLD_PERCENTAGE = 0.1\n+# minimal seconds diff for something to be a regression (for very fast benchmarks)\n+REGRESSION_THRESHOLD_SECONDS = regression_threshold_seconds\n \n if not os.path.isfile(old_runner_path):\n     print(f\"Failed to find old runner {old_runner_path}\")\n---\n"}], "ideal": "Title: add regression_threshold_seconds argument to `regression/test_runner.py`\n\nBody: Current value of the `regression_threshold_seconds` is for very fast benchmarks (e.g. sf=0.1). Which more likely causes false positive regressions on large scale factor (sf=100) regression tests.\r\n\r\nWhen run large scale factor (sf=100) regression tests, we'd like to define a bigger `regression_threshold_seconds` value.\r\n\r\nRun the `regression/test_runner.py --regression-threshold-seconds=2.0` when with large scale factor. There is a default value of the regression threshold in seconds for very fast benchmarks, so run it without passing `regression-threshold-seconds` parameter to the test runner."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/NightlyTests.yml\n@@ -105,7 +105,7 @@ jobs:\n     needs: linux-memory-leaks\n     env:\n       GEN: ninja\n-      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet;httpfs\"\n+      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet;httpfs;spatial\"\n       DISABLE_SANITIZER: 1\n       CRASH_ON_ASSERT: 1\n       RUN_SLOW_VERIFIERS: 1\n---\nextension/parquet/column_writer.cpp\n@@ -225,7 +225,7 @@ void ColumnWriter::HandleDefineLevels(ColumnWriterState &state, ColumnWriterStat\n //===--------------------------------------------------------------------===//\n // Used to store the metadata for a WKB-encoded geometry column when writing\n // GeoParquet files.\n-class WKBColumnWriterState final : public StandardColumnWriterState<string_t, string_t, ParquetCastOperator> {\n+class WKBColumnWriterState final : public StandardColumnWriterState<string_t, string_t, ParquetStringOperator> {\n public:\n \tWKBColumnWriterState(ParquetWriter &writer, duckdb_parquet::RowGroup &row_group, idx_t col_idx)\n \t    : StandardColumnWriterState(writer, row_group, col_idx), geo_data(), geo_data_writer(writer.GetContext()) {\n---\nextension/parquet/geo_parquet.cpp\n@@ -413,7 +413,7 @@ unique_ptr<ColumnReader> GeoParquetFileMetadata::CreateColumnReader(ParquetReade\n \t\tauto child_reader = ColumnReader::CreateReader(reader, schema.children[0]);\n \n \t\t// Create an expression reader that applies the conversion function to the child reader\n-\t\treturn make_uniq<ExpressionColumnReader>(context, std::move(child_reader), std::move(expr));\n+\t\treturn make_uniq<ExpressionColumnReader>(context, std::move(child_reader), std::move(expr), schema);\n \t}\n \n \t// Otherwise, unrecognized encoding\n---\nextension/parquet/include/reader/expression_column_reader.hpp\n@@ -19,17 +19,18 @@ class ExpressionColumnReader : public ColumnReader {\n \tstatic constexpr const PhysicalType TYPE = PhysicalType::INVALID;\n \n public:\n-\tExpressionColumnReader(ClientContext &context, unique_ptr<ColumnReader> child_reader, unique_ptr<Expression> expr);\n \tExpressionColumnReader(ClientContext &context, unique_ptr<ColumnReader> child_reader, unique_ptr<Expression> expr,\n-\t                       unique_ptr<ParquetColumnSchema> expression_schema);\n-\n-\tunique_ptr<ParquetColumnSchema> cast_schema;\n+\t                       const ParquetColumnSchema &schema);\n+\tExpressionColumnReader(ClientContext &context, unique_ptr<ColumnReader> child_reader, unique_ptr<Expression> expr,\n+\t                       unique_ptr<ParquetColumnSchema> owned_schema);\n \n \tunique_ptr<ColumnReader> child_reader;\n \tDataChunk intermediate_chunk;\n \tunique_ptr<Expression> expr;\n \tExpressionExecutor executor;\n-\tunique_ptr<ParquetColumnSchema> expression_schema;\n+\n+\t// If this reader was created on top of a child reader, after-the-fact, the schema needs to live somewhere\n+\tunique_ptr<ParquetColumnSchema> owned_schema;\n \n public:\n \tvoid InitializeRead(idx_t row_group_idx_p, const vector<ColumnChunk> &columns, TProtocol &protocol_p) override;\n---\nextension/parquet/reader/expression_column_reader.cpp\n@@ -7,18 +7,18 @@ namespace duckdb {\n // Expression Column Reader\n //===--------------------------------------------------------------------===//\n ExpressionColumnReader::ExpressionColumnReader(ClientContext &context, unique_ptr<ColumnReader> child_reader_p,\n-                                               unique_ptr<Expression> expr_p)\n-    : ColumnReader(child_reader_p->Reader(), child_reader_p->Schema()), child_reader(std::move(child_reader_p)),\n+                                               unique_ptr<Expression> expr_p, const ParquetColumnSchema &schema_p)\n+    : ColumnReader(child_reader_p->Reader(), schema_p), child_reader(std::move(child_reader_p)),\n       expr(std::move(expr_p)), executor(context, expr.get()) {\n \tvector<LogicalType> intermediate_types {child_reader->Type()};\n \tintermediate_chunk.Initialize(reader.allocator, intermediate_types);\n }\n \n ExpressionColumnReader::ExpressionColumnReader(ClientContext &context, unique_ptr<ColumnReader> child_reader_p,\n                                                unique_ptr<Expression> expr_p,\n-                                               unique_ptr<ParquetColumnSchema> expression_schema_p)\n-    : ColumnReader(child_reader_p->Reader(), *expression_schema_p), child_reader(std::move(child_reader_p)),\n-      expr(std::move(expr_p)), executor(context, expr.get()), expression_schema(std::move(expression_schema_p)) {\n+                                               unique_ptr<ParquetColumnSchema> owned_schema_p)\n+    : ColumnReader(child_reader_p->Reader(), *owned_schema_p), child_reader(std::move(child_reader_p)),\n+      expr(std::move(expr_p)), executor(context, expr.get()), owned_schema(std::move(owned_schema_p)) {\n \tvector<LogicalType> intermediate_types {child_reader->Type()};\n \tintermediate_chunk.Initialize(reader.allocator, intermediate_types);\n }\n---\n"}], "ideal": "Title: Fix GeoParquet ExpressionColumnReader schema\n\nBody: This PR fixes an issue where GeoParquet support broke due to changes made to the parquet expression column reader. \r\n\r\nPreviously, if no schema was provided when creating a ExpressionColumnReader, the schema of the child column reader was used, but this no longer works as GEOMETRY columns now create a nested child schema for the underlying blob column, meaning that the resulting expression reader would have the same return type as the child, i.e. BLOB, not GEOMETRY. The fix here is simply to pass the schema in the ExpressionColumnReader constructor, and not copy the child schema. \r\n\r\nHowever, there are other places where we add an expression reader on top of an existing reader/schema, and create a new column schema. In these cases its important that this new schema lives as long as the ExpressionColumnReader, hence the ExpressionColumnReader now takes a schema either as a const reference (if it already has a corresponding schema, as is the case for GeoParquet), or as a \"owning\" unique pointer (if its added on top of an existing schema afterwards).\r\n\r\nAdditionally, this PR also adds spatial to the nightly test CI to avoid this breaking again unnoticed in the future."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/tests/fast/arrow/test_arrow_fetch.py\n@@ -19,18 +19,6 @@ def check_equal(duckdb_conn):\n \n \n class TestArrowFetch(object):\n-    def test_over_vector_size(self, duckdb_cursor):\n-        if not can_run:\n-            return\n-\n-        duckdb_conn = duckdb.connect()\n-        duckdb_conn.execute(\"CREATE TABLE test (a  INTEGER)\")\n-        for value in range(10000):\n-            duckdb_conn.execute(\"INSERT INTO  test VALUES (\" + str(value) + \");\")\n-        duckdb_conn.execute(\"INSERT INTO  test VALUES(NULL);\")\n-\n-        check_equal(duckdb_conn)\n-\n     def test_empty_table(self, duckdb_cursor):\n         if not can_run:\n             return\n---\n"}], "ideal": "Title: Remove duplicated arrow fetch test\n\nBody: # PR Summary\r\nSmall PR - removes the duplicated `test_over_vector_size` test - both contain the same logic so one of them is redundant. "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/binder/statement/bind_drop.cpp\n@@ -41,8 +41,8 @@ BoundStatement Binder::Bind(DropStatement &stmt) {\n \t\t\tproperties.RegisterDBRead(*catalog, context);\n \t\t}\n \t\tEntryLookupInfo entry_lookup(stmt.info->type, stmt.info->name);\n-\t\tauto entry = Catalog::GetEntry(context, stmt.info->catalog, stmt.info->schema, entry_lookup,\n-\t\t                               OnEntryNotFound::RETURN_NULL);\n+\t\tauto entry =\n+\t\t    Catalog::GetEntry(context, stmt.info->catalog, stmt.info->schema, entry_lookup, stmt.info->if_not_found);\n \t\tif (!entry) {\n \t\t\tbreak;\n \t\t}\n---\n"}], "ideal": "Title: DROP of missing entry should fail in binding\n\nBody: `DROP` statements should fail during bind if we fail to find an entry. Only `DROP <CATALOG TYPE> IF NOT EXISTS` statements should be tolerant to missing entries."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "test/data/json/sample.json\n@@ -0,0 +1,4 @@\n+[\n+    { \"id\": 1, \"name\": \"Alice\" },\n+    { \"id\": 2, \"name\": \"Bob\" }  ]\n+  \n\\ No newline at end of file\n---\ntest/sql/data/json_auto.test\n@@ -0,0 +1,6 @@\n+\n+-- Test for read_json_auto function\n+CREATE TABLE test_json AS \n+SELECT * FROM read_json_auto('test/data/json/sample.json');\n+\n+SELECT COUNT(*) FROM test_json;\n---\n"}], "ideal": "Title: Add unit test for read_json_auto function\n\nBody: This PR introduces a simple unit test for the `read_json_auto` function.\r\nIt creates a table from a sample JSON file and performs a basic row count check.\r\n\r\nThis contribution supports the testability and reliability of the DuckDB file reader module, aligning with software architecture principles such as modularity and verifiability.\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/storage/compression/dictionary/decompression.cpp\n@@ -90,11 +90,11 @@ void CompressedStringScanState::ScanToFlatVector(Vector &result, idx_t result_of\n \n void CompressedStringScanState::ScanToDictionaryVector(ColumnSegment &segment, Vector &result, idx_t result_offset,\n                                                        idx_t start, idx_t scan_count) {\n-\tD_ASSERT(start % BitpackingPrimitives::BITPACKING_ALGORITHM_GROUP_SIZE == 0);\n \tD_ASSERT(scan_count == STANDARD_VECTOR_SIZE);\n \tD_ASSERT(result_offset == 0);\n \n-\tidx_t decompress_count = BitpackingPrimitives::RoundUpToAlgorithmGroupSize(scan_count);\n+\tidx_t start_offset = start % BitpackingPrimitives::BITPACKING_ALGORITHM_GROUP_SIZE;\n+\tidx_t decompress_count = BitpackingPrimitives::RoundUpToAlgorithmGroupSize(scan_count + start_offset);\n \n \t// Create a selection vector of sufficient size if we don't already have one.\n \tif (!sel_vec || sel_vec_size < decompress_count) {\n@@ -104,9 +104,15 @@ void CompressedStringScanState::ScanToDictionaryVector(ColumnSegment &segment, V\n \n \t// Scanning 2048 values, emitting a dict vector\n \tdata_ptr_t dst = data_ptr_cast(sel_vec->data());\n-\tdata_ptr_t src = data_ptr_cast(&base_data[(start * current_width) / 8]);\n+\tdata_ptr_t src = data_ptr_cast(&base_data[((start - start_offset) * current_width) / 8]);\n \n-\tBitpackingPrimitives::UnPackBuffer<sel_t>(dst, src, scan_count, current_width);\n+\tBitpackingPrimitives::UnPackBuffer<sel_t>(dst, src, decompress_count, current_width);\n+\n+\tif (start_offset != 0) {\n+\t\tfor (idx_t i = 0; i < scan_count; i++) {\n+\t\t\tsel_vec->set_index(i, sel_vec->get_index(i + start_offset));\n+\t\t}\n+\t}\n \n \tresult.Dictionary(*(dictionary), dictionary_size, *sel_vec, scan_count);\n \tDictionaryVector::SetDictionaryId(result, to_string(CastPointerToValue(&segment)));\n---\nsrc/storage/compression/dictionary_compression.cpp\n@@ -135,8 +135,7 @@ void DictionaryCompressionStorage::StringScanPartial(ColumnSegment &segment, Col\n \tauto &scan_state = state.scan_state->Cast<CompressedStringScanState>();\n \n \tauto start = segment.GetRelativeIndex(state.row_index);\n-\tif (!ALLOW_DICT_VECTORS || scan_count != STANDARD_VECTOR_SIZE ||\n-\t    start % BitpackingPrimitives::BITPACKING_ALGORITHM_GROUP_SIZE != 0) {\n+\tif (!ALLOW_DICT_VECTORS || scan_count != STANDARD_VECTOR_SIZE) {\n \t\tscan_state.ScanToFlatVector(result, result_offset, start, scan_count);\n \t} else {\n \t\tscan_state.ScanToDictionaryVector(segment, result, result_offset, start, scan_count);\n---\n"}], "ideal": "Title: Emit dictionary vectors with unaligned start index\n\nBody: Hi,\r\n\r\nThis PR enables emitting Dictionary Vectors even if the start index is not aligned with `BITPACKING_ALGORITHM_GROUP_SIZE`. The logic is similar to `ScanToFlatVector` in the sense that it might read extra values and, at the end, it corrects the emitted selection vector.\r\n\r\nWhen looking into the `Title` and `URL` column of Clickbench, I noticed that the number of Dictionary Vectors emitted was far less than I expected (~68% of the values are dictionary encoded in the URL column for example). With this change, the number of emitted Dictionary Vector is almost three times higher for the mentioned columns. Therefore, it results in less memory usage and better utilization of `TryAddCompressedGroups` in Aggregate HashTable.\r\n\r\nPerformance results for queries 33, 34, 36, 37, and 38 of Clickbench:\r\n![emit_dict_comparison](https://github.com/user-attachments/assets/0a72c13f-33c2-4b11-8cf6-4ba01cd5d010)\r\n\r\nThe benchmark was ran on a MacBook Air M2, 16GB memory with 1 thread on 10% of the data. \r\n\r\nThe most notable performance gain was for query 33 with 10-12% speedup as it contains no filtering and is a single column `GROUP BY`.\r\n\r\nAre there any implicit behavior can be caused by this change? I have not encountered any so far.\r\nPlease let me know if you'd like me to adjust anything!\r\n\r\nThanks"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/include/core_functions/scalar/date_functions.hpp\n@@ -465,7 +465,7 @@ struct TimeTZSortKeyFun {\n \tstatic constexpr const char *Name = \"timetz_byte_comparable\";\n \tstatic constexpr const char *Parameters = \"time_tz\";\n \tstatic constexpr const char *Description = \"Converts a TIME WITH TIME ZONE to an integer sort key\";\n-\tstatic constexpr const char *Example = \"timetz_byte_comparable('18:18:16.21-07:00'::TIME_TZ)\";\n+\tstatic constexpr const char *Example = \"timetz_byte_comparable('18:18:16.21-07:00'::TIMETZ)\";\n \tstatic constexpr const char *Categories = \"\";\n \n \tstatic ScalarFunction GetFunction();\n---\nextension/core_functions/scalar/date/functions.json\n@@ -325,7 +325,7 @@\n         \"name\": \"timetz_byte_comparable\",\n         \"parameters\": \"time_tz\",\n         \"description\": \"Converts a TIME WITH TIME ZONE to an integer sort key\",\n-        \"example\": \"timetz_byte_comparable('18:18:16.21-07:00'::TIME_TZ)\",\n+        \"example\": \"timetz_byte_comparable('18:18:16.21-07:00'::TIMETZ)\",\n         \"type\": \"scalar_function\"\n     },\n     {\n---\n"}], "ideal": "Title: Fix TIMETZ cast in example\n\nBody: The previous example was failing:\r\n\r\n```\r\nD select timetz_byte_comparable('18:18:16.21-07:00'::TIME_TZ);\r\nCatalog Error:\r\nType with name TIME_TZ does not exist!\r\nDid you mean \"timestamp_ms\"?\r\n```"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/optimizer/join_order/relation_statistics_helper.hpp\n@@ -69,6 +69,7 @@ class RelationStatisticsHelper {\n \tstatic void CopyRelationStats(RelationStats &to, const RelationStats &from);\n \n private:\n+\tstatic idx_t GetDistinctCount(LogicalGet &get, ClientContext &context, idx_t column_id);\n };\n \n } // namespace duckdb\n---\nsrc/include/duckdb/storage/statistics/base_statistics.hpp\n@@ -138,7 +138,7 @@ class BaseStatistics {\n \tbool has_null;\n \t//! Whether or not the segment can contain values that are not null\n \tbool has_no_null;\n-\t// estimate that one may have even if distinct_stats==nullptr\n+\t//! estimate that one may have even if distinct_stats==nullptr\n \tidx_t distinct_count;\n \t//! Numeric and String stats\n \tunion {\n---\nsrc/optimizer/join_order/relation_statistics_helper.cpp\n@@ -54,6 +54,18 @@ static ExpressionBinding GetChildColumnBinding(Expression &expr) {\n \treturn ret;\n }\n \n+idx_t RelationStatisticsHelper::GetDistinctCount(LogicalGet &get, ClientContext &context, idx_t column_id) {\n+\tif (!get.function.statistics) {\n+\t\treturn 0;\n+\t}\n+\tauto column_statistics = get.function.statistics(context, get.bind_data.get(), column_id);\n+\tif (!column_statistics) {\n+\t\treturn 0;\n+\t}\n+\tauto distinct_count = column_statistics->GetDistinctCount();\n+\treturn distinct_count;\n+}\n+\n RelationStats RelationStatisticsHelper::ExtractGetStats(LogicalGet &get, ClientContext &context) {\n \tauto return_stats = RelationStats();\n \n@@ -68,33 +80,17 @@ RelationStats RelationStatisticsHelper::ExtractGetStats(LogicalGet &get, ClientC\n \t\treturn_stats.table_name = name;\n \t}\n \n-\t// if we can get the catalog table, then our column statistics will be accurate\n-\t// parquet readers etc. will still return statistics, but they initialize distinct column\n-\t// counts to 0.\n-\t// TODO: fix this, some file formats can encode distinct counts, we don't want to rely on\n-\t//  getting a catalog table to know that we can use statistics.\n-\tbool have_catalog_table_statistics = false;\n-\tif (get.GetTable()) {\n-\t\thave_catalog_table_statistics = true;\n-\t}\n-\n \t// first push back basic distinct counts for each column (if we have them).\n \tauto &column_ids = get.GetColumnIds();\n \tfor (idx_t i = 0; i < column_ids.size(); i++) {\n \t\tauto column_id = column_ids[i].GetPrimaryIndex();\n-\t\tbool have_distinct_count_stats = false;\n-\t\tif (get.function.statistics) {\n-\t\t\tcolumn_statistics = get.function.statistics(context, get.bind_data.get(), column_id);\n-\t\t\tif (column_statistics && have_catalog_table_statistics) {\n-\t\t\t\tauto distinct_count = MaxValue<idx_t>(1, column_statistics->GetDistinctCount());\n-\t\t\t\tauto column_distinct_count = DistinctCount({distinct_count, true});\n-\t\t\t\treturn_stats.column_distinct_count.push_back(column_distinct_count);\n-\t\t\t\treturn_stats.column_names.push_back(name + \".\" + get.names.at(column_id));\n-\t\t\t\thave_distinct_count_stats = true;\n-\t\t\t}\n-\t\t}\n-\t\tif (!have_distinct_count_stats) {\n-\t\t\t// currently treating the cardinality as the distinct count.\n+\t\tauto distinct_count = GetDistinctCount(get, context, column_id);\n+\t\tif (distinct_count > 0) {\n+\t\t\tauto column_distinct_count = DistinctCount({distinct_count, true});\n+\t\t\treturn_stats.column_distinct_count.push_back(column_distinct_count);\n+\t\t\treturn_stats.column_names.push_back(name + \".\" + get.names.at(column_id));\n+\t\t} else {\n+\t\t\t// treat the cardinality as the distinct count.\n \t\t\t// the cardinality estimator will update these distinct counts based\n \t\t\t// on the extra columns that are joined on.\n \t\t\tauto column_distinct_count = DistinctCount({cardinality_after_filters, false});\n---\n"}], "ideal": "Title: If distinct count from stats is 0, do not use it in Join Order Optimizer\n\nBody: This causes bad join orderings when reading from scanners that provide 0 as a distinct count.\r\nIf a table truly has a distinct count of 0, then the cardinality is also 0, so falling back to the cardinality count is fine in this case."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/capi/data_chunk-c.cpp\n@@ -113,7 +113,14 @@ uint64_t *duckdb_vector_get_validity(duckdb_vector vector) {\n \t\treturn nullptr;\n \t}\n \tauto v = reinterpret_cast<duckdb::Vector *>(vector);\n-\treturn duckdb::FlatVector::Validity(*v).GetData();\n+\tswitch (v->GetVectorType()) {\n+\tcase duckdb::VectorType::CONSTANT_VECTOR:\n+\t\treturn duckdb::ConstantVector::Validity(*v).GetData();\n+\tcase duckdb::VectorType::FLAT_VECTOR:\n+\t\treturn duckdb::FlatVector::Validity(*v).GetData();\n+\tdefault:\n+\t\treturn nullptr;\n+\t}\n }\n \n void duckdb_vector_ensure_validity_writable(duckdb_vector vector) {\n---\ntest/api/capi/capi_scalar_functions.cpp\n@@ -486,3 +486,74 @@ TEST_CASE(\"Test Scalar Function with Bind Info\", \"[capi]\") {\n \n \tREQUIRE(first_connection_id != second_connection_id);\n }\n+\n+void ListSum(duckdb_function_info, duckdb_data_chunk input, duckdb_vector output) {\n+\tauto input_vector = duckdb_data_chunk_get_vector(input, 0);\n+\tauto input_size = duckdb_data_chunk_get_size(input);\n+\tauto input_validity = duckdb_vector_get_validity(input_vector);\n+\n+\tauto list_entry = reinterpret_cast<duckdb_list_entry *>(duckdb_vector_get_data(input_vector));\n+\tauto list_child = duckdb_list_vector_get_child(input_vector);\n+\tauto child_validity = duckdb_vector_get_validity(list_child);\n+\tauto child_data = reinterpret_cast<uint64_t *>(duckdb_vector_get_data(list_child));\n+\n+\tauto result_data = reinterpret_cast<uint64_t *>(duckdb_vector_get_data(output));\n+\tduckdb_vector_ensure_validity_writable(output);\n+\tauto result_validity = duckdb_vector_get_validity(output);\n+\n+\tfor (idx_t row = 0; row < input_size; row++) {\n+\t\tif (!duckdb_validity_row_is_valid(input_validity, row)) {\n+\t\t\tduckdb_validity_set_row_invalid(result_validity, row);\n+\t\t\tcontinue;\n+\t\t}\n+\t\tauto entry = list_entry[row];\n+\t\tauto offset = entry.offset;\n+\t\tauto length = entry.length;\n+\t\tuint64_t sum = 0;\n+\t\tfor (idx_t idx = offset; idx < offset + length; idx++) {\n+\t\t\tif (duckdb_validity_row_is_valid(child_validity, idx)) {\n+\t\t\t\tsum += child_data[idx];\n+\t\t\t}\n+\t\t}\n+\t\tresult_data[row] = sum;\n+\t}\n+}\n+\n+static void CAPIRegisterListSum(duckdb_connection connection, const char *name) {\n+\tduckdb_state status;\n+\n+\tauto function = duckdb_create_scalar_function();\n+\tduckdb_scalar_function_set_name(function, name);\n+\n+\tauto ubigint_type = duckdb_create_logical_type(DUCKDB_TYPE_UBIGINT);\n+\tauto list_type = duckdb_create_list_type(ubigint_type);\n+\tduckdb_scalar_function_add_parameter(function, list_type);\n+\tduckdb_scalar_function_set_return_type(function, ubigint_type);\n+\tduckdb_destroy_logical_type(&list_type);\n+\tduckdb_destroy_logical_type(&ubigint_type);\n+\n+\tduckdb_scalar_function_set_function(function, ListSum);\n+\tstatus = duckdb_register_scalar_function(connection, function);\n+\tREQUIRE(status == DuckDBSuccess);\n+\tduckdb_destroy_scalar_function(&function);\n+}\n+\n+TEST_CASE(\"Test Scalar Functions - LIST\", \"[capi]\") {\n+\tCAPITester tester;\n+\tduckdb::unique_ptr<CAPIResult> result;\n+\n+\tREQUIRE(tester.OpenDatabase(nullptr));\n+\tCAPIRegisterListSum(tester.connection, \"my_list_sum\");\n+\n+\tresult = tester.Query(\"SELECT my_list_sum([1::uint64])\");\n+\tREQUIRE_NO_FAIL(*result);\n+\tREQUIRE(result->Fetch<uint64_t>(0, 0) == 1);\n+\n+\tresult = tester.Query(\"SELECT my_list_sum(NULL)\");\n+\tREQUIRE_NO_FAIL(*result);\n+\tREQUIRE(result->IsNull(0, 0));\n+\n+\tresult = tester.Query(\"SELECT my_list_sum([])\");\n+\tREQUIRE_NO_FAIL(*result);\n+\tREQUIRE(result->Fetch<uint64_t>(0, 0) == 0);\n+}\n---\n"}], "ideal": "Title: [Fix] Switch between constant and flat vector in C API\n\nBody: The scalar function callback sets the vector type to constant, if all input arguments are constant - so we need to add that open when retrieving validity masks.\r\n\r\nClose https://github.com/duckdb/duckdb/issues/17298."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/string_util.cpp\n@@ -562,18 +562,20 @@ unique_ptr<ComplexJSON> StringUtil::ParseJSONMap(const string &json, bool ignore\n \tyyjson_val *key, *value;\n \twhile ((key = yyjson_obj_iter_next(&iter))) {\n \t\tvalue = yyjson_obj_iter_get_val(key);\n+\t\tconst auto key_val = yyjson_get_str(key);\n+\t\tconst auto key_len = yyjson_get_len(key);\n \t\tauto type = yyjson_get_type(value);\n \t\tif (type == YYJSON_TYPE_STR) {\n \t\t\t// Since this is a string, we can directly add the value\n-\t\t\tconst auto key_val = yyjson_get_str(key);\n-\t\t\tconst auto key_len = yyjson_get_len(key);\n \t\t\tconst auto value_val = yyjson_get_str(value);\n \t\t\tconst auto value_len = yyjson_get_len(value);\n \t\t\tresult->AddObject(string(key_val, key_len), make_uniq<ComplexJSON>(string(value_val, value_len)));\n+\t\t} else if (type == YYJSON_TYPE_BOOL) {\n+\t\t\t// boolean values\n+\t\t\tbool bool_val = yyjson_get_bool(value);\n+\t\t\tresult->AddObject(string(key_val, key_len), make_uniq<ComplexJSON>(bool_val ? \"true\" : \"false\"));\n \t\t} else if (type == YYJSON_TYPE_OBJ) {\n \t\t\t// We recurse, this is a complex json\n-\t\t\tconst auto key_val = yyjson_get_str(key);\n-\t\t\tconst auto key_len = yyjson_get_len(key);\n \t\t\t// Convert the object value to a JSON string and recurse\n \t\t\tsize_t json_str_len;\n \t\t\tchar *json_str = yyjson_val_write(value, 0, &json_str_len);\n---\ntest/sql/json/test_json_bool.test\n@@ -0,0 +1,9 @@\n+# name: test/sql/json/test_json_bool.test\n+# description: Test if Json can parse bool\n+# group: [json]\n+\n+statement ok\n+set custom_profiling_settings='{\"optimizer_filter_pullup\":true}';\n+\n+statement ok\n+set custom_profiling_settings='{\"optimizer_filter_pullup\":\"true\"}';\n\\ No newline at end of file\n---\n"}], "ideal": "Title: fix parsing bool values in JSON\n\nBody: This PR enables to read boolean values while parsing a JSON into a map. Before, when we parsed, this threw an error for boolean values (e.g. while parsing some Arrow metadata)."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/scalar/array/array_functions.cpp\n@@ -7,18 +7,18 @@ namespace duckdb {\n static unique_ptr<FunctionData> ArrayGenericBinaryBind(ClientContext &context, ScalarFunction &bound_function,\n                                                        vector<unique_ptr<Expression>> &arguments) {\n \n-\tconst auto lhs_is_param = arguments[0]->HasParameter();\n-\tconst auto rhs_is_param = arguments[1]->HasParameter();\n-\n-\tif (lhs_is_param && rhs_is_param) {\n-\t\tthrow ParameterNotResolvedException();\n-\t}\n-\n \tconst auto &lhs_type = arguments[0]->return_type;\n \tconst auto &rhs_type = arguments[1]->return_type;\n \n-\tbound_function.arguments[0] = lhs_is_param ? rhs_type : lhs_type;\n-\tbound_function.arguments[1] = rhs_is_param ? lhs_type : rhs_type;\n+\tif (lhs_type.IsUnknown() && rhs_type.IsUnknown()) {\n+\t\tbound_function.arguments[0] = rhs_type;\n+\t\tbound_function.arguments[1] = lhs_type;\n+\t\tbound_function.return_type = LogicalType::UNKNOWN;\n+\t\treturn nullptr;\n+\t}\n+\n+\tbound_function.arguments[0] = lhs_type.IsUnknown() ? rhs_type : lhs_type;\n+\tbound_function.arguments[1] = rhs_type.IsUnknown() ? lhs_type : rhs_type;\n \n \tif (bound_function.arguments[0].id() != LogicalTypeId::ARRAY ||\n \t    bound_function.arguments[1].id() != LogicalTypeId::ARRAY) {\n---\nextension/core_functions/scalar/list/flatten.cpp\n@@ -162,9 +162,9 @@ static unique_ptr<FunctionData> ListFlattenBind(ClientContext &context, ScalarFu\n \n \tauto &input_type = arguments[0]->return_type;\n \tbound_function.arguments[0] = input_type;\n-\tif (input_type.id() == LogicalTypeId::UNKNOWN) {\n-\t\tbound_function.arguments[0] = LogicalType(LogicalTypeId::UNKNOWN);\n-\t\tbound_function.return_type = LogicalType(LogicalTypeId::SQLNULL);\n+\tif (input_type.IsUnknown()) {\n+\t\tbound_function.arguments[0] = LogicalType::UNKNOWN;\n+\t\tbound_function.return_type = LogicalType::UNKNOWN;\n \t\treturn nullptr;\n \t}\n \tD_ASSERT(input_type.id() == LogicalTypeId::LIST);\n@@ -174,9 +174,9 @@ static unique_ptr<FunctionData> ListFlattenBind(ClientContext &context, ScalarFu\n \t\tbound_function.return_type = input_type;\n \t\treturn make_uniq<VariableReturnBindData>(bound_function.return_type);\n \t}\n-\tif (child_type.id() == LogicalTypeId::UNKNOWN) {\n-\t\tbound_function.arguments[0] = LogicalType(LogicalTypeId::UNKNOWN);\n-\t\tbound_function.return_type = LogicalType(LogicalTypeId::SQLNULL);\n+\tif (child_type.IsUnknown()) {\n+\t\tbound_function.arguments[0] = LogicalType::UNKNOWN;\n+\t\tbound_function.return_type = LogicalType::UNKNOWN;\n \t\treturn nullptr;\n \t}\n \tD_ASSERT(child_type.id() == LogicalTypeId::LIST);\n---\nextension/core_functions/scalar/list/list_has_any_or_all.cpp\n@@ -12,29 +12,29 @@ static unique_ptr<FunctionData> ListHasAnyOrAllBind(ClientContext &context, Scal\n \targuments[0] = BoundCastExpression::AddArrayCastToList(context, std::move(arguments[0]));\n \targuments[1] = BoundCastExpression::AddArrayCastToList(context, std::move(arguments[1]));\n \n-\tconst auto lhs_is_param = arguments[0]->HasParameter();\n-\tconst auto rhs_is_param = arguments[1]->HasParameter();\n+\tconst auto &lhs_type = arguments[0]->return_type;\n+\tconst auto &rhs_type = arguments[1]->return_type;\n \n-\tif (lhs_is_param && rhs_is_param) {\n-\t\tthrow ParameterNotResolvedException();\n+\tif (lhs_type.IsUnknown() && rhs_type.IsUnknown()) {\n+\t\tbound_function.arguments[0] = rhs_type;\n+\t\tbound_function.arguments[1] = lhs_type;\n+\t\tbound_function.return_type = LogicalType::UNKNOWN;\n+\t\treturn nullptr;\n \t}\n \n-\tconst auto &lhs_list = arguments[0]->return_type;\n-\tconst auto &rhs_list = arguments[1]->return_type;\n+\t// LHS and RHS must have the same input type.\n+\t// Thus, we can proceed binding, even if we only know the type of one of the arguments.\n \n-\tif (lhs_is_param) {\n-\t\tbound_function.arguments[0] = rhs_list;\n-\t\tbound_function.arguments[1] = rhs_list;\n-\t\treturn nullptr;\n-\t}\n-\tif (rhs_is_param) {\n-\t\tbound_function.arguments[0] = lhs_list;\n-\t\tbound_function.arguments[1] = lhs_list;\n+\tif (lhs_type.IsUnknown() || rhs_type.IsUnknown()) {\n+\t\tbound_function.arguments[0] = lhs_type.IsUnknown() ? rhs_type : lhs_type;\n+\t\tbound_function.arguments[1] = rhs_type.IsUnknown() ? lhs_type : rhs_type;\n \t\treturn nullptr;\n \t}\n \n-\tbound_function.arguments[0] = lhs_list;\n-\tbound_function.arguments[1] = rhs_list;\n+\t// Ensure the lists have the same child type, else throw.\n+\n+\tbound_function.arguments[0] = lhs_type;\n+\tbound_function.arguments[1] = rhs_type;\n \n \tconst auto &lhs_child = ListType::GetChildType(bound_function.arguments[0]);\n \tconst auto &rhs_child = ListType::GetChildType(bound_function.arguments[1]);\n---\nsrc/function/function_binder.cpp\n@@ -142,21 +142,18 @@ optional_idx FunctionBinder::BindFunctionFromArguments(const string &name, Funct\n                                                        const vector<LogicalType> &arguments, ErrorData &error) {\n \tauto candidate_functions = BindFunctionsFromArguments<T>(name, functions, arguments, error);\n \tif (candidate_functions.empty()) {\n-\t\t// no candidates\n+\t\t// No candidates, return an invalid index.\n \t\treturn optional_idx();\n \t}\n \tif (candidate_functions.size() > 1) {\n-\t\t// multiple candidates, check if there are any unknown arguments\n-\t\tbool has_parameters = false;\n+\t\t// Multiple candidates, check if there are any unknown arguments.\n \t\tfor (auto &arg_type : arguments) {\n-\t\t\tif (arg_type.id() == LogicalTypeId::UNKNOWN) {\n-\t\t\t\t//! there are! we could not resolve parameters in this case\n+\t\t\tif (arg_type.IsUnknown()) {\n+\t\t\t\t// We cannot resolve the parameters to a function.\n \t\t\t\tthrow ParameterNotResolvedException();\n \t\t\t}\n \t\t}\n-\t\tif (!has_parameters) {\n-\t\t\treturn MultipleCandidateException(name, functions, candidate_functions, arguments, error);\n-\t\t}\n+\t\treturn MultipleCandidateException(name, functions, candidate_functions, arguments, error);\n \t}\n \treturn candidate_functions[0];\n }\n---\nsrc/function/scalar/list/contains_or_position.cpp\n@@ -43,17 +43,20 @@ static unique_ptr<FunctionData> ListSearchBind(ClientContext &context, ScalarFun\n \t\treturn make_uniq<VariableReturnBindData>(bound_function.return_type);\n \t}\n \n-\tconst auto list_is_param = list.id() == LogicalTypeId::UNKNOWN;\n-\tconst auto value_is_param = value.id() == LogicalTypeId::UNKNOWN;\n-\n-\tif (list_is_param) {\n-\t\tif (!value_is_param) {\n-\t\t\t// only list is a parameter, cast it to a list of value type\n-\t\t\tbound_function.arguments[0] = LogicalType::LIST(value);\n-\t\t\tbound_function.arguments[1] = value;\n-\t\t}\n-\t} else if (value_is_param) {\n-\t\t// only value is a parameter: we expect the child type of list\n+\tif (list.IsUnknown() && value.IsUnknown()) {\n+\t\tbound_function.arguments[0] = list;\n+\t\tbound_function.arguments[1] = value;\n+\t\treturn nullptr;\n+\t}\n+\n+\tif (list.IsUnknown()) {\n+\t\t// Only the list type is unknown.\n+\t\t// We can infer its type from the type of the value.\n+\t\tbound_function.arguments[0] = LogicalType::LIST(value);\n+\t\tbound_function.arguments[1] = value;\n+\t} else if (value.IsUnknown()) {\n+\t\t// Only the value type is unknown.\n+\t\t// We can infer its type from the child type of the list.\n \t\tbound_function.arguments[0] = list;\n \t\tbound_function.arguments[1] = ListType::GetChildType(list);\n \t} else {\n---\nsrc/include/duckdb/common/types.hpp\n@@ -269,6 +269,9 @@ struct LogicalType {\n \t\t}\n \t\treturn false;\n \t}\n+\tinline bool IsUnknown() const {\n+\t\treturn id_ == LogicalTypeId::UNKNOWN;\n+\t}\n \n \tinline shared_ptr<ExtraTypeInfo> GetAuxInfoShrPtr() const {\n \t\treturn type_info_;\n---\ntest/sql/catalog/function/test_macro_with_unknown_types.test\n@@ -0,0 +1,16 @@\n+# name: test/sql/catalog/function/test_macro_with_unknown_types.test\n+# description: Test MACRO binding with unknown parameter types.\n+# group: [function]\n+\n+statement ok\n+PRAGMA enable_verification;\n+\n+statement ok\n+CREATE TEMP MACRO m1(x, y) AS (\n+    SELECT list_has_all(x, y) AND list_has_all(y, x)\n+);\n+\n+query I\n+SELECT m1([1, 2], [1, 2]);\n+----\n+true\n---\n"}], "ideal": "Title: [Fix] Macro binding with unknown parameters in list_has_all and some other code tidying\n\nBody: Close https://github.com/duckdblabs/duckdb-internal/issues/4813."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "data/csv/header_only_2.csv\n@@ -0,0 +1 @@\n+foo;bar\n---\nsrc/execution/operator/csv_scanner/table_function/csv_multi_file_info.cpp\n@@ -133,10 +133,17 @@ void SchemaDiscovery(ClientContext &context, ReadCSVData &result, CSVReaderOptio\n \t\treturn_types = best_schema.GetTypes();\n \t}\n \tif (only_header_or_empty_files == current_file && !options.columns_set) {\n-\t\tfor (auto &type : return_types) {\n-\t\t\tD_ASSERT(type.id() == LogicalTypeId::BOOLEAN);\n+\t\tfor (idx_t i = 0; i < return_types.size(); i++) {\n+\t\t\tif (!options.sql_types_per_column.empty()) {\n+\t\t\t\tif (options.sql_types_per_column.find(names[i]) != options.sql_types_per_column.end()) {\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t} else if (i < options.sql_type_list.size()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tD_ASSERT(return_types[i].id() == LogicalTypeId::BOOLEAN);\n \t\t\t// we default to varchar if all files are empty or only have a header after all the sniffing\n-\t\t\ttype = LogicalType::VARCHAR;\n+\t\t\treturn_types[i] = LogicalType::VARCHAR;\n \t\t}\n \t}\n \tresult.csv_schema = best_schema;\n---\nsrc/function/table/sniff_csv.cpp\n@@ -155,10 +155,18 @@ static void CSVSniffFunction(ClientContext &context, TableFunctionInput &data_p,\n \tCSVSniffer sniffer(sniffer_options, file_options, buffer_manager, CSVStateMachineCache::Get(context));\n \tauto sniffer_result = sniffer.SniffCSV(data.force_match);\n \tif (sniffer.EmptyOrOnlyHeader()) {\n-\t\tfor (auto &type : sniffer_result.return_types) {\n-\t\t\tD_ASSERT(type.id() == LogicalTypeId::BOOLEAN);\n+\t\tfor (idx_t i = 0; i < sniffer_result.return_types.size(); i++) {\n+\t\t\tif (!sniffer_options.sql_types_per_column.empty()) {\n+\t\t\t\tif (sniffer_options.sql_types_per_column.find(sniffer_result.names[i]) !=\n+\t\t\t\t    sniffer_options.sql_types_per_column.end()) {\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t} else if (i < sniffer_options.sql_type_list.size()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tD_ASSERT(sniffer_result.return_types[i].id() == LogicalTypeId::BOOLEAN);\n \t\t\t// we default to varchar if all files are empty or only have a header after all the sniffing\n-\t\t\ttype = LogicalType::VARCHAR;\n+\t\t\tsniffer_result.return_types[i] = LogicalType::VARCHAR;\n \t\t}\n \t}\n \tstring str_opt;\n---\ntest/sql/copy/csv/test_header_only.test\n@@ -30,3 +30,74 @@ SELECT REGEXP_MATCHES(abs_file_name, 'foo')\n   FROM ( SELECT abs_file_name FROM read_csv(['data/csv/header_only.csv','data/csv/bool.csv','data/csv/header_only.csv'], header=True, ignore_errors=True))\n ----\n No function matches the given name and argument types 'regexp_matches(BOOLEAN, STRING_LITERAL)'. You might need to add explicit type casts.\n+\n+# Try to replace column type\n+query I\n+SELECT columns from sniff_csv('data/csv/header_only.csv', types = ['INTEGER'])\n+----\n+[{'name': abs_file_name, 'type': INTEGER}]\n+\n+query I\n+SELECT columns from sniff_csv('data/csv/header_only.csv', types = {'abs_file_name':'INTEGER'})\n+----\n+[{'name': abs_file_name, 'type': INTEGER}]\n+\n+statement ok\n+CREATE TABLE T AS from read_csv('data/csv/header_only.csv',types = ['INTEGER'])\n+\n+query IIIIII\n+DESCRIBE T\n+----\n+abs_file_name\tINTEGER\tYES\tNULL\tNULL\tNULL\n+\n+statement ok\n+CREATE OR REPLACE TABLE T AS from read_csv('data/csv/header_only.csv', types = {'abs_file_name':'INTEGER'})\n+\n+query IIIIII\n+DESCRIBE T\n+----\n+abs_file_name\tINTEGER\tYES\tNULL\tNULL\tNULL\n+\n+# Now with two columns\n+query I\n+SELECT columns from sniff_csv('data/csv/header_only_2.csv', types = ['INTEGER'])\n+----\n+[{'name': foo, 'type': INTEGER}, {'name': bar, 'type': VARCHAR}]\n+\n+query I\n+SELECT columns from sniff_csv('data/csv/header_only_2.csv', types = {'foo':'INTEGER'})\n+----\n+[{'name': foo, 'type': INTEGER}, {'name': bar, 'type': VARCHAR}]\n+\n+# This is clearly wrong\n+query I\n+SELECT columns from sniff_csv('data/csv/header_only_2.csv', types = {'bar':'INTEGER'})\n+----\n+[{'name': foo, 'type': VARCHAR}, {'name': bar, 'type': INTEGER}]\n+\n+statement ok\n+CREATE OR REPLACE TABLE T AS from read_csv('data/csv/header_only_2.csv', types = ['INTEGER'])\n+\n+query IIIIII\n+DESCRIBE T\n+----\n+foo\tINTEGER\tYES\tNULL\tNULL\tNULL\n+bar\tVARCHAR\tYES\tNULL\tNULL\tNULL\n+\n+statement ok\n+CREATE OR REPLACE TABLE T AS from read_csv('data/csv/header_only_2.csv', types = {'foo':'INTEGER'})\n+\n+query IIIIII\n+DESCRIBE T\n+----\n+foo\tINTEGER\tYES\tNULL\tNULL\tNULL\n+bar\tVARCHAR\tYES\tNULL\tNULL\tNULL\n+\n+statement ok\n+CREATE OR REPLACE TABLE T AS from read_csv('data/csv/header_only_2.csv', types = {'bar':'INTEGER'})\n+\n+query IIIIII\n+DESCRIBE T\n+----\n+foo\tVARCHAR\tYES\tNULL\tNULL\tNULL\n+bar\tINTEGER\tYES\tNULL\tNULL\tNULL\n---\n"}], "ideal": "Title: [CSV Sniffer] Proper type replacement in header only files\n\nBody: Fix: https://github.com/duckdb/duckdb/issues/17431"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/pyproject.toml\n@@ -28,6 +28,9 @@ Source = \"https://github.com/duckdb/duckdb/blob/main/tools/pythonpkg\"\n Issues = \"https://github.com/duckdb/duckdb/issues\"\n Changelog = \"https://github.com/duckdb/duckdb/releases\"\n \n+[tool.setuptools_scm]\n+root = \"../..\"\n+\n ### CI Builwheel configurations ###\n \n # Default config runs all tests and requires at least one extension to be tested against\n---\n"}], "ideal": "Title: Fix: pyproject.toml does not contain a tool.setuptools_scm section\n\nBody: Fix \"pyproject.toml does not contain a tool.setuptools_scm section\" by adding a minimal entry to pyproject.toml"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/json/json_functions/json_table_in_out.cpp\n@@ -97,7 +97,7 @@ static unique_ptr<GlobalTableFunctionState> JSONTableInOutInitGlobal(ClientConte\n \t\t\t}\n \t\t}\n \t}\n-\treturn result;\n+\treturn std::move(result);\n }\n \n struct JSONTableInOutRecursionNode {\n---\n"}], "ideal": "Title: Fix JSON extension compilation on Ubuntu 22.04\n\nBody: After #17406 the JSON extension fails compilation with GCC 11.0.4 on Ubuntu 22.04:\r\n\r\n```\r\n$ CORE_EXTENSIONS='json' BUILD_EXTENSIONS_ONLY=1 GEN=ninja make\r\nmkdir -p ./build/release && \\\r\ncd build/release && \\\r\ncmake -G \"Ninja\" -DFORCE_COLORED_OUTPUT=1        -DENABLE_EXTENSION_AUTOLOADING= -DENABLE_EXTENSION_AUTOINSTALL= -DBUILD_EXTENSIONS_ONLY=1 -DCORE_EXTENSIONS=\"json\" -DLOCAL_EXTENSION_REPO=\"\"  -DOVERRIDE_GIT_DESCRIBE=\"\"  -DCMAKE_BUILD_TYPE=Release ../.. && \\\r\ncmake --build . --config Release\r\n-- Could NOT find Python3 (missing: Python3_EXECUTABLE Interpreter)\r\n-- The C compiler identification is GNU 11.4.0\r\n-- The CXX compiler identification is GNU 11.4.0\r\n[...]\r\n-- Load extension 'json' from '/home/duck/duckdb/extensions' @ 30e907b52f\r\n-- Load extension 'core_functions' from '/home/duck/duckdb/extensions' @ 30e907b52f\r\n-- Load extension 'parquet' from '/home/duck/duckdb/extensions' @ 30e907b52f\r\n-- Load extension 'jemalloc' from '/home/duck/duckdb/extensions' @ 30e907b52f\r\n-- Extensions linked into DuckDB: [json, core_functions, parquet, jemalloc]\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/duck/duckdb/build/release\r\n[19/380] Building CXX object extension/json/CMak...nsion.dir/json_functions/json_table_in_out.cpp.o\r\nFAILED: extension/json/CMakeFiles/json_extension.dir/json_functions/json_table_in_out.cpp.o\r\n/usr/bin/c++ -DDUCKDB_BUILD_LIBRARY -DEXT_VERSION_JSON=\\\"30e907b52f\\\" -I/home/duck/duckdb/src/include -I/home/duck/duckdb/third_party/fsst -I/home/duck/duckdb/third_party/fmt/include -I/home/duck/duckdb/third_party/hyperloglog -I/home/duck/duckdb/third_party/fastpforlib -I/home/duck/duckdb/third_party/skiplist -I/home/duck/duckdb/third_party/fast_float -I/home/duck/duckdb/third_party/re2 -I/home/duck/duckdb/third_party/miniz -I/home/duck/duckdb/third_party/utf8proc/include -I/home/duck/duckdb/third_party/concurrentqueue -I/home/duck/duckdb/third_party/pcg -I/home/duck/duckdb/third_party/tdigest -I/home/duck/duckdb/third_party/mbedtls/include -I/home/duck/duckdb/third_party/jaro_winkler -I/home/duck/duckdb/third_party/yyjson/include -I/home/duck/duckdb/third_party/zstd/include -I/home/duck/duckdb/extension/json/include -O3 -DNDEBUG -O3 -DNDEBUG   -fPIC -fdiagnostics-color=always -std=c++11 -MD -MT extension/json/CMakeFiles/json_extension.dir/json_functions/json_table_in_out.cpp.o -MF extension/json/CMakeFiles/json_extension.dir/json_functions/json_table_in_out.cpp.o.d -o extension/json/CMakeFiles/json_extension.dir/json_functions/json_table_in_out.cpp.o -c /home/duck/duckdb/extension/json/json_functions/json_table_in_out.cpp\r\n/home/duck/duckdb/extension/json/json_functions/json_table_in_out.cpp: In function 'duckdb::unique_ptr<duckdb::GlobalTableFunctionState, std::default_delete<duckdb::GlobalTableFunctionState>, true> duckdb::JSONTableInOutInitGlobal(duckdb::ClientContext&, duckdb::TableFunctionInitInput&)':\r\n/home/duck/duckdb/extension/json/json_functions/json_table_in_out.cpp:100:16: error: could not convert 'result' from 'unique_ptr<duckdb::JSONTableInOutGlobalState,default_delete<duckdb::JSONTableInOutGlobalState>,[...]>' to 'unique_ptr<duckdb::GlobalTableFunctionState,default_delete<duckdb::GlobalTableFunctionState>,[...]>'\r\n  100 |         return result;\r\n      |                ^~~~~~\r\n      |                |\r\n      |                unique_ptr<duckdb::JSONTableInOutGlobalState,default_delete<duckdb::JSONTableInOutGlobalState>,[...]>\r\n```\r\n\r\nThe problem does not happen on later version of GCC or on CLang/MSVC.\r\n\r\nProposed change adds explicit `std::move()` cast to the result to satisfy GCC 11."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/lambda_functions.cpp\n@@ -229,15 +229,15 @@ void ListLambdaBindData::Serialize(Serializer &serializer, const optional_ptr<Fu\n \tserializer.WriteProperty(100, \"return_type\", bind_data.return_type);\n \tserializer.WritePropertyWithDefault(101, \"lambda_expr\", bind_data.lambda_expr, unique_ptr<Expression>());\n \tserializer.WriteProperty(102, \"has_index\", bind_data.has_index);\n-\tserializer.WriteProperty(103, \"has_initial\", bind_data.has_initial);\n+\tserializer.WritePropertyWithDefault<bool>(103, \"has_initial\", bind_data.has_initial, false);\n }\n \n unique_ptr<FunctionData> ListLambdaBindData::Deserialize(Deserializer &deserializer, ScalarFunction &) {\n \tauto return_type = deserializer.ReadProperty<LogicalType>(100, \"return_type\");\n \tauto lambda_expr = deserializer.ReadPropertyWithExplicitDefault<unique_ptr<Expression>>(101, \"lambda_expr\",\n \t                                                                                        unique_ptr<Expression>());\n \tauto has_index = deserializer.ReadProperty<bool>(102, \"has_index\");\n-\tauto has_initial = deserializer.ReadProperty<bool>(103, \"has_initial\");\n+\tauto has_initial = deserializer.ReadPropertyWithExplicitDefault<bool>(103, \"has_initial\", false);\n \treturn make_uniq<ListLambdaBindData>(return_type, std::move(lambda_expr), has_index, has_initial);\n }\n \n---\ntest/sql/attach/attach_lambda_view.test\n@@ -0,0 +1,45 @@\n+# name: test/sql/attach/attach_lambda_view.test\n+# description: Test lambdas in attached databases\n+# group: [attach]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+foreach i 2 3\n+\n+statement ok\n+ATTACH '__TEST_DIR__/version_1_${i}_0.db' (STORAGE_VERSION 'v1.${i}.0');\n+\n+statement ok\n+CREATE TABLE version_1_${i}_0.lists(l integer[], initial integer);\n+\n+statement ok\n+INSERT INTO version_1_${i}_0.lists VALUES ([1], -1), ([1, 2, 3], -2), (NULL, -3), ([-1, NULL, 2], -3);\n+\n+statement ok\n+CREATE VIEW version_1_${i}_0.reduced_lists AS\n+\tSELECT list_reduce(l, LAMBDA x, y : x + y, initial) AS r FROM version_1_${i}_0.lists;\n+\n+query I\n+FROM version_1_${i}_0.reduced_lists;\n+----\n+0\n+4\n+NULL\n+NULL\n+\n+statement ok\n+DETACH version_1_${i}_0\n+\n+statement ok\n+ATTACH '__TEST_DIR__/version_1_${i}_0.db'\n+\n+query I\n+FROM version_1_${i}_0.reduced_lists;\n+----\n+0\n+4\n+NULL\n+NULL\n+\n+endloop\n---\n"}], "ideal": "Title: [Serializer] Lambda Compatibilty Fix\n\nBody: #16602 introduced a new member to `ListLambdaBindData`; however, a default value was not set for the serializer/deserializer, which made it incompatible with previous versions. This has now been fixed. "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/csv_scanner/scanner/string_value_scanner.cpp\n@@ -749,7 +749,7 @@ bool LineError::HandleErrors(StringValueResult &result) {\n \t\tdefault:\n \t\t\tthrow InvalidInputException(\"CSV Error not allowed when inserting row\");\n \t\t}\n-\t\tresult.error_handler.Error(csv_error);\n+\t\tresult.error_handler.Error(csv_error, result.try_row);\n \t}\n \tif (is_error_in_line && scan_id != StringValueScanner::LINE_FINDER_ID) {\n \t\tif (result.sniffing) {\n@@ -777,7 +777,7 @@ void StringValueResult::NullPaddingQuotedNewlineCheck() const {\n \t\t// If we have null_padding set, we found a quoted new line, we are scanning the file in parallel; We error.\n \t\tLinesPerBoundary lines_per_batch(iterator.GetBoundaryIdx(), lines_read);\n \t\tauto csv_error = CSVError::NullPaddingFail(state_machine.options, lines_per_batch, path);\n-\t\terror_handler.Error(csv_error);\n+\t\terror_handler.Error(csv_error, try_row);\n \t}\n }\n \n@@ -847,13 +847,13 @@ bool StringValueResult::AddRowInternal() {\n \t\t\t\t\t    state_machine.options, cur_col_id - 1, lines_per_batch, borked_line,\n \t\t\t\t\t    current_line_position.begin.GetGlobalPosition(requested_size, first_nl),\n \t\t\t\t\t    last_position.GetGlobalPosition(requested_size, first_nl), path);\n-\t\t\t\t\terror_handler.Error(csv_error);\n+\t\t\t\t\terror_handler.Error(csv_error, try_row);\n \t\t\t\t} else {\n \t\t\t\t\tauto csv_error = CSVError::IncorrectColumnAmountError(\n \t\t\t\t\t    state_machine.options, cur_col_id - 1, lines_per_batch, borked_line,\n \t\t\t\t\t    current_line_position.begin.GetGlobalPosition(requested_size, first_nl),\n \t\t\t\t\t    last_position.GetGlobalPosition(requested_size), path);\n-\t\t\t\t\terror_handler.Error(csv_error);\n+\t\t\t\t\terror_handler.Error(csv_error, try_row);\n \t\t\t\t}\n \t\t\t}\n \t\t\t// If we are here we ignore_errors, so we delete this line\n@@ -966,6 +966,7 @@ StringValueScanner::StringValueScanner(idx_t scanner_idx_p, const shared_ptr<CSV\n \t\tlines_read += csv_file_scan->skipped_rows;\n \t}\n \titerator.buffer_size = state_machine->options.buffer_size_option.GetValue();\n+\tresult.try_row = scanner_idx == LINE_FINDER_ID;\n }\n \n StringValueScanner::StringValueScanner(const shared_ptr<CSVBufferManager> &buffer_manager,\n@@ -1710,19 +1711,24 @@ bool StringValueScanner::IsRowValid(CSVIterator &current_iterator) const {\n \t\treturn false;\n \t}\n \tconstexpr idx_t result_size = 1;\n-\tauto scan_finder = make_uniq<StringValueScanner>(StringValueScanner::LINE_FINDER_ID, buffer_manager,\n-\t                                                 state_machine_strict, make_shared_ptr<CSVErrorHandler>(),\n-\t                                                 csv_file_scan, false, current_iterator, result_size);\n-\tauto &tuples = scan_finder->ParseChunk();\n-\tcurrent_iterator.pos = scan_finder->GetIteratorPosition();\n-\tbool has_error = false;\n-\tif (tuples.current_errors.HasError()) {\n-\t\tif (tuples.current_errors.Size() != 1 || !tuples.current_errors.HasErrorType(MAXIMUM_LINE_SIZE)) {\n-\t\t\t// We ignore maximum line size errors\n-\t\t\thas_error = true;\n-\t\t}\n-\t}\n-\treturn (tuples.number_of_rows == 1 || tuples.first_line_is_comment) && !has_error && tuples.borked_rows.empty();\n+\tauto scan_finder = make_uniq<StringValueScanner>(LINE_FINDER_ID, buffer_manager, state_machine_strict,\n+\t                                                 make_shared_ptr<CSVErrorHandler>(), csv_file_scan, false,\n+\t                                                 current_iterator, result_size);\n+\ttry {\n+\t\tauto &tuples = scan_finder->ParseChunk();\n+\t\tcurrent_iterator.pos = scan_finder->GetIteratorPosition();\n+\t\tbool has_error = false;\n+\t\tif (tuples.current_errors.HasError()) {\n+\t\t\tif (tuples.current_errors.Size() != 1 || !tuples.current_errors.HasErrorType(MAXIMUM_LINE_SIZE)) {\n+\t\t\t\t// We ignore maximum line size errors\n+\t\t\t\thas_error = true;\n+\t\t\t}\n+\t\t}\n+\t\treturn (tuples.number_of_rows == 1 || tuples.first_line_is_comment) && !has_error && tuples.borked_rows.empty();\n+\t} catch (const Exception &e) {\n+\t\treturn false;\n+\t}\n+\treturn true;\n }\n \n ValidRowInfo StringValueScanner::TryRow(CSVState state, idx_t start_pos, idx_t end_pos) const {\n---\nsrc/execution/operator/csv_scanner/util/csv_error.cpp\n@@ -60,7 +60,7 @@ void CSVErrorHandler::ThrowError(const CSVError &csv_error) {\n \n void CSVErrorHandler::Error(const CSVError &csv_error, bool force_error) {\n \tlock_guard<mutex> parallel_lock(main_mutex);\n-\tif ((ignore_errors && !force_error) || (PrintLineNumber(csv_error) && !CanGetLine(csv_error.GetBoundaryIndex()))) {\n+\tif (!force_error && (ignore_errors || (PrintLineNumber(csv_error) && !CanGetLine(csv_error.GetBoundaryIndex())))) {\n \t\t// We store this error, we can't throw it now, or we are ignoring it\n \t\terrors.push_back(csv_error);\n \t\treturn;\n---\nsrc/include/duckdb/execution/operator/csv_scanner/string_value_scanner.hpp\n@@ -217,6 +217,9 @@ class StringValueResult : public ScannerResult {\n \tbool added_last_line = false;\n \tbool quoted_new_line = false;\n \n+\t//! If we are trying a row or not when figuring out the next row to start from.\n+\tbool try_row = false;\n+\n \tunsafe_unique_array<ParseTypeInfo> parse_types;\n \tvector<string> names;\n \n@@ -376,7 +379,7 @@ class StringValueScanner : public BaseScanner {\n \tidx_t start_pos;\n \t//! Pointer to the previous buffer handle, necessary for over-buffer values\n \tshared_ptr<CSVBufferHandle> previous_buffer_handle;\n-\t//! Strict state machine, is basically a state machine with rfc 4180 set to true, used to figure out new line.\n+\t//! Strict state machine is basically a state machine with rfc 4180 set to true, used to figure out a new line.\n \tshared_ptr<CSVStateMachine> state_machine_strict;\n };\n \n---\n"}], "ideal": "Title: Force errors when trying lines as early as possible\n\nBody: When detecting where a line starts in a CSV batch, if the file contains quoted newlines, it could create a situation where the entire batch needs to be processed before the system detects that it can't find a new row. This drastically slows down CSV processing.\r\n\r\nThis PR changes that so errors are reported as early as possible.\r\n\r\nFix: https://github.com/duckdb/duckdb/issues/17362"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/optimizer/statistics/operator/propagate_filter.cpp\n@@ -230,6 +230,11 @@ FilterPropagateResult StatisticsPropagator::HandleFilter(unique_ptr<Expression>\n \tif (ExpressionIsConstant(*condition, Value::BOOLEAN(true))) {\n \t\treturn FilterPropagateResult::FILTER_ALWAYS_TRUE;\n \t}\n+\n+\tif (ExpressionIsConstantOrNull(*condition, Value::BOOLEAN(true))) {\n+\t\treturn FilterPropagateResult::FILTER_TRUE_OR_NULL;\n+\t}\n+\n \tif (ExpressionIsConstant(*condition, Value::BOOLEAN(false)) ||\n \t    ExpressionIsConstantOrNull(*condition, Value::BOOLEAN(false))) {\n \t\treturn FilterPropagateResult::FILTER_FALSE_OR_NULL;\n---\nsrc/optimizer/statistics/operator/propagate_get.cpp\n@@ -1,8 +1,10 @@\n+#include \"duckdb/common/helper.hpp\"\n #include \"duckdb/optimizer/statistics_propagator.hpp\"\n #include \"duckdb/planner/expression/bound_columnref_expression.hpp\"\n #include \"duckdb/planner/filter/conjunction_filter.hpp\"\n #include \"duckdb/planner/filter/constant_filter.hpp\"\n #include \"duckdb/planner/filter/expression_filter.hpp\"\n+#include \"duckdb/planner/filter/null_filter.hpp\"\n #include \"duckdb/planner/operator/logical_get.hpp\"\n #include \"duckdb/planner/table_filter.hpp\"\n \n@@ -14,8 +16,12 @@ FilterPropagateResult StatisticsPropagator::PropagateTableFilter(ColumnBinding s\n \t\tauto &expr_filter = filter.Cast<ExpressionFilter>();\n \t\tauto column_ref = make_uniq<BoundColumnRefExpression>(stats.GetType(), stats_binding);\n \t\tauto filter_expr = expr_filter.ToExpression(*column_ref);\n-\t\tUpdateFilterStatistics(*filter_expr);\n-\t\treturn HandleFilter(filter_expr);\n+\t\t// handle the filter before updating the statistics\n+\t\t// otherwise the filter can be pruned by the updated statistics\n+\t\tauto copy_expr = filter_expr->Copy();\n+\t\tauto propagate_result = HandleFilter(filter_expr);\n+\t\tUpdateFilterStatistics(*copy_expr);\n+\t\treturn propagate_result;\n \t}\n \treturn filter.CheckStatistics(stats);\n }\n@@ -93,6 +99,10 @@ unique_ptr<NodeStatistics> StatisticsPropagator::PropagateStatistics(LogicalGet\n \t\t\t// erase this condition\n \t\t\tget.table_filters.filters.erase(table_filter_column);\n \t\t\tbreak;\n+\t\tcase FilterPropagateResult::FILTER_TRUE_OR_NULL:\n+\t\t\t// filter is true or null; we can replace this with a not null filter\n+\t\t\tget.table_filters.filters[table_filter_column] = make_uniq<IsNotNullFilter>();\n+\t\t\tbreak;\n \t\tcase FilterPropagateResult::FILTER_FALSE_OR_NULL:\n \t\tcase FilterPropagateResult::FILTER_ALWAYS_FALSE:\n \t\t\t// filter is always false; this entire filter should be replaced by an empty result block\n---\ntest/optimizer/statistics/statistics_null_comparison.test\n@@ -8,6 +8,12 @@ CREATE TABLE integers AS SELECT * FROM (VALUES (10, 1), (20, 2), (30, NULL)) tbl\n statement ok\n CREATE TABLE integers2 AS SELECT * FROM (VALUES (1), (2), (NULL)) tbl(i);\n \n+statement ok\n+CREATE TABLE t1 AS SELECT * FROM (VALUES (1)) tbl(c0);\n+\n+statement ok\n+CREATE TABLE t2 AS SELECT * FROM (VALUES (NULL)) tbl(c0);\n+\n statement ok\n PRAGMA enable_verification\n \n@@ -39,6 +45,12 @@ EXPLAIN SELECT * FROM integers WHERE i>j ORDER BY i;\n ----\n logical_opt\t<REGEX>:.*constant_or_null.*\n \n+# t2.c0>=t2.c0 means always true or null which is equals to is not null\n+query II\n+EXPLAIN SELECT * FROM t1 INNER JOIN t2 ON ((t2.c0)>=(t2.c0));\n+----\n+logical_opt\t<REGEX>:.*IS NOT NULL.*\n+\n # now verify that the results are correct\n query I\n SELECT i=j FROM integers ORDER BY i;\n@@ -70,3 +82,16 @@ SELECT * FROM integers WHERE i>j ORDER BY i;\n ----\n 10\t1\n 20\t2\n+\n+# relate issue 17338\n+query II\n+SELECT * FROM t1 INNER JOIN t2 ON ((t2.c0)>=(t2.c0));\n+----\n+\n+statement ok\n+INSERT INTO t2 VALUES(1);\n+\n+query II\n+SELECT * FROM t1 INNER JOIN t2 ON ((t2.c0)>=(t2.c0));\n+----\n+1\t1\n---\n"}], "ideal": "Title: Fixes filter pruning use the statistics updated by the same filter\n\nBody: This PR fixes #17338, which prunes filter by using the statistics updated by the same filter. Also replacing filter returning true or null with is_not_null filter."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/json/json_functions/read_json.cpp\n@@ -132,6 +132,10 @@ class JSONSchemaTask : public BaseExecutorTask {\n \t\t}\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"JSONSchemaTask\";\n+\t}\n+\n private:\n \tAutoDetectState &auto_detect_state;\n \tJSONStructureNode &node;\n---\nsrc/common/sort/partition_state.cpp\n@@ -581,6 +581,10 @@ class PartitionMergeTask : public ExecutorTask {\n \n \tTaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;\n \n+\tstring TaskType() const override {\n+\t\treturn \"PartitionMergeTask\";\n+\t}\n+\n private:\n \tstruct ExecutorCallback : public PartitionGlobalMergeStates::Callback {\n \t\texplicit ExecutorCallback(Executor &executor) : executor(executor) {\n---\nsrc/execution/operator/aggregate/physical_hash_aggregate.cpp\n@@ -488,6 +488,10 @@ class HashAggregateFinalizeTask : public ExecutorTask {\n public:\n \tTaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;\n \n+\tstring TaskType() const override {\n+\t\treturn \"HashAggregateFinalizeTask\";\n+\t}\n+\n private:\n \tClientContext &context;\n \tPipeline &pipeline;\n@@ -547,6 +551,10 @@ class HashAggregateDistinctFinalizeTask : public ExecutorTask {\n public:\n \tTaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;\n \n+\tstring TaskType() const override {\n+\t\treturn \"HashAggregateDistinctFinalizeTask\";\n+\t}\n+\n private:\n \tTaskExecutionResult AggregateDistinctGrouping(const idx_t grouping_idx);\n \n---\nsrc/execution/operator/aggregate/physical_ungrouped_aggregate.cpp\n@@ -445,6 +445,10 @@ class UngroupedDistinctAggregateFinalizeTask : public ExecutorTask {\n \n \tTaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;\n \n+\tstring TaskType() const override {\n+\t\treturn \"UngroupedDistinctAggregateFinalizeTask\";\n+\t}\n+\n private:\n \tTaskExecutionResult AggregateDistinct();\n \n---\nsrc/execution/operator/join/physical_hash_join.cpp\n@@ -469,6 +469,10 @@ class HashJoinTableInitTask : public ExecutorTask {\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"HashJoinTableInitTask\";\n+\t}\n+\n private:\n \tHashJoinGlobalSinkState &sink;\n \tidx_t entry_idx_from;\n@@ -524,6 +528,9 @@ class HashJoinFinalizeTask : public ExecutorTask {\n \t\tevent->FinishTask();\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n+\tstring TaskType() const override {\n+\t\treturn \"HashJoinFinalizeTask\";\n+\t}\n \n private:\n \tHashJoinGlobalSinkState &sink;\n@@ -607,6 +614,10 @@ class HashJoinRepartitionTask : public ExecutorTask {\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"HashJoinRepartitionTask\";\n+\t}\n+\n private:\n \tJoinHashTable &global_ht;\n \tJoinHashTable &local_ht;\n---\nsrc/execution/operator/join/physical_range_join.cpp\n@@ -102,6 +102,10 @@ class RangeJoinMergeTask : public ExecutorTask {\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"RangeJoinMergeTask\";\n+\t}\n+\n private:\n \tClientContext &context;\n \tGlobalSortedTable &table;\n---\nsrc/execution/operator/order/physical_order.cpp\n@@ -127,6 +127,10 @@ class PhysicalOrderMergeTask : public ExecutorTask {\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"PhysicalOrderMergeTask\";\n+\t}\n+\n private:\n \tClientContext &context;\n \tOrderGlobalSinkState &state;\n---\nsrc/execution/operator/persistent/physical_batch_copy_to_file.cpp\n@@ -265,6 +265,10 @@ class ProcessRemainingBatchesTask : public ExecutorTask {\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"ProcessRemainingBatchesTask\";\n+\t}\n+\n private:\n \tconst PhysicalBatchCopyToFile &op;\n \tFixedBatchCopyGlobalState &gstate;\n---\nsrc/include/duckdb/common/arrow/arrow_merge_event.hpp\n@@ -34,6 +34,10 @@ class ArrowBatchTask : public ExecutorTask {\n \tvoid ProduceRecordBatches();\n \tTaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;\n \n+\tstring TaskType() const override {\n+\t\treturn \"ArrowBatchTask\";\n+\t}\n+\n private:\n \tArrowQueryResult &result;\n \tvector<idx_t> record_batch_indices;\n---\nsrc/include/duckdb/common/multi_file/union_by_name.hpp\n@@ -32,6 +32,10 @@ class UnionByReaderTask : public BaseExecutorTask {\n \t\treaders[file_idx] = OP::GetUnionData(std::move(reader), file_idx);\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"UnionByReaderTask\";\n+\t}\n+\n private:\n \tClientContext &context;\n \tconst OpenFileInfo &file;\n---\nsrc/include/duckdb/parallel/pipeline.hpp\n@@ -33,6 +33,10 @@ class PipelineTask : public ExecutorTask {\n \tPipeline &pipeline;\n \tunique_ptr<PipelineExecutor> pipeline_executor;\n \n+\tstring TaskType() const override {\n+\t\treturn \"PipelineTask\";\n+\t}\n+\n public:\n \tconst PipelineExecutor &GetPipelineExecutor() const;\n \tbool TaskBlockedOnResult() const override;\n---\nsrc/include/duckdb/parallel/task.hpp\n@@ -52,6 +52,10 @@ class Task : public enable_shared_from_this<Task> {\n \t\treturn false;\n \t}\n \n+\tvirtual string TaskType() const {\n+\t\treturn \"UnnamedTask\";\n+\t}\n+\n public:\n \toptional_ptr<ProducerToken> token;\n };\n---\nsrc/parallel/pipeline_finish_event.cpp\n@@ -46,6 +46,10 @@ class PipelineFinishTask : public ExecutorTask {\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"PipelineFinishTask\";\n+\t}\n+\n private:\n #ifdef DUCKDB_DEBUG_ASYNC_SINK_SOURCE\n \t//! Debugging state: number of times blocked\n---\nsrc/parallel/pipeline_initialize_event.cpp\n@@ -22,6 +22,10 @@ class PipelineInitializeTask : public ExecutorTask {\n \t\tevent->FinishTask();\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n+\n+\tstring TaskType() const override {\n+\t\treturn \"PipelineInitializeTask\";\n+\t}\n };\n \n void PipelineInitializeEvent::Schedule() {\n---\nsrc/parallel/pipeline_prepare_finish_event.cpp\n@@ -20,6 +20,10 @@ class PipelinePreFinishTask : public ExecutorTask {\n \t\tevent->FinishTask();\n \t\treturn TaskExecutionResult::TASK_FINISHED;\n \t}\n+\n+\tstring TaskType() const override {\n+\t\treturn \"PipelinePreFinishTask\";\n+\t}\n };\n \n void PipelinePrepareFinishEvent::Schedule() {\n---\nsrc/storage/table/row_group_collection.cpp\n@@ -787,6 +787,10 @@ class CheckpointTask : public BaseCheckpointTask {\n \t\tcheckpoint_state.write_data[index] = row_group.WriteToDisk(*checkpoint_state.writers[index]);\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"CheckpointTask\";\n+\t}\n+\n private:\n \tidx_t index;\n };\n@@ -908,6 +912,10 @@ class VacuumTask : public BaseCheckpointTask {\n \t\t}\n \t}\n \n+\tstring TaskType() const override {\n+\t\treturn \"VacuumTask\";\n+\t}\n+\n private:\n \tVacuumState &vacuum_state;\n \tidx_t segment_idx;\n---\n"}], "ideal": "Title: Give tasks a `TaskType` with a name\n\nBody: This allows tasks executed by the task scheduler to be uniquely identified for e.g. logging purposes\r\n\r\nCC @bleskes "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/storage/wal_replay.cpp\n@@ -43,6 +43,18 @@ class ReplayState {\n \toptional_ptr<TableCatalogEntry> current_table;\n \tMetaBlockPointer checkpoint_id;\n \tidx_t wal_version = 1;\n+\n+\tstruct ReplayIndexInfo {\n+\t\tReplayIndexInfo(TableIndexList &index_list, unique_ptr<Index> index, const string &table_schema,\n+\t\t                const string &table_name)\n+\t\t    : index_list(index_list), index(std::move(index)), table_schema(table_schema), table_name(table_name) {};\n+\n+\t\treference<TableIndexList> index_list;\n+\t\tunique_ptr<Index> index;\n+\t\tstring table_schema;\n+\t\tstring table_name;\n+\t};\n+\tvector<ReplayIndexInfo> replay_index_infos;\n };\n \n class WriteAheadLogDeserializer {\n@@ -242,6 +254,13 @@ unique_ptr<WriteAheadLog> WriteAheadLog::ReplayInternal(AttachedDatabase &databa\n \t\t\tauto deserializer = WriteAheadLogDeserializer::Open(state, reader);\n \t\t\tif (deserializer.ReplayEntry()) {\n \t\t\t\tcon.Commit();\n+\n+\t\t\t\t// Commit any outstanding indexes.\n+\t\t\t\tfor (auto &info : state.replay_index_infos) {\n+\t\t\t\t\tinfo.index_list.get().AddIndex(std::move(info.index));\n+\t\t\t\t}\n+\t\t\t\tstate.replay_index_infos.clear();\n+\n \t\t\t\tsuccessful_offset = reader.offset;\n \t\t\t\t// check if the file is exhausted\n \t\t\t\tif (reader.Finished()) {\n@@ -390,6 +409,14 @@ void WriteAheadLogDeserializer::ReplayDropTable() {\n \t\treturn;\n \t}\n \n+\t// Remove any replay indexes of this table.\n+\tstate.replay_index_infos.erase(std::remove_if(state.replay_index_infos.begin(), state.replay_index_infos.end(),\n+\t                                              [&info](const ReplayState::ReplayIndexInfo &replay_info) {\n+\t\t                                              return replay_info.table_schema == info.schema &&\n+\t\t                                                     replay_info.table_name == info.name;\n+\t                                              }),\n+\t                               state.replay_index_infos.end());\n+\n \tcatalog.DropEntry(context, info);\n }\n \n@@ -488,7 +515,10 @@ void WriteAheadLogDeserializer::ReplayAlter() {\n \n \tauto index_type = context.db->config.GetIndexTypes().FindByName(ART::TYPE_NAME);\n \tauto index_instance = index_type->create_instance(input);\n-\tstorage.AddIndex(std::move(index_instance));\n+\n+\tauto &table_index_list = storage.GetDataTableInfo()->GetIndexes();\n+\tstate.replay_index_infos.emplace_back(table_index_list, std::move(index_instance), table_info.schema,\n+\t                                      table_info.name);\n \n \tcatalog.Alter(context, alter_info);\n }\n@@ -665,7 +695,11 @@ void WriteAheadLogDeserializer::ReplayCreateIndex() {\n \t\tinfo.index_type = ART::TYPE_NAME;\n \t}\n \n-\tauto &table = catalog.GetEntry<TableCatalogEntry>(context, create_info->schema, info.table).Cast<DuckTableEntry>();\n+\tconst auto schema_name = create_info->schema;\n+\tconst auto table_name = info.table;\n+\n+\tauto &entry = catalog.GetEntry<TableCatalogEntry>(context, schema_name, table_name);\n+\tauto &table = entry.Cast<DuckTableEntry>();\n \tauto &storage = table.GetStorage();\n \tauto &io_manager = TableIOManager::Get(storage);\n \n@@ -674,7 +708,9 @@ void WriteAheadLogDeserializer::ReplayCreateIndex() {\n \n \t// add the index to the storage\n \tauto unbound_index = make_uniq<UnboundIndex>(std::move(create_info), std::move(index_info), io_manager, db);\n-\tstorage.AddIndex(std::move(unbound_index));\n+\n+\tauto &table_index_list = storage.GetDataTableInfo()->GetIndexes();\n+\tstate.replay_index_infos.emplace_back(table_index_list, std::move(unbound_index), schema_name, table_name);\n }\n \n void WriteAheadLogDeserializer::ReplayDropIndex() {\n@@ -686,6 +722,14 @@ void WriteAheadLogDeserializer::ReplayDropIndex() {\n \t\treturn;\n \t}\n \n+\t// Remove the replay index, if any.\n+\tstate.replay_index_infos.erase(std::remove_if(state.replay_index_infos.begin(), state.replay_index_infos.end(),\n+\t                                              [&info](const ReplayState::ReplayIndexInfo &replay_info) {\n+\t\t                                              return replay_info.table_schema == info.schema &&\n+\t\t                                                     replay_info.index->GetIndexName() == info.name;\n+\t                                              }),\n+\t                               state.replay_index_infos.end());\n+\n \tcatalog.DropEntry(context, info);\n }\n \n---\ntest/sql/index/art/storage/test_art_wal_replay_drop_table.test\n@@ -0,0 +1,60 @@\n+# name: test/sql/index/art/storage/test_art_wal_replay_drop_table.test\n+# description: Test replaying the WAL after creating an index in a transaction, and then dropping the table.\n+# group: [storage]\n+\n+load __TEST_DIR__/art_wal_replay_drop_table.db\n+\n+statement ok\n+SET threads=1;\n+\n+statement ok\n+SET wal_autocheckpoint='1TB';\n+\n+statement ok\n+PRAGMA disable_checkpoint_on_shutdown;\n+\n+statement ok\n+CREATE TABLE test (a INTEGER);\n+\n+statement ok\n+INSERT INTO test SELECT range + 42 FROM range(100);\n+\n+statement ok\n+CREATE TABLE alter_test (a INTEGER);\n+\n+statement ok\n+INSERT INTO alter_test SELECT range + 42 FROM range(100);\n+\n+# Let's also have an index outside the transaction.\n+statement ok\n+CREATE INDEX other_idx ON test(a);\n+\n+statement ok\n+BEGIN TRANSACTION\n+\n+# We add this to the UndoBuffer on commit.\n+statement ok\n+INSERT INTO test VALUES (0), (1);\n+\n+statement ok\n+INSERT INTO alter_test VALUES (0), (1);\n+\n+# We add this to the UndoBuffer immediately.\n+statement ok\n+CREATE UNIQUE INDEX i_index ON test(a);\n+\n+statement ok\n+ALTER TABLE alter_test ADD PRIMARY KEY(a);\n+\n+# Now drop the tables in the transaction.\n+\n+statement ok\n+DROP TABLE test;\n+\n+statement ok\n+DROP TABLE alter_test;\n+\n+statement ok\n+COMMIT;\n+\n+restart\n---\ntest/sql/index/art/storage/test_art_wal_replay_in_tx.test\n@@ -0,0 +1,92 @@\n+# name: test/sql/index/art/storage/test_art_wal_replay_in_tx.test\n+# description: Test replaying the WAL after creating an index in a transaction.\n+# group: [storage]\n+\n+load __TEST_DIR__/art_wal_replay_tx.db\n+\n+statement ok\n+SET threads=1;\n+\n+statement ok\n+SET wal_autocheckpoint='1TB';\n+\n+statement ok\n+PRAGMA disable_checkpoint_on_shutdown;\n+\n+statement ok\n+CREATE TABLE test (a INTEGER);\n+\n+statement ok\n+INSERT INTO test SELECT range + 42 FROM range(100);\n+\n+statement ok\n+CREATE TABLE alter_test (a INTEGER);\n+\n+statement ok\n+INSERT INTO alter_test SELECT range + 42 FROM range(100);\n+\n+statement ok\n+CREATE TABLE drop_test (a INTEGER);\n+\n+statement ok\n+INSERT INTO drop_test SELECT range + 42 FROM range(100);\n+\n+# Let's also have an index outside the transaction.\n+statement ok\n+CREATE INDEX other_idx ON test(a);\n+\n+statement ok\n+BEGIN TRANSACTION\n+\n+# We add this to the UndoBuffer on commit.\n+statement ok\n+INSERT INTO test VALUES (0), (1);\n+\n+statement ok\n+INSERT INTO alter_test VALUES (0), (1);\n+\n+statement ok\n+INSERT INTO drop_test VALUES (0), (1);\n+\n+# We add this to the UndoBuffer immediately.\n+statement ok\n+CREATE UNIQUE INDEX i_index ON test(a);\n+\n+statement ok\n+ALTER TABLE alter_test ADD PRIMARY KEY(a);\n+\n+statement ok\n+CREATE INDEX drop_idx ON drop_test(a);\n+\n+statement ok\n+DROP INDEX drop_idx;\n+\n+statement ok\n+DELETE FROM test WHERE a = 1;\n+\n+statement ok\n+DELETE FROM alter_test WHERE a = 1;\n+\n+statement ok\n+COMMIT;\n+\n+restart\n+\n+statement error\n+INSERT INTO test VALUES (0);\n+----\n+<REGEX>:Constraint Error.*violates unique constraint.*\n+\n+statement error\n+INSERT INTO alter_test VALUES (0);\n+----\n+<REGEX>:Constraint Error.*violates primary key constraint.*\n+\n+statement ok\n+INSERT INTO test VALUES (1);\n+\n+statement ok\n+INSERT INTO alter_test VALUES (1);\n+\n+statement ok\n+CREATE INDEX drop_idx ON test(a);\n---\n"}], "ideal": "Title: WAL replay index fixes\n\nBody: Fixes a bug where we replay insertions after `CREATE INDEX`, if inside a transaction.\r\n\r\nCloses https://github.com/duckdblabs/duckdb-internal/issues/4716"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/catalog/catalog_entry/duck_schema_entry.cpp\n@@ -68,11 +68,14 @@ static void FindForeignKeyInformation(TableCatalogEntry &table, AlterForeignKeyT\n }\n \n DuckSchemaEntry::DuckSchemaEntry(Catalog &catalog, CreateSchemaInfo &info)\n-    : SchemaCatalogEntry(catalog, info), tables(catalog, make_uniq<DefaultViewGenerator>(catalog, *this)),\n-      indexes(catalog), table_functions(catalog, make_uniq<DefaultTableFunctionGenerator>(catalog, *this)),\n+    : SchemaCatalogEntry(catalog, info),\n+      tables(catalog, catalog.IsSystemCatalog() ? make_uniq<DefaultViewGenerator>(catalog, *this) : nullptr),\n+      indexes(catalog),\n+      table_functions(catalog,\n+                      catalog.IsSystemCatalog() ? make_uniq<DefaultTableFunctionGenerator>(catalog, *this) : nullptr),\n       copy_functions(catalog), pragma_functions(catalog),\n-      functions(catalog, make_uniq<DefaultFunctionGenerator>(catalog, *this)), sequences(catalog), collations(catalog),\n-      types(catalog, make_uniq<DefaultTypeGenerator>(catalog, *this)) {\n+      functions(catalog, catalog.IsSystemCatalog() ? make_uniq<DefaultFunctionGenerator>(catalog, *this) : nullptr),\n+      sequences(catalog), collations(catalog), types(catalog, make_uniq<DefaultTypeGenerator>(catalog, *this)) {\n }\n \n unique_ptr<CatalogEntry> DuckSchemaEntry::Copy(ClientContext &context) const {\n---\ntest/api/adbc/test_adbc.cpp\n@@ -1020,7 +1020,7 @@ TEST_CASE(\"Test ADBC ConnectionGetTableSchema\", \"[adbc]\") {\n \tREQUIRE(StringUtil::Contains(adbc_error.message, \"Catalog \\\"bla\\\" does not exist\"));\n \tadbc_error.release(&adbc_error);\n \n-\tREQUIRE(SUCCESS(AdbcConnectionGetTableSchema(&adbc_connection, \"memory\", \"main\", \"duckdb_indexes\", &arrow_schema,\n+\tREQUIRE(SUCCESS(AdbcConnectionGetTableSchema(&adbc_connection, \"system\", \"main\", \"duckdb_indexes\", &arrow_schema,\n \t                                             &adbc_error)));\n \tREQUIRE((arrow_schema.n_children == 14));\n \tarrow_schema.release(&arrow_schema);\n---\ntest/sql/pg_catalog/system_functions.test\n@@ -93,3 +93,8 @@ query I\n select pg_typeof(1);\n ----\n integer\n+\n+statement error\n+SELECT temp.current_user()\n+----\n+main.current_user\n---\ntest/sql/table_function/duckdb_tables.test\n@@ -34,3 +34,8 @@ memory\tmain\tintegers\tFalse\tFalse\t0\t1\t0\t0\n memory\tmyschema\tmytable\tFalse\tFalse\t0\t1\t0\t0\n temp\tmain\tmytemp\tTrue\tFalse\t0\t1\t0\t0\n memory\tmain\tpk\tFalse\tTrue\t0\t2\t1\t1\n+\n+statement error\n+SELECT * FROM temp.duckdb_tables\n+----\n+main.duckdb_tables\n---\n"}], "ideal": "Title: Avoid generating default views and macros in the temporary catalog\n\nBody: We should only be generating these in the `system` catalog - there is no need to generate them in the `temp` catalog."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/profiling_info.cpp\n@@ -39,6 +39,8 @@ ProfilingInfo::ProfilingInfo(const profiler_settings_t &n_settings, const idx_t\n profiler_settings_t ProfilingInfo::DefaultSettings() {\n \treturn {MetricsType::QUERY_NAME,\n \t        MetricsType::BLOCKED_THREAD_TIME,\n+\t        MetricsType::SYSTEM_PEAK_BUFFER_MEMORY,\n+\t        MetricsType::SYSTEM_PEAK_TEMP_DIR_SIZE,\n \t        MetricsType::CPU_TIME,\n \t        MetricsType::EXTRA_INFO,\n \t        MetricsType::CUMULATIVE_CARDINALITY,\n---\ntest/sql/pragma/profiling/test_custom_profiling_memory_and_temp_dir.test_slow\n@@ -13,14 +13,11 @@ PRAGMA enable_profiling = 'json';\n statement ok\n PRAGMA profiling_output = '__TEST_DIR__/profiling_output.json';\n \n-statement ok\n-PRAGMA custom_profiling_settings='{\"SYSTEM_PEAK_BUFFER_MEMORY\": \"true\", \"SYSTEM_PEAK_TEMP_DIR_SIZE\": \"true\"}';\n-\n statement ok\n SET memory_limit = '0.7gb';\n \n statement ok\n-CREATE TABLE test AS SELECT hash(range) i FROM range(100_000_000);\n+CREATE OR REPLACE TABLE test AS SELECT hash(range) i FROM range(100_000_000);\n \n statement ok\n PRAGMA disable_profiling;\n@@ -37,3 +34,29 @@ SELECT\n FROM metrics_output;\n ----\n true\ttrue\t1.0\n+\n+# disabling should omit them from output\n+statement ok\n+PRAGMA enable_profiling = 'json';\n+\n+statement ok\n+PRAGMA custom_profiling_settings='{\"SYSTEM_PEAK_BUFFER_MEMORY\": \"false\", \"SYSTEM_PEAK_TEMP_DIR_SIZE\": \"false\"}';\n+\n+statement ok\n+CREATE OR REPLACE TABLE test AS SELECT hash(range) i FROM range(100_000_000);\n+\n+statement ok\n+PRAGMA disable_profiling;\n+\n+statement ok\n+CREATE OR REPLACE TABLE metrics_output AS SELECT * FROM '__TEST_DIR__/profiling_output.json';\n+\n+statement error\n+SELECT system_peak_buffer_memory FROM metrics_output;\n+----\n+Binder Error\n+\n+statement error\n+SELECT system_peak_temp_dir_size FROM metrics_output;\n+----\n+Binder Error\n---\ntest/sql/pragma/profiling/test_custom_profiling_optimizer.test\n@@ -171,3 +171,5 @@ SELECT unnest(res) FROM (\n \"QUERY_NAME\": \"true\"\n \"RESULT_SET_SIZE\": \"true\"\n \"ROWS_RETURNED\": \"true\"\n+\"SYSTEM_PEAK_BUFFER_MEMORY\": \"true\"\n+\"SYSTEM_PEAK_TEMP_DIR_SIZE\": \"true\"\n---\ntest/sql/pragma/profiling/test_default_profiling_settings.test\n@@ -42,6 +42,8 @@ SELECT unnest(res) FROM (\n \"QUERY_NAME\": \"true\"\n \"RESULT_SET_SIZE\": \"true\"\n \"ROWS_RETURNED\": \"true\"\n+\"SYSTEM_PEAK_BUFFER_MEMORY\": \"true\"\n+\"SYSTEM_PEAK_TEMP_DIR_SIZE\": \"true\"\n \n statement ok\n CREATE OR REPLACE TABLE metrics_output AS SELECT * FROM '__TEST_DIR__/profiling_output.json';\n---\n"}], "ideal": "Title: Enable `SYSTEM_PEAK_BUFFER_MEMORY` and `SYSTEM_PEAK_TEMP_DIR_SIZE` profiling by default\n\nBody: This PR implements always collecting these statistics (introduced in https://github.com/duckdb/duckdb/pull/17164) when profiling is enabled."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/box_renderer.cpp\n@@ -1027,7 +1027,7 @@ void BoxRenderer::Render(ClientContext &context, const vector<string> &names, co\n \tauto collections = FetchRenderCollections(context, result, top_rows, bottom_rows);\n \tauto column_names = names;\n \tauto result_types = result.Types();\n-\tif (config.render_mode == RenderMode::COLUMNS) {\n+\tif (config.render_mode == RenderMode::COLUMNS && rows_to_render > 0) {\n \t\tcollections = PivotCollections(context, std::move(collections), column_names, result_types, row_count);\n \t}\n \n---\ntools/shell/shell.cpp\n@@ -3139,7 +3139,7 @@ MetadataResult ToggleEcho(ShellState &state, const char **azArg, idx_t nArg) {\n }\n \n MetadataResult ExitProcess(ShellState &state, const char **azArg, idx_t nArg) {\n-\tif (nArg >= 2) {\n+\tif (nArg > 2) {\n \t\treturn MetadataResult::PRINT_USAGE;\n \t}\n \tint rc = 0;\n@@ -3246,6 +3246,10 @@ MetadataResult EnableSafeMode(ShellState &state, const char **azArg, idx_t nArg)\n bool ShellState::SetOutputMode(const char *mode_str, const char *tbl_name) {\n \tidx_t n2 = StringLength(mode_str);\n \tchar c2 = mode_str[0];\n+\tif (tbl_name && !(c2 == 'i' && strncmp(mode_str, \"insert\", n2) == 0)) {\n+\t\traw_printf(stderr, \"TABLE argument can only be used with .mode insert\");\n+\t\treturn false;\n+\t}\n \tif (c2 == 'l' && n2 > 2 && strncmp(mode_str, \"lines\", n2) == 0) {\n \t\tmode = RenderMode::LINE;\n \t\trowSeparator = SEP_Row;\n---\ntools/shell/tests/test_readable_numbers.py\n@@ -0,0 +1,124 @@\n+# fmt: off\n+\n+import pytest\n+import subprocess\n+import sys\n+from typing import List\n+from conftest import ShellTest\n+import os\n+\n+def test_readable_numbers(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".large_number_rendering footer\")\n+        .statement(\"select 59986052 as count, 123456789 as count2, 9999999999 count3, -9999999999 count4;\")\n+    )\n+    result = test.run()\n+    result.check_stdout(\"(59.99 million)\")\n+    result.check_stdout(\"(123.46 million)\")\n+    result.check_stdout(\"(10.00 billion)\")\n+    result.check_stdout(\"(-10.00 billion)\")\n+\n+@pytest.mark.parametrize('test_rounding', [False, True])\n+def test_readable_numbers_exhaustive(shell, test_rounding):\n+    query = \"select \"\n+    for i in range(1, 20):\n+        if i > 1:\n+            query += \", \"\n+        if test_rounding:\n+            query += '9' * i\n+        else:\n+            query += '1' + ('0' * i)\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".large_number_rendering all\")\n+        .statement(\".maxwidth 99999\")\n+        .statement(query)\n+    )\n+    result = test.run()\n+    for unit in ['million', 'billion', 'trillion', 'quadrillion', 'quintillion']:\n+        for number in ['1.00', '10.00', '100.00']:\n+            if unit == 'quintillion' and number in ['10.00', '100.00']:\n+                continue\n+            result.check_stdout(number + \" \" + unit)\n+\n+def test_readable_numbers_rounding(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".large_number_rendering footer\")\n+        .statement(\".maxwidth 99999\")\n+        .statement(\"select 1005000, 1004999, -1005000, -1004999;\")\n+    )\n+    result = test.run()\n+    result.check_stdout(\"(1.01 million)\")\n+    result.check_stdout(\"(1.00 million)\")\n+    result.check_stdout(\"(-1.01 million)\")\n+    result.check_stdout(\"(-1.00 million)\")\n+\n+def test_readable_rounding_edge_case(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".large_number_rendering all\")\n+        .statement(\".maxwidth 99999\")\n+        .statement(\"select 994999, 995000\")\n+    )\n+    result = test.run()\n+    result.check_stdout(\"1.00 million\")\n+\n+def test_readable_numbers_limit(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".maxwidth 99999\")\n+        .statement(\".large_number_rendering all\")\n+        .statement(\"select 18446744073709551616, -18446744073709551616, 9999999999999999999, -9999999999999999999;\")\n+    )\n+    result = test.run()\n+    result.check_stdout(\"10.00 quintillion\")\n+    result.check_stdout(\"-10.00 quintillion\")\n+\n+def test_decimal_separator(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".decimal_sep ,\")\n+        .statement(\".large_number_rendering all\")\n+        .statement(\"select 59986052, 59986052.5, 999999999.123456789, 1e20, 'nan'::double;\")\n+    )\n+    result = test.run()\n+    result.check_stdout(\"59,99 million\")\n+    result.check_stdout(\"1,00 billion\")\n+\n+def test_odd_floating_points(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\"select 1e20, 'nan'::double;\")\n+    )\n+    result = test.run()\n+    result.check_stdout(\"nan\")\n+\n+def test_disable_readable_numbers(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".large_number_rendering off\")\n+        .statement(\"select 123456789;\")\n+    )\n+    result = test.run()\n+    result.check_not_exist('(123.46 million)')\n+\n+def test_large_number_rendering_all(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".large_number_rendering all\")\n+        .statement(\"select 123456789 from range(10);\")\n+    )\n+    result = test.run()\n+    result.check_stdout('123.46 million')\n+    result.check_not_exist('(123.46 million)')\n+\n+def test_readable_numbers_columns(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".columns\")\n+        .statement(\"select 123456789;\")\n+    )\n+    result = test.run()\n+    result.check_not_exist('(123.46 million)')\n---\ntools/shell/tests/test_shell_basics.py\n@@ -182,6 +182,11 @@ def test_exit(shell, alias):\n     test = ShellTest(shell).statement(f\".{alias}\")\n     result = test.run()\n \n+def test_exit_rc(shell):\n+    test = ShellTest(shell).statement(f\".exit 17\")\n+    result = test.run()\n+    assert result.status_code == 17\n+\n def test_print(shell):\n     test = ShellTest(shell).statement(\".print asdf\")\n     result = test.run()\n@@ -636,6 +641,15 @@ def test_mode_insert(shell):\n     result.check_not_exist('3.139999')\n     result.check_not_exist('2.710000')\n \n+def test_mode_insert_table(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".mode insert my_table\")\n+        .statement(\"SELECT 42;\")\n+    )\n+    result = test.run()\n+    result.check_stdout('my_table')\n+\n def test_mode_line(shell):\n     test = (\n         ShellTest(shell)\n---\ntools/shell/tests/test_shell_rendering.py\n@@ -40,3 +40,12 @@ def test_markdown(shell):\n \n     result = test.run()\n     result.check_stdout(\"| a  |  str  |\")\n+\n+def test_mode_insert_table(shell):\n+    test = (\n+        ShellTest(shell)\n+        .statement(\".mode box mytable\")\n+    )\n+\n+    result = test.run()\n+    result.check_stderr(\"TABLE argument can only be used with .mode insert\")\n---\n"}], "ideal": "Title: Minor fixes for the CLI\n\nBody: * Correctly support `.exit [CODE]`\r\n* Throw error when using `.mode ... TABLE` for any mode except for `.mode insert`\r\n* Fix for rendering empty results in `.columns` in DuckBox mode\r\n* Add a test that was previously not checked in by accident (for readable numbers rendering)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/parser/qualified_name.hpp\n@@ -24,6 +24,7 @@ struct QualifiedName {\n \t//! Parse the (optional) schema and a name from a string in the format of e.g. \"schema\".\"table\"; if there is no dot\n \t//! the schema will be set to INVALID_SCHEMA\n \tstatic QualifiedName Parse(const string &input);\n+\tstatic vector<string> ParseComponents(const string &input);\n \tstring ToString() const;\n };\n \n---\nsrc/parser/qualified_name.cpp\n@@ -7,13 +7,11 @@ string QualifiedName::ToString() const {\n \treturn ParseInfo::QualifierToString(catalog, schema, name);\n }\n \n-QualifiedName QualifiedName::Parse(const string &input) {\n-\tstring catalog;\n-\tstring schema;\n-\tstring name;\n+vector<string> QualifiedName::ParseComponents(const string &input) {\n+\tvector<string> result;\n \tidx_t idx = 0;\n-\tvector<string> entries;\n \tstring entry;\n+\n normal:\n \t//! quote\n \tfor (; idx < input.size(); idx++) {\n@@ -27,7 +25,7 @@ QualifiedName QualifiedName::Parse(const string &input) {\n \t}\n \tgoto end;\n separator:\n-\tentries.push_back(entry);\n+\tresult.push_back(entry);\n \tentry = \"\";\n \tidx++;\n \tgoto normal;\n@@ -41,22 +39,38 @@ QualifiedName QualifiedName::Parse(const string &input) {\n \t\t}\n \t\tentry += input[idx];\n \t}\n-\tthrow ParserException(\"Unterminated quote in qualified name!\");\n+\tthrow ParserException(\"Unterminated quote in qualified name! (input: %s)\", input);\n end:\n+\tif (!entry.empty()) {\n+\t\tresult.push_back(entry);\n+\t}\n+\treturn result;\n+}\n+\n+QualifiedName QualifiedName::Parse(const string &input) {\n+\tstring catalog;\n+\tstring schema;\n+\tstring name;\n+\n+\tauto entries = ParseComponents(input);\n \tif (entries.empty()) {\n \t\tcatalog = INVALID_CATALOG;\n \t\tschema = INVALID_SCHEMA;\n-\t\tname = entry;\n \t} else if (entries.size() == 1) {\n \t\tcatalog = INVALID_CATALOG;\n-\t\tschema = entries[0];\n-\t\tname = entry;\n+\t\tschema = INVALID_SCHEMA;\n+\t\tname = entries[0];\n \t} else if (entries.size() == 2) {\n+\t\tcatalog = INVALID_CATALOG;\n+\t\tschema = entries[0];\n+\t\tname = entries[1];\n+\t} else if (entries.size() == 3) {\n \t\tcatalog = entries[0];\n \t\tschema = entries[1];\n-\t\tname = entry;\n+\t\tname = entries[2];\n \t} else {\n-\t\tthrow ParserException(\"Expected catalog.entry, schema.entry or entry: too many entries found\");\n+\t\tthrow ParserException(\"Expected catalog.entry, schema.entry or entry: too many entries found (input: %s)\",\n+\t\t                      input);\n \t}\n \treturn QualifiedName {catalog, schema, name};\n }\n---\n"}], "ideal": "Title: [Dev] Add `QualifiedName::ParseComponents`, add input to the error messages\n\nBody: I'm slightly abusing the `QualifiedName::Parse` method in iceberg (https://github.com/duckdb/duckdb-iceberg/pull/224), which causes a problem when the path contains multiple dots.\r\nI'm adding `QualifiedName::ParseComponents` so we can make use of the parsing logic safely.\r\n\r\nOne other thing that stood out to me was that an empty input is not invalid, which feels wrong, but I preserved the behavior in this change."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/exception.cpp\n@@ -355,7 +355,23 @@ InvalidConfigurationException::InvalidConfigurationException(const string &msg,\n     : Exception(ExceptionType::INVALID_CONFIGURATION, msg, extra_info) {\n }\n \n-OutOfMemoryException::OutOfMemoryException(const string &msg) : Exception(ExceptionType::OUT_OF_MEMORY, msg) {\n+OutOfMemoryException::OutOfMemoryException(const string &msg)\n+    : Exception(ExceptionType::OUT_OF_MEMORY, ExtendOutOfMemoryError(msg)) {\n+}\n+\n+string OutOfMemoryException::ExtendOutOfMemoryError(const string &msg) {\n+\tstring link = \"https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads\";\n+\tif (StringUtil::Contains(msg, link)) {\n+\t\t// already extended\n+\t\treturn msg;\n+\t}\n+\tstring new_msg = msg;\n+\tnew_msg += \"\\n\\nPossible solutions:\\n\";\n+\tnew_msg += \"* Reducing the number of threads (SET threads=X)\\n\";\n+\tnew_msg += \"* Disabling insertion-order preservation (SET preserve_insertion_order=false)\\n\";\n+\tnew_msg += \"* Increasing the memory limit (SET memory_limit='...GB')\\n\";\n+\tnew_msg += \"\\nSee also \" + link;\n+\treturn new_msg;\n }\n \n ParameterNotAllowedException::ParameterNotAllowedException(const string &msg)\n---\nsrc/include/duckdb/common/exception.hpp\n@@ -192,6 +192,9 @@ class OutOfMemoryException : public Exception {\n \texplicit OutOfMemoryException(const string &msg, ARGS... params)\n \t    : OutOfMemoryException(ConstructMessage(msg, params...)) {\n \t}\n+\n+private:\n+\tstring ExtendOutOfMemoryError(const string &msg);\n };\n \n class SyntaxException : public Exception {\n---\n"}], "ideal": "Title: Provide suggestions and a link to the documentation for OOM errors\n\nBody: This PR extends the `OutOfMemoryException` with extra text that can help the user to remedy the issue, e.g.:\r\n\r\n```sql\r\nD set memory_limit='100mb';\r\nD select list(l_orderkey) from lineitemsf10.parquet;\r\n```\r\n\r\n```\r\nOut of Memory Error:\r\nfailed to allocate data of size 4.0 MiB (93.7 MiB/95.3 MiB used)\r\n\r\nPossible solutions:\r\n* Reducing the number of threads (SET threads=X)\r\n* Disabling insertion-order preservation (SET preserve_insertion_order=false)\r\n* Increasing the memory limit (SET memory_limit='...GB')\r\n\r\nSee also https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads\r\n\r\n```"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/Python.yml\n@@ -64,6 +64,7 @@ env:\n jobs:\n # This is just a sanity check of Python 3.10 running with Arrow / Spark\n   linux-python3-10:\n+    if: false\n     name: Python 3.10 Linux\n     runs-on: ubuntu-22.04\n \n---\n"}], "ideal": "Title: Temporarily skip jobs in Python.yml\n\nBody: Unsure if worth removing, fixing and reverting, or just let it fail."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/virtual_file_system.cpp\n@@ -5,7 +5,10 @@\n \n namespace duckdb {\n \n-VirtualFileSystem::VirtualFileSystem() : default_fs(FileSystem::CreateLocal()) {\n+VirtualFileSystem::VirtualFileSystem() : VirtualFileSystem(FileSystem::CreateLocal()) {\n+}\n+\n+VirtualFileSystem::VirtualFileSystem(unique_ptr<FileSystem> &&inner) : default_fs(std::move(inner)) {\n \tVirtualFileSystem::RegisterSubSystem(FileCompressionType::GZIP, make_uniq<GZipFileSystem>());\n }\n \n---\nsrc/include/duckdb/common/virtual_file_system.hpp\n@@ -18,6 +18,7 @@ namespace duckdb {\n class VirtualFileSystem : public FileSystem {\n public:\n \tVirtualFileSystem();\n+\texplicit VirtualFileSystem(unique_ptr<FileSystem> &&inner_file_system);\n \n \tvoid Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n \tvoid Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n---\nsrc/main/database.cpp\n@@ -431,7 +431,7 @@ void DatabaseInstance::Configure(DBConfig &new_config, const char *database_path\n \tif (new_config.file_system) {\n \t\tconfig.file_system = std::move(new_config.file_system);\n \t} else {\n-\t\tconfig.file_system = make_uniq<VirtualFileSystem>();\n+\t\tconfig.file_system = make_uniq<VirtualFileSystem>(FileSystem::CreateLocal());\n \t}\n \tif (database_path && !config.options.enable_external_access) {\n \t\tconfig.AddAllowedPath(database_path);\n---\n"}], "ideal": "Title: VirtualFileSystem to take an input, allowing to customize behaviour\n\nBody: Bumped against this in the duckdb-wasm side, I think it's useful to allow customize inner file system (and skip default creating a LocalFileSystem).\r\n\r\nUnsure on keeping both constructors (default one that relies on Local one), or forcing a to provide an input."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/window/window_value_function.cpp\n@@ -309,15 +309,20 @@ void WindowLeadLagExecutor::EvaluateInternal(WindowExecutorGlobalState &gstate,\n \t\tframes.resize(1);\n \t\tauto &frame = frames[0];\n \t\tfor (idx_t i = 0; i < count; ++i, ++row_idx) {\n+\t\t\tint64_t offset = 1;\n+\t\t\tif (wexpr.offset_expr) {\n+\t\t\t\tif (leadlag_offset.CellIsNull(i)) {\n+\t\t\t\t\tFlatVector::SetNull(result, i, true);\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\toffset = leadlag_offset.GetCell<int64_t>(i);\n+\t\t\t}\n+\n \t\t\t// (1) compute the ROW_NUMBER of the own row\n \t\t\tframe = FrameBounds(frame_begin[i], frame_end[i]);\n \t\t\tconst auto own_row = glstate.row_tree->Rank(frame.start, frame.end, row_idx) - 1;\n \t\t\t// (2) adjust the row number by adding or subtracting an offset\n \t\t\tauto val_idx = NumericCast<int64_t>(own_row);\n-\t\t\tint64_t offset = 1;\n-\t\t\tif (wexpr.offset_expr) {\n-\t\t\t\toffset = leadlag_offset.GetCell<int64_t>(i);\n-\t\t\t}\n \t\t\tif (wexpr.GetExpressionType() == ExpressionType::WINDOW_LEAD) {\n \t\t\t\tval_idx = AddOperatorOverflowCheck::Operation<int64_t, int64_t, int64_t>(val_idx, offset);\n \t\t\t} else {\n@@ -368,6 +373,12 @@ void WindowLeadLagExecutor::EvaluateInternal(WindowExecutorGlobalState &gstate,\n \tfor (idx_t i = 0; i < count;) {\n \t\tint64_t offset = 1;\n \t\tif (wexpr.offset_expr) {\n+\t\t\tif (leadlag_offset.CellIsNull(i)) {\n+\t\t\t\tFlatVector::SetNull(result, i, true);\n+\t\t\t\t++i;\n+\t\t\t\t++row_idx;\n+\t\t\t\tcontinue;\n+\t\t\t}\n \t\t\toffset = leadlag_offset.GetCell<int64_t>(i);\n \t\t}\n \t\tint64_t val_idx = (int64_t)row_idx;\n---\ntest/sql/window/test_lead_lag.test\n@@ -105,3 +105,33 @@ ORDER BY 2, 1\n 2024-01-01\t7\t1000\tlate\t2024-03-01\tdispatched\n 2024-02-01\t7\t1000\tdispatched\tNULL\tNULL\n 2024-03-01\t7\t1000\tdispatched\tNULL\tNULL\n+\n+# test lag and lead when offset is null\n+statement ok\n+CREATE TABLE issue17266(c1 INT, c2 SMALLINT, c3 BITSTRING);\n+\n+statement ok\n+INSERT INTO issue17266 VALUES\n+(0, null, null),\n+(1, 32767, '101'),\n+(2, -32767, '101'),\n+(3, 0, '000'),\n+(4, null, null);\n+\n+query IIII\n+SELECT c1, c3, c2, LAG(c3, c2, BITSTRING'010101010') OVER (PARTITION BY c1 ORDER BY c3) FROM issue17266 ORDER BY c1;\n+----\n+0\tNULL\tNULL\tNULL\n+1\t101\t32767\t010101010\n+2\t101\t-32767\t010101010\n+3\t000\t0\t000\n+4\tNULL\tNULL\tNULL\n+\n+query IIII\n+SELECT c1, c3, c2, LEAD(c3, c2, BITSTRING'010101010') OVER (PARTITION BY c1 ORDER BY c3) FROM issue17266 ORDER BY c1;\n+----\n+0\tNULL\tNULL\tNULL\n+1\t101\t32767\t010101010\n+2\t101\t-32767\t010101010\n+3\t000\t0\t000\n+4\tNULL\tNULL\tNULL\n---\ntest/sql/window/test_leadlag_orderby.test\n@@ -52,3 +52,51 @@ ORDER BY i\n 7\t3\t8\t6\n 8\t4\t9\t7\n 9\t4\tNULL\t8\n+\n+# test lag and lead when offset is null\n+statement ok\n+CREATE TABLE issue17266(c1 INT, c2 SMALLINT, c3 BITSTRING);\n+\n+statement ok\n+INSERT INTO issue17266 VALUES\n+(0, null, null),\n+(1, 32767, '101'),\n+(2, -32767, '101'),\n+(3, 0, '000'),\n+(4, 1, '010'),\n+(5, 0, '110'),\n+(6, null, null);\n+\n+query IIII\n+SELECT\n+\tc1,\n+        c3,\n+        c2,\n+        LAG(c3, c2 ORDER BY c1, BITSTRING'010101010') OVER (PARTITION BY c1 ORDER BY c3)\n+FROM issue17266\n+ORDER BY c1;\n+----\n+0\tNULL\tNULL\tNULL\n+1\t101\t32767\tNULL\n+2\t101\t-32767\tNULL\n+3\t000\t0\t000\n+4\t010\t1\tNULL\n+5\t110\t0\t110\n+6\tNULL\tNULL\tNULL\n+\n+query IIII\n+SELECT\n+\tc1,\n+        c3,\n+        c2,\n+        LEAD(c3, c2 ORDER BY c1, BITSTRING'010101010') OVER (PARTITION BY c1 ORDER BY c3)\n+FROM issue17266\n+ORDER BY c1;\n+----\n+0\tNULL\tNULL\tNULL\n+1\t101\t32767\tNULL\n+2\t101\t-32767\tNULL\n+3\t000\t0\t000\n+4\t010\t1\tNULL\n+5\t110\t0\t110\n+6\tNULL\tNULL\tNULL\n---\n"}], "ideal": "Title: Issue #17266: Lead Lag Nulls \n\nBody: * fix the result of lag/lead when the offset is null\r\n* optimize lag/lead when offset is null\r\n* add test for lag/lead when offset is null\r\n* add test for lag/lead with orderby when offset is null\r\n* Fix small typos.\r\n\r\nfixes: https://github.com/duckdb/duckdb/issues/17266\r\nfixes: duckdblabs/duckdb-internal#4760"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/Main.yml\n@@ -274,6 +274,15 @@ jobs:\n       GEN: ninja\n \n     steps:\n+      - name: Clean up the disc space\n+        shell: bash\n+        run: |\n+          echo \"Disk usage before clean up:\"\n+          df -h\n+          rm -rf /opt/hostedtoolcache/CodeQL Java* Pypy Ruby go node\n+          echo \"Disk usage after clean up:\"\n+          df -h\n+\n       - uses: actions/checkout@v3\n         with:\n           fetch-depth: 0\n---\n"}], "ideal": "Title: add step to clean up the disc space to fix `No space left on device`\n\nBody: This PR should fix https://github.com/duckdblabs/duckdb-internal/issues/4753\r\n\r\nThere are two cases of this issue - when it runs out the space on Build and on `temp_directory_enable_external_access.test`.\r\n\r\nThis fix should help to clean up at least 5GB additional space by removing tools preinstalled in the GH runner (except for Python).\r\n\r\nThere are more preinstalled stuff to remove, so it this fix don't help, we can remove more things."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types/vector.cpp\n@@ -924,6 +924,27 @@ void Vector::Flatten(idx_t count) {\n \tswitch (GetVectorType()) {\n \tcase VectorType::FLAT_VECTOR:\n \t\t// already a flat vector\n+\t\tswitch (GetType().InternalType()) {\n+\t\tcase PhysicalType::STRUCT: {\n+\t\t\tauto &entries = StructVector::GetEntries(*this);\n+\t\t\tfor (auto &entry : entries) {\n+\t\t\t\tentry->Flatten(count);\n+\t\t\t}\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PhysicalType::LIST: {\n+\t\t\tauto &entry = ListVector::GetEntry(*this);\n+\t\t\tentry.Flatten(ListVector::GetListSize(*this));\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase PhysicalType::ARRAY: {\n+\t\t\tauto &entry = ArrayVector::GetEntry(*this);\n+\t\t\tentry.Flatten(ArrayVector::GetTotalSize(*this));\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\tbreak;\n+\t\t}\n \t\tbreak;\n \tcase VectorType::FSST_VECTOR: {\n \t\t// Even though count may only be a part of the vector, we need to flatten the whole thing due to the way\n---\n"}], "ideal": "Title: [Dev] Flatten any deeper children vectors, when the top level is a FLAT vector\n\nBody: This PR fixes an issue found in <https://github.com/duckdb/duckdb/pull/17169>\r\n\r\nBut since it has (likely) effect on much more tests and codepaths, I'm making this PR separately."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/pythonpkg/pyproject.toml\n@@ -32,7 +32,7 @@ Changelog = \"https://github.com/duckdb/duckdb/releases\"\n \n # Default config runs all tests and requires at least one extension to be tested against\n [tool.cibuildwheel]\n-dependency-versions = \"latest\"\n+dependency-versions = \"pinned\"\n before-build = 'pip install numpy>=2.0'\n before-test = 'python scripts/optional_requirements.py'\n test-requires = [\"pytest\", \"google-cloud-storage\", \"mypy\"]\n---\n"}], "ideal": "Title: Fix Python CI: pin virtualenv to previous version\n\nBody: Fixes an issue caused by https://github.com/pypa/virtualenv/issues/2883"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/binder/expression/bind_star_expression.cpp\n@@ -300,7 +300,21 @@ void Binder::ExpandStarExpression(unique_ptr<ParsedExpression> expr,\n \t\t\t\tnew_list.push_back(std::move(expanded_expr));\n \t\t\t}\n \t\t\tif (new_list.empty()) {\n-\t\t\t\tauto err = StringUtil::Format(\"No matching columns found that match regex \\\"%s\\\"\", regex_str);\n+\t\t\t\tvector<string> candidates;\n+\t\t\t\tfor (auto &expanded_expr : star_list) {\n+\t\t\t\t\tauto child_expr = GetResolvedColumnExpression(*expanded_expr);\n+\t\t\t\t\tif (!child_expr) {\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t}\n+\t\t\t\t\tauto &colref = child_expr->Cast<ColumnRefExpression>();\n+\t\t\t\t\tcandidates.push_back(colref.GetColumnName());\n+\t\t\t\t}\n+\t\t\t\tstring candidate_str;\n+\t\t\t\tif (!candidates.empty()) {\n+\t\t\t\t\tcandidate_str = \"\\n\" + StringUtil::CandidatesErrorMessage(candidates, regex_str, \"Did you mean\");\n+\t\t\t\t}\n+\t\t\t\tauto err =\n+\t\t\t\t    StringUtil::Format(\"No matching columns found that match regex \\\"%s\\\"%s\", regex_str, candidate_str);\n \t\t\t\tthrow BinderException(*star, err);\n \t\t\t}\n \t\t\tstar_list = std::move(new_list);\n---\ntest/sql/parser/test_columns.test\n@@ -46,6 +46,11 @@ SELECT COLUMNS(*) + COLUMNS(*) FROM integers\n statement ok\n CREATE TABLE grouped_table AS SELECT  1 id, 42 index1, 84 index2 UNION ALL SELECT 2, 13, 14\n \n+statement error\n+SELECT COLUMNS('indxe.*') FROM grouped_table\n+----\n+\"index1\"\n+\n query III\n SELECT id, MIN(COLUMNS('index[0-9]')) FROM grouped_table GROUP BY all ORDER BY ALL\n ----\n---\n"}], "ideal": "Title: Add candidates suggestion when COLUMNS regex does not match any columns\n\nBody: When `COLUMNS('<regex>')` does not match any columns, we find the columns that most closely match the regular expression and provide them as suggestions."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/parser/transform/statement/transform_update.cpp\n@@ -11,6 +11,9 @@ unique_ptr<UpdateSetInfo> Transformer::TransformUpdateSetInfo(duckdb_libpgquery:\n \n \tfor (auto cell = root->head; cell != nullptr; cell = cell->next) {\n \t\tauto target = PGPointerCast<duckdb_libpgquery::PGResTarget>(cell->data.ptr_value);\n+\t\tif (target->indirection) {\n+\t\t\tthrow ParserException(\"Qualified column names in UPDATE .. SET not supported\");\n+\t\t}\n \t\tresult->columns.emplace_back(target->name);\n \t\tresult->expressions.push_back(TransformExpression(target->val));\n \t}\n---\nsrc/planner/binder/statement/bind_update.cpp\n@@ -33,7 +33,12 @@ unique_ptr<LogicalOperator> Binder::BindUpdateSet(LogicalOperator &op, unique_pt\n \t\tauto &colname = set_info.columns[i];\n \t\tauto &expr = set_info.expressions[i];\n \t\tif (!table.ColumnExists(colname)) {\n-\t\t\tthrow BinderException(\"Referenced update column %s not found in table!\", colname);\n+\t\t\tvector<string> column_names;\n+\t\t\tfor (auto &col : table.GetColumns().Physical()) {\n+\t\t\t\tcolumn_names.push_back(col.Name());\n+\t\t\t}\n+\t\t\tauto candidates = StringUtil::CandidatesErrorMessage(column_names, colname, \"Did you mean\");\n+\t\t\tthrow BinderException(\"Referenced update column %s not found in table!\\n%s\", colname, candidates);\n \t\t}\n \t\tauto &column = table.GetColumn(colname);\n \t\tif (column.Generated()) {\n---\ntest/sql/update/update_error_suggestions.test\n@@ -0,0 +1,19 @@\n+# name: test/sql/update/update_error_suggestions.test\n+# description: Test suggestions on UPDATE statement\n+# group: [update]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE tbl(mycol INTEGER)\n+\n+statement error\n+UPDATE tbl SET myco=42\n+----\n+\"mycol\"\n+\n+statement error\n+UPDATE tbl SET tbl.mycol=42\n+----\n+not supported\n---\n"}], "ideal": "Title: Improve error messages in UPDATE ... SET\n\nBody: This PR improves `UPDATE .. SET` in two different manners:\r\n\r\n* SET statements on missing columns now provide suggestions for column names.\r\n* Qualified SET statements are no longer silently ignored. Previously `UPDATE tbl SET tbl.x = 5` would try to set the column `\"tbl\"` while silently ignoring the `\"x\"` part. This now throws an error."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/binder/expression/bind_function_expression.cpp\n@@ -56,18 +56,25 @@ BindResult ExpressionBinder::BindExpression(FunctionExpression &function, idx_t\n \t\t// not a table function - check if the schema is set\n \t\tif (!function.schema.empty()) {\n \t\t\t// the schema is set - check if we can turn this the schema into a column ref\n-\t\t\tErrorData error;\n-\t\t\tunique_ptr<ColumnRefExpression> colref;\n-\t\t\tif (function.catalog.empty()) {\n-\t\t\t\tcolref = make_uniq<ColumnRefExpression>(function.schema);\n-\t\t\t} else {\n-\t\t\t\tcolref = make_uniq<ColumnRefExpression>(function.schema, function.catalog);\n-\t\t\t}\n-\t\t\tauto new_colref = QualifyColumnName(*colref, error);\n-\t\t\tbool is_col = !error.HasError();\n-\t\t\tbool is_col_alias = QualifyColumnAlias(*colref);\n-\n-\t\t\tif (is_col || is_col_alias) {\n+\t\t\t// does this function exist in the system catalog?\n+\t\t\tfunc = GetCatalogEntry(INVALID_CATALOG, INVALID_SCHEMA, function_lookup, OnEntryNotFound::RETURN_NULL);\n+\t\t\tif (func) {\n+\t\t\t\t// the function exists in the system catalog - turn this into a dot call\n+\t\t\t\tErrorData error;\n+\t\t\t\tunique_ptr<ColumnRefExpression> colref;\n+\t\t\t\tif (function.catalog.empty()) {\n+\t\t\t\t\tcolref = make_uniq<ColumnRefExpression>(function.schema);\n+\t\t\t\t} else {\n+\t\t\t\t\tcolref = make_uniq<ColumnRefExpression>(function.schema, function.catalog);\n+\t\t\t\t}\n+\t\t\t\tauto new_colref = QualifyColumnName(*colref, error);\n+\t\t\t\tif (error.HasError()) {\n+\t\t\t\t\t// could not find the column - try to qualify the alias\n+\t\t\t\t\tif (!QualifyColumnAlias(*colref)) {\n+\t\t\t\t\t\t// no alias found either - throw\n+\t\t\t\t\t\terror.Throw();\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t\t// we can! transform this into a function call on the column\n \t\t\t\t// i.e. \"x.lower()\" becomes \"lower(x)\"\n \t\t\t\tfunction.children.insert(function.children.begin(), std::move(colref));\n@@ -76,7 +83,10 @@ BindResult ExpressionBinder::BindExpression(FunctionExpression &function, idx_t\n \t\t\t}\n \t\t}\n \t\t// rebind the function\n-\t\tfunc = GetCatalogEntry(function.catalog, function.schema, function_lookup, OnEntryNotFound::THROW_EXCEPTION);\n+\t\tif (!func) {\n+\t\t\tfunc =\n+\t\t\t    GetCatalogEntry(function.catalog, function.schema, function_lookup, OnEntryNotFound::THROW_EXCEPTION);\n+\t\t}\n \t}\n \n \tif (func->type != CatalogType::AGGREGATE_FUNCTION_ENTRY &&\n---\ntest/sql/cast/dot_function_missing_error.test\n@@ -0,0 +1,23 @@\n+# name: test/sql/cast/dot_function_missing_error.test\n+# description: Test reporting of dot with missing identifier\n+# group: [cast]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+# file does not exist\n+statement error\n+select file.replace('a', 'b') from (values ('xxxx')) t(\"filename\");\n+----\n+\"file\" not found in FROM clause\n+\n+# function does not exist\n+statement error\n+select filename.replacezxcv('a', 'b') from (values ('xxxx')) t(\"filename\");\n+----\n+replacezxcv does not exist\n+\n+statement error\n+select replacezxcv('a', 'b') from (values ('xxxx')) t(\"filename\");\n+----\n+replacezxcv does not exist\n---\ntest/sql/parser/function_chaining.test\n@@ -14,7 +14,7 @@ INSERT INTO varchars VALUES ('Hello'), ('World')\n statement error\n SELECT x.lower() FROM varchars\n ----\n-Function with name lower does not exist\n+not found in FROM clause\n \n # conflict\n statement error\n---\n"}], "ideal": "Title: Function chaining: report missing column instead of missing function if function exists\n\nBody: When doing column function chaining error messages are confusing when the wrong column name is used:\r\n\r\n```sql\r\nD select l_coment.replace('a', 'b') from lineitem;\r\n-- Catalog Error:\r\n-- Scalar Function with name \"replace\" is not in the catalog, but it exists in the core_functions extension.\r\n```\r\n\r\nClearly the `replace` function exists - the error message the user is looking for is reporting that there is a typo in `l_comment`. \r\n\r\nThis PR resolves this by checking if the function exists - and if it exists, we throw the missing column error message instead. "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/scalar/map/map_extract.cpp\n@@ -68,7 +68,6 @@ static void MapExtractValueFunc(DataChunk &args, ExpressionState &state, Vector\n \tconst auto pos_data = UnifiedVectorFormat::GetData<int32_t>(pos_format);\n \tconst auto inc_list_data = ListVector::GetData(map_vec);\n \n-\tauto &result_validity = FlatVector::Validity(result);\n \tfor (idx_t row_idx = 0; row_idx < count; row_idx++) {\n \t\tauto lst_idx = lst_format.sel->get_index(row_idx);\n \t\tif (!lst_format.validity.RowIsValid(lst_idx)) {\n@@ -79,7 +78,7 @@ static void MapExtractValueFunc(DataChunk &args, ExpressionState &state, Vector\n \t\tconst auto pos_idx = pos_format.sel->get_index(row_idx);\n \t\tif (!pos_format.validity.RowIsValid(pos_idx)) {\n \t\t\t// We didnt find the key in the map, so return NULL\n-\t\t\tresult_validity.SetInvalid(row_idx);\n+\t\t\tFlatVector::SetNull(result, row_idx, true);\n \t\t\tcontinue;\n \t\t}\n \n---\ntest/sql/types/map/map_extract_nested_null.test\n@@ -0,0 +1,13 @@\n+# name: test/sql/types/map/map_extract_nested_null.test\n+# group: [map]\n+\n+statement ok\n+CREATE TABLE test(id int, attr MAP(VARCHAR, UNION(i INT, s VARCHAR)));\n+\n+statement ok\n+INSERT INTO test VALUES (1, MAP{'key1': 'str'});\n+\n+query II\n+SELECT id, attr['key2'] FROM test;\n+----\n+1\tNULL\n\\ No newline at end of file\n---\n"}], "ideal": "Title: fix not setting nested validity when map_extract returns null\n\nBody: Closes https://github.com/users/Maxxen/projects/2/views/1?pane=issue&itemId=109461215&issue=duckdblabs%7Cduckdb-internal%7C4812"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "tools/shell/shell.cpp\n@@ -3629,7 +3629,7 @@ bool ShellState::OpenDatabase(const char **azArg, idx_t nArg) {\n \topenFlags = openFlags & ~(SQLITE_OPEN_NOFOLLOW); // don't overwrite settings loaded in the command line\n \tszMax = 0;\n \t/* Check for command-line arguments */\n-\tfor (idx_t iName = 1; iName < nArg && azArg[iName][0] == '-'; iName++) {\n+\tfor (iName = 1; iName < nArg && azArg[iName][0] == '-'; iName++) {\n \t\tconst char *z = azArg[iName];\n \t\tif (optionMatch(z, \"new\")) {\n \t\t\tnewFlag = true;\n---\n"}], "ideal": "Title: fix #17258: Allow to open database in readonly mode within cli\n\nBody: Since the iName variable declared before the for loop was shadowed by a new one inside the loop, its value remained unchanged after the loop. As a result, when it was later used to retrieve the database name, it still held the initial value 1, which pointed to the --readonly parameter."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/table/range.cpp\n@@ -29,7 +29,7 @@ static void GetParameters(int64_t values[], idx_t value_count, hugeint_t &start,\n }\n \n struct RangeFunctionBindData : public TableFunctionData {\n-\texplicit RangeFunctionBindData(const vector<Value> &inputs) : cardinality(0) {\n+\texplicit RangeFunctionBindData(const vector<Value> &inputs, bool generate_series) : cardinality(0) {\n \t\tint64_t values[3];\n \t\tfor (idx_t i = 0; i < inputs.size(); i++) {\n \t\t\tif (inputs[i].IsNull()) {\n@@ -41,7 +41,15 @@ struct RangeFunctionBindData : public TableFunctionData {\n \t\thugeint_t end;\n \t\thugeint_t increment;\n \t\tGetParameters(values, inputs.size(), start, end, increment);\n+\t\tif (generate_series) {\n+\t\t\t// generate_series has inclusive bounds on the RHS\n+\t\t\tend += 1;\n+\t\t}\n+\n \t\tcardinality = Hugeint::Cast<idx_t>((end - start) / increment);\n+\t\tif ((end - start) % increment != 0) {\n+\t\t\tcardinality += 1;\n+\t\t}\n \t}\n \n \tidx_t cardinality;\n@@ -59,7 +67,7 @@ static unique_ptr<FunctionData> RangeFunctionBind(ClientContext &context, TableF\n \tif (input.inputs.empty() || input.inputs.size() > 3) {\n \t\treturn nullptr;\n \t}\n-\treturn make_uniq<RangeFunctionBindData>(input.inputs);\n+\treturn make_uniq<RangeFunctionBindData>(input.inputs, GENERATE_SERIES);\n }\n \n struct RangeFunctionLocalState : public LocalTableFunctionState {\n---\ntest/sql/function/list/generate_series.test\n@@ -218,3 +218,50 @@ SELECT * FROM (SELECT 1 UNION ALL SELECT 0 UNION ALL SELECT 2) AS _(x), generate\n 1\t1\n 2\t1\n 2\t2\n+\n+# Check that explain cardinalities are correct\n+query II\n+EXPLAIN FROM range(0);\n+----\n+physical_plan\t<REGEX>:.*~0 Rows.*\n+\n+query II\n+EXPLAIN FROM range(-1);\n+----\n+physical_plan\t<REGEX>:.*~0 Rows.*\n+\n+query II\n+EXPLAIN FROM range(-5, -20, -1);\n+----\n+physical_plan\t<REGEX>:.*~15 Rows.*\n+\n+query II\n+EXPLAIN FROM range(1, 4, 2);\n+----\n+physical_plan\t<REGEX>:.*~2 Rows.*\n+\n+query II\n+EXPLAIN FROM range(1, 5, 2);\n+----\n+physical_plan\t<REGEX>:.*~2 Rows.*\n+\n+query II\n+EXPLAIN FROM generate_series(0);\n+----\n+physical_plan\t<REGEX>:.*~1 Rows.*\n+\n+query II\n+EXPLAIN FROM generate_series(1);\n+----\n+physical_plan\t<REGEX>:.*~2 Rows.*\n+\n+query II\n+EXPLAIN FROM generate_series(1, 4, 2);\n+----\n+physical_plan\t<REGEX>:.*~2 Rows.*\n+\n+query II\n+EXPLAIN FROM generate_series(1, 5, 2);\n+----\n+physical_plan\t<REGEX>:.*~3 Rows.*\n+\n---\n"}], "ideal": "Title: Fix two off-by-one errors in row estimate of range and generate_series\n\nBody: While playing around with `range` and `generate_series` I noticed the\nexpected number of rows was often off-by-one from the real cardinality.\nIt turned out that there were two issues which were causing this, this\nPR fixes both.\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/parquet_multi_file_info.cpp\n@@ -267,6 +267,19 @@ vector<PartitionStatistics> ParquetGetPartitionStats(ClientContext &context, Get\n \t\t\t// no cache entry found\n \t\t\treturn result;\n \t\t}\n+\t\t// check if the file has any deletes\n+\t\tif (file.extended_info) {\n+\t\t\tauto entry = file.extended_info->options.find(\"has_deletes\");\n+\t\t\tif (entry != file.extended_info->options.end()) {\n+\t\t\t\tif (BooleanValue::Get(entry->second)) {\n+\t\t\t\t\t// the file has deletes - skip emitting partition stats\n+\t\t\t\t\t// FIXME: we could emit partition stats but set count to `COUNT_APPROXIMATE` instead of\n+\t\t\t\t\t// `COUNT_EXACT`\n+\t\t\t\t\treturn result;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n \t\t// check if the cache is valid based ONLY on the OpenFileInfo (do not do any file system requests here)\n \t\tauto is_valid = metadata_entry->IsValid(file);\n \t\tif (is_valid != ParquetCacheValidity::VALID) {\n---\n"}], "ideal": "Title: Skip emitting partition stats if \"has_deletes\" is set in the file info\n\nBody: Follow-up from https://github.com/duckdb/duckdb/pull/17344\r\n\r\nWhen emitting partition stats based on cached Parquet files, we check the `OpenFileInfo` for a boolean `has_deletes`. If that is set to true - we skip emitting partition stats. This should allow the partition stats from cached Parquet files to be used also for Lakehouse formats when there are no deletes for a given file. \r\n\r\nCC @Tishj @samansmink @Tmonster "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "third_party/hyperloglog/sds.cpp\n@@ -226,6 +226,7 @@ sds sdsMakeRoomFor(sds s, size_t addlen) {\n \n     hdrlen = sdsHdrSize(type);\n     assert(hdrlen + newlen + 1 > reqlen);  /* Catch size_t overflow */\n+    (void)reqlen;\n     if (oldtype==type) {\n         newsh = realloc(sh, hdrlen+newlen+1);\n         if (newsh == NULL) return NULL;\n---\n"}], "ideal": "Title: Fixup amalgamation: reqlen is only used with assert enabled\n\nBody: Trivial fix up after https://github.com/duckdb/duckdb/pull/17339, restoring amalgamation CI run."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "scripts/format.py\n@@ -12,6 +12,7 @@\n import tempfile\n import uuid\n import concurrent.futures\n+import argparse\n from python_helpers import open_utf8\n \n try:\n@@ -120,51 +121,30 @@\n force = False\n \n \n-def print_usage():\n-    print(\"Usage: python scripts/format.py [revision|--all] [--check|--fix] [--force]\")\n-    print(\n-        \"   [revision]     is an optional revision number, all files that changed since that revision will be formatted (default=HEAD)\"\n-    )\n-    print(\"                  if [revision] is set to --all, all files will be formatted\")\n-    print(\"   --check only prints differences, --fix also fixes the files (--check is default)\")\n+parser = argparse.ArgumentParser(prog='python scripts/format.py', description='Format source directory files')\n+parser.add_argument(\n+    'revision', nargs='?', default='HEAD', help='Revision number or --all to format all files (default: HEAD)'\n+)\n+parser.add_argument('--check', action='store_true', help='Only print differences (default)')\n+parser.add_argument('--fix', action='store_true', help='Fix the files')\n+parser.add_argument('-a', '--all', action='store_true', help='Format all files')\n+parser.add_argument('-d', '--directories', nargs='*', default=[], help='Format specified directories')\n+parser.add_argument('-y', '--noconfirm', action='store_true', help='Skip confirmation prompt')\n+parser.add_argument('-q', '--silent', action='store_true', help='Suppress output')\n+parser.add_argument('-f', '--force', action='store_true', help='Force formatting')\n+args = parser.parse_args()\n+\n+revision = args.revision\n+if args.check and args.fix:\n+    parser.print_usage()\n     exit(1)\n-\n-\n-if len(sys.argv) == 1:\n-    revision = \"HEAD\"\n-elif len(sys.argv) >= 2:\n-    revision = sys.argv[1]\n-else:\n-    print_usage()\n-\n-directories = False\n-if len(sys.argv) > 2:\n-    for arg in sys.argv[2:]:\n-        prev_directories = directories\n-        directories = False\n-        if arg == '--check':\n-            check_only = True\n-        elif arg == '--fix':\n-            check_only = False\n-        elif arg == '--noconfirm':\n-            confirm = False\n-        elif arg == '--confirm':\n-            confirm = True\n-        elif arg == '--silent':\n-            silent = True\n-        elif arg == '--force':\n-            force = True\n-        elif arg == '--directories':\n-            directories = True\n-            formatted_directories = []\n-        elif prev_directories:\n-            directories = True\n-            formatted_directories += [arg]\n-        else:\n-            print_usage()\n-\n-if revision == '--all':\n-    format_all = True\n+check_only = not args.fix\n+confirm = not args.noconfirm\n+silent = args.silent\n+force = args.force\n+format_all = args.all\n+if args.directories:\n+    formatted_directories = args.directories\n \n \n def file_is_ignored(full_path):\n---\n"}], "ideal": "Title: Use argparse in scripts/format.py\n\nBody: `argparse` is a widely used library and provides robust argument handling functionality."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/parallel/thread_context.cpp\n@@ -11,12 +11,22 @@ ThreadContext::ThreadContext(ClientContext &context) : profiler(context) {\n \tlog_context.connection_id = context.GetConnectionId();\n \tif (context.transaction.HasActiveTransaction()) {\n \t\tlog_context.transaction_id = context.transaction.ActiveTransaction().global_transaction_id;\n-\t\tlog_context.query_id = context.transaction.GetActiveQuery();\n+\t\tauto query_id = context.transaction.GetActiveQuery();\n+\t\tif (query_id == DConstants::INVALID_INDEX) {\n+\t\t\tlog_context.query_id = optional_idx();\n+\t\t} else {\n+\t\t\tlog_context.query_id = query_id;\n+\t\t}\n \t}\n \n \tlog_context.thread_id = TaskScheduler::GetEstimatedCPUId();\n \tif (context.transaction.HasActiveTransaction()) {\n-\t\tlog_context.transaction_id = context.transaction.GetActiveQuery();\n+\t\tauto query_id = context.transaction.GetActiveQuery();\n+\t\tif (query_id == DConstants::INVALID_INDEX) {\n+\t\t\tlog_context.transaction_id = optional_idx();\n+\t\t} else {\n+\t\t\tlog_context.transaction_id = query_id;\n+\t\t}\n \t}\n \tlogger = context.db->GetLogManager().CreateLogger(log_context, true);\n }\n---\n"}], "ideal": "Title: Fix internal exception from assigning invalid index to `optional_idx query_id;`\n\nBody: This PR fixes https://github.com/duckdblabs/duckdb-internal/issues/4806\r\n\r\nThis should fix the issue encountered [here](https://github.com/duckdb/duckdb-iceberg/issues/213)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/parser/parsed_data/attach_info.cpp\n@@ -9,6 +9,7 @@ namespace duckdb {\n \n StorageOptions AttachInfo::GetStorageOptions() const {\n \tStorageOptions storage_options;\n+\tstring storage_version_user_provided = \"\";\n \tfor (auto &entry : options) {\n \t\tif (entry.first == \"block_size\") {\n \t\t\t// Extract the block allocation size. This is NOT the actual memory available on a block (block_size),\n@@ -17,15 +18,25 @@ StorageOptions AttachInfo::GetStorageOptions() const {\n \t\t} else if (entry.first == \"encryption_key\") {\n \t\t\tstorage_options.block_header_size = DEFAULT_ENCRYPTION_BLOCK_HEADER_SIZE;\n \t\t\tstorage_options.encryption = true;\n-\t\t\t// set storage version to v1.3.0\n-\t\t\tstorage_options.storage_version = SerializationCompatibility::FromString(\"v1.3.0\").serialization_version;\n \t\t} else if (entry.first == \"row_group_size\") {\n \t\t\tstorage_options.row_group_size = entry.second.GetValue<uint64_t>();\n \t\t} else if (entry.first == \"storage_version\") {\n+\t\t\tstorage_version_user_provided = entry.second.ToString();\n \t\t\tstorage_options.storage_version =\n \t\t\t    SerializationCompatibility::FromString(entry.second.ToString()).serialization_version;\n \t\t}\n \t}\n+\tif (storage_options.encryption && (!storage_options.storage_version.IsValid() ||\n+\t                                   storage_options.storage_version.GetIndex() <\n+\t                                       SerializationCompatibility::FromString(\"v1.3.0\").serialization_version)) {\n+\t\tif (!storage_version_user_provided.empty()) {\n+\t\t\tthrow InvalidInputException(\n+\t\t\t    \"Explicit provided STORAGE_VERSION (\\\"%s\\\") and ENCRYPTION_KEY (storage >= v1.3.0) are not compatible\",\n+\t\t\t    storage_version_user_provided);\n+\t\t}\n+\t\t// set storage version to v1.3.0\n+\t\tstorage_options.storage_version = SerializationCompatibility::FromString(\"v1.3.0\").serialization_version;\n+\t}\n \treturn storage_options;\n }\n \n---\ntest/sql/attach/attach_encryption_block_header.test\n@@ -68,3 +68,10 @@ DETACH encrypted\n statement ok\n DETACH unencrypted\n \n+statement error\n+ATTACH '__TEST_DIR__/incompatible_parameters.duckdb' (ENCRYPTION_KEY 'asdf', STORAGE_VERSION 'v1.2.0');\n+----\n+Explicit provided STORAGE_VERSION (\"v1.2.0\") and ENCRYPTION_KEY (storage >= v1.3.0) are not compatible\n+\n+statement ok\n+ATTACH '__TEST_DIR__/compatible_parameters.duckdb' (ENCRYPTION_KEY 'asdf', STORAGE_VERSION 'v1.3.0');\n---\n"}], "ideal": "Title: Handle both ENCRYPTION_KEY and STORAGE_VERSION passed as options\n\nBody: Pre this PR `GetStorageOptions` would possibly set the same settings via 2 different paths via `ENCRYPTION_KEY` or `STORAGE_VERSION`, potentially ending up discarding user input and leading to confusing situations.\r\n\r\nThis should fix that, and add a relevant test."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "benchmark/CMakeLists.txt\n@@ -11,7 +11,18 @@ if(${_index} GREATER -1)\n endif()\n \n add_extension_definitions()\n-add_definitions(-DDUCKDB_ROOT_DIRECTORY=\"${PROJECT_SOURCE_DIR}\")\n+set(BENCHMARK_ROOT_DIRECTORY\n+    \"\"\n+    CACHE\n+      STRING\n+      \"The benchmark runner root directory (defaults to the project source directory)\"\n+)\n+if(DEFINED BENCHMARK_ROOT_DIRECTORY AND NOT BENCHMARK_ROOT_DIRECTORY STREQUAL\n+                                        \"\")\n+  add_definitions(-DDUCKDB_ROOT_DIRECTORY=\"${BENCHMARK_ROOT_DIRECTORY}\")\n+else()\n+  add_definitions(-DDUCKDB_ROOT_DIRECTORY=\"${PROJECT_SOURCE_DIR}\")\n+endif()\n \n add_executable(benchmark_runner benchmark_runner.cpp interpreted_benchmark.cpp\n                                 ${BENCHMARK_OBJECT_FILES})\n---\nbenchmark/interpreted_benchmark.cpp\n@@ -160,7 +160,8 @@ void InterpretedBenchmark::LoadBenchmark() {\n \t\t}\n \t\t// look for a command in this line\n \t\tauto splits = StringUtil::Split(StringUtil::Lower(line), ' ');\n-\t\tif (splits[0] == \"load\" || splits[0] == \"run\" || splits[0] == \"init\" || splits[0] == \"cleanup\") {\n+\t\tif (splits[0] == \"load\" || splits[0] == \"run\" || splits[0] == \"init\" || splits[0] == \"cleanup\" ||\n+\t\t    splits[0] == \"reload\") {\n \t\t\tif (queries.find(splits[0]) != queries.end()) {\n \t\t\t\tthrow std::runtime_error(\"Multiple calls to \" + splits[0] + \" in the same benchmark file\");\n \t\t\t}\n@@ -244,6 +245,12 @@ void InterpretedBenchmark::LoadBenchmark() {\n \t\t\t\tcache_file = cache_db;\n \t\t\t\tcache_db = string();\n \t\t\t}\n+\t\t} else if (splits[0] == \"cache_file\") {\n+\t\t\tif (splits.size() == 2) {\n+\t\t\t\tcache_file = splits[1];\n+\t\t\t} else {\n+\t\t\t\tthrow std::runtime_error(reader.FormatException(\"cache_file requires a single file\"));\n+\t\t\t}\n \t\t} else if (splits[0] == \"storage\") {\n \t\t\tif (splits.size() < 2) {\n \t\t\t\tthrow std::runtime_error(reader.FormatException(\"storage requires at least one parameter\"));\n@@ -431,12 +438,19 @@ unique_ptr<BenchmarkState> InterpretedBenchmark::Initialize(BenchmarkConfigurati\n \tif (queries.find(\"load\") != queries.end()) {\n \t\tload_query = queries[\"load\"];\n \t}\n+\tstring reload_query;\n+\tif (queries.find(\"reload\") != queries.end()) {\n+\t\treload_query = queries[\"reload\"];\n+\t}\n \n \tif (!cache_file.empty()) {\n \t\tauto fs = FileSystem::CreateLocal();\n \t\tif (!fs->FileExists(fs->JoinPath(BenchmarkRunner::DUCKDB_BENCHMARK_DIRECTORY, cache_file))) {\n \t\t\t// no cache or db_path specified: just run the initialization code\n \t\t\tresult = RunLoadQuery(*state, load_query);\n+\t\t} else if (!reload_query.empty()) {\n+\t\t\t// run reload query\n+\t\t\tresult = RunLoadQuery(*state, reload_query);\n \t\t}\n \t} else if (cache_db.empty() && cache_db.compare(DEFAULT_DB_PATH) != 0) {\n \t\t// no cache or db_path specified: just run the initialization code\n@@ -459,6 +473,9 @@ unique_ptr<BenchmarkState> InterpretedBenchmark::Initialize(BenchmarkConfigurati\n \t\tif (!in_memory_db_has_data) {\n \t\t\t// failed to load: write the cache\n \t\t\tresult = RunLoadQuery(*state, load_query);\n+\t\t} else if (!reload_query.empty()) {\n+\t\t\t// succeeded: run the reload query\n+\t\t\tresult = RunLoadQuery(*state, reload_query);\n \t\t}\n \t}\n \twhile (result) {\n---\n"}], "ideal": "Title: Add support for BENCHMARK_ROOT_DIRECTORY cmake option to change benchmark runner root directory, and add support for cache_file and reload options to enable better caching for non-DuckDB databases\n\nBody: The `BENCHMARK_ROOT_DIRECTORY` can be set to change the root directory of the benchmark runner (similar to https://github.com/duckdb/duckdb/pull/16540 for the unit tester).\r\n\r\nIn addition, this PR adds two new options:\r\n\r\n* `cache_file` - specifies a non-database file to use as a caching point - `load` is run only when the cache file does not exist\r\n* `reload` - specifies a query to run to initialize the system when reloading (i.e. when the `cache_file` exists)\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/radix_partitioning.cpp\n@@ -25,7 +25,7 @@ struct RadixPartitioningConstants {\n };\n \n template <class OP, class RETURN_TYPE, typename... ARGS>\n-RETURN_TYPE RadixBitsSwitch(const idx_t radix_bits, ARGS &&... args) {\n+RETURN_TYPE RadixBitsSwitch(const idx_t radix_bits, ARGS &&...args) {\n \tD_ASSERT(radix_bits <= RadixPartitioning::MAX_RADIX_BITS);\n \tswitch (radix_bits) {\n \tcase 0:\n---\nsrc/include/duckdb/common/allocator.hpp\n@@ -145,7 +145,7 @@ void DeleteArray(T *ptr, idx_t size) {\n }\n \n template <typename T, typename... ARGS>\n-T *AllocateObject(ARGS &&... args) {\n+T *AllocateObject(ARGS &&...args) {\n \tauto data = Allocator::DefaultAllocator().AllocateData(sizeof(T));\n \treturn new (data) T(std::forward<ARGS>(args)...);\n }\n---\nsrc/include/duckdb/execution/physical_plan_generator.hpp\n@@ -35,7 +35,7 @@ class PhysicalPlan {\n \n public:\n \ttemplate <class T, class... ARGS>\n-\tPhysicalOperator &Make(ARGS &&... args) {\n+\tPhysicalOperator &Make(ARGS &&...args) {\n \t\tstatic_assert(std::is_base_of<PhysicalOperator, T>::value, \"T must be a physical operator\");\n \t\tauto mem = arena.AllocateAligned(sizeof(T));\n \t\tauto ptr = new (mem) T(std::forward<ARGS>(args)...);\n@@ -91,7 +91,7 @@ class PhysicalPlanGenerator {\n \tstatic OrderPreservationType OrderPreservationRecursive(PhysicalOperator &op);\n \n \ttemplate <class T, class... ARGS>\n-\tPhysicalOperator &Make(ARGS &&... args) {\n+\tPhysicalOperator &Make(ARGS &&...args) {\n \t\treturn physical_plan->Make<T>(std::forward<ARGS>(args)...);\n \t}\n \n---\nsrc/include/duckdb/main/client_context_state.hpp\n@@ -111,7 +111,7 @@ class ClientContextState {\n class RegisteredStateManager {\n public:\n \ttemplate <class T, typename... ARGS>\n-\tshared_ptr<T> GetOrCreate(const string &key, ARGS &&... args) {\n+\tshared_ptr<T> GetOrCreate(const string &key, ARGS &&...args) {\n \t\tlock_guard<mutex> l(lock);\n \t\tauto lookup = registered_state.find(key);\n \t\tif (lookup != registered_state.end()) {\n---\nsrc/include/duckdb/storage/object_cache.hpp\n@@ -47,7 +47,7 @@ class ObjectCache {\n \t}\n \n \ttemplate <class T, class... ARGS>\n-\tshared_ptr<T> GetOrCreate(const string &key, ARGS &&... args) {\n+\tshared_ptr<T> GetOrCreate(const string &key, ARGS &&...args) {\n \t\tlock_guard<mutex> glock(lock);\n \n \t\tauto entry = cache.find(key);\n---\ntools/pythonpkg/src/include/duckdb_python/pybind11/pybind_wrapper.hpp\n@@ -90,7 +90,7 @@ bool try_cast(const handle &object, T &result) {\n } // namespace py\n \n template <class T, typename... ARGS>\n-void DefineMethod(std::vector<const char *> aliases, T &mod, ARGS &&... args) {\n+void DefineMethod(std::vector<const char *> aliases, T &mod, ARGS &&...args) {\n \tfor (auto &alias : aliases) {\n \t\tmod.def(alias, args...);\n \t}\n---\n"}], "ideal": "Title: Whitespace only changes\n\nBody: These changes revert custom formatting changes in format.py, so we could use `clang-format -i`.\r\n\r\nThe idea is to work with clang folks to work out a new config option which gives us the desired behavior."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/persistent/physical_copy_to_file.cpp\n@@ -97,18 +97,20 @@ class CopyToFunctionGlobalState : public GlobalSinkState {\n \t\tcreated_directories.insert(dir_path);\n \t}\n \n-\tstring GetOrCreateDirectory(const vector<idx_t> &cols, const vector<string> &names, const vector<Value> &values,\n-\t                            string path, FileSystem &fs) {\n+\tstring GetOrCreateDirectory(const vector<idx_t> &cols, bool hive_file_pattern, const vector<string> &names,\n+\t                            const vector<Value> &values, string path, FileSystem &fs) {\n \t\tCreateDir(path, fs);\n-\t\tfor (idx_t i = 0; i < cols.size(); i++) {\n-\t\t\tconst auto &partition_col_name = names[cols[i]];\n-\t\t\tconst auto &partition_value = values[i];\n-\t\t\tstring p_dir;\n-\t\t\tp_dir += HivePartitioning::Escape(partition_col_name);\n-\t\t\tp_dir += \"=\";\n-\t\t\tp_dir += HivePartitioning::Escape(partition_value.ToString());\n-\t\t\tpath = fs.JoinPath(path, p_dir);\n-\t\t\tCreateDir(path, fs);\n+\t\tif (hive_file_pattern) {\n+\t\t\tfor (idx_t i = 0; i < cols.size(); i++) {\n+\t\t\t\tconst auto &partition_col_name = names[cols[i]];\n+\t\t\t\tconst auto &partition_value = values[i];\n+\t\t\t\tstring p_dir;\n+\t\t\t\tp_dir += HivePartitioning::Escape(partition_col_name);\n+\t\t\t\tp_dir += \"=\";\n+\t\t\t\tp_dir += HivePartitioning::Escape(partition_value.ToString());\n+\t\t\t\tpath = fs.JoinPath(path, p_dir);\n+\t\t\t\tCreateDir(path, fs);\n+\t\t\t}\n \t\t}\n \t\treturn path;\n \t}\n@@ -167,14 +169,19 @@ class CopyToFunctionGlobalState : public GlobalSinkState {\n \t\t\t}\n \t\t}\n \t\tidx_t offset = 0;\n-\t\tauto prev_offset = previous_partitions.find(values);\n-\t\tif (prev_offset != previous_partitions.end()) {\n-\t\t\toffset = prev_offset->second;\n+\t\tif (op.hive_file_pattern) {\n+\t\t\tauto prev_offset = previous_partitions.find(values);\n+\t\t\tif (prev_offset != previous_partitions.end()) {\n+\t\t\t\toffset = prev_offset->second;\n+\t\t\t}\n+\t\t} else {\n+\t\t\toffset = global_offset++;\n \t\t}\n \t\tauto &fs = FileSystem::GetFileSystem(context.client);\n \t\t// Create a writer for the current file\n \t\tauto trimmed_path = op.GetTrimmedPath(context.client);\n-\t\tstring hive_path = GetOrCreateDirectory(op.partition_columns, op.names, values, trimmed_path, fs);\n+\t\tstring hive_path =\n+\t\t    GetOrCreateDirectory(op.partition_columns, op.hive_file_pattern, op.names, values, trimmed_path, fs);\n \t\tstring full_path(op.filename_pattern.CreateFilename(fs, hive_path, op.file_extension, offset));\n \t\tif (op.overwrite_mode == CopyOverwriteMode::COPY_APPEND) {\n \t\t\t// when appending, we first check if the file exists\n@@ -226,6 +233,7 @@ class CopyToFunctionGlobalState : public GlobalSinkState {\n \t//! The active writes per partition (for partitioned write)\n \tvector_of_value_map_t<unique_ptr<PartitionWriteInfo>> active_partitioned_writes;\n \tvector_of_value_map_t<idx_t> previous_partitions;\n+\tidx_t global_offset = 0;\n };\n \n string PhysicalCopyToFile::GetTrimmedPath(ClientContext &context) const {\n---\nsrc/execution/physical_plan/plan_copy_to_file.cpp\n@@ -78,6 +78,7 @@ PhysicalOperator &PhysicalPlanGenerator::CreatePlan(LogicalCopyToFile &op) {\n \tcast_copy.expected_types = op.expected_types;\n \tcast_copy.parallel = mode == CopyFunctionExecutionMode::PARALLEL_COPY_TO_FILE;\n \tcast_copy.write_empty_file = op.write_empty_file;\n+\tcast_copy.hive_file_pattern = op.hive_file_pattern;\n \n \tcast_copy.children.push_back(plan);\n \treturn copy;\n---\nsrc/include/duckdb/execution/operator/persistent/physical_copy_to_file.hpp\n@@ -52,6 +52,7 @@ class PhysicalCopyToFile : public PhysicalOperator {\n \tbool partition_output;\n \tbool write_partition_columns;\n \tbool write_empty_file;\n+\tbool hive_file_pattern;\n \tvector<idx_t> partition_columns;\n \tvector<string> names;\n \tvector<LogicalType> expected_types;\n---\nsrc/include/duckdb/planner/operator/logical_copy_to_file.hpp\n@@ -44,6 +44,7 @@ class LogicalCopyToFile : public LogicalOperator {\n \tbool partition_output;\n \tbool write_partition_columns;\n \tbool write_empty_file = true;\n+\tbool hive_file_pattern = true;\n \tPreserveOrderType preserve_order = PreserveOrderType::AUTOMATIC;\n \tvector<idx_t> partition_columns;\n \tvector<string> names;\n---\nsrc/planner/binder/statement/bind_copy.cpp\n@@ -57,6 +57,7 @@ BoundStatement Binder::BindCopyTo(CopyStatement &stmt, CopyToType copy_to_type)\n \tbool seen_filepattern = false;\n \tbool write_partition_columns = false;\n \tbool write_empty_file = true;\n+\tbool hive_file_pattern = true;\n \tPreserveOrderType preserve_order = PreserveOrderType::AUTOMATIC;\n \tCopyFunctionReturnType return_type = CopyFunctionReturnType::CHANGED_ROWS;\n \n@@ -137,6 +138,8 @@ BoundStatement Binder::BindCopyTo(CopyStatement &stmt, CopyToType copy_to_type)\n \t\t\twrite_partition_columns = GetBooleanArg(context, option.second);\n \t\t} else if (loption == \"write_empty_file\") {\n \t\t\twrite_empty_file = GetBooleanArg(context, option.second);\n+\t\t} else if (loption == \"hive_file_pattern\") {\n+\t\t\thive_file_pattern = GetBooleanArg(context, option.second);\n \t\t} else {\n \t\t\tstmt.info->options[option.first] = option.second;\n \t\t}\n@@ -271,6 +274,7 @@ BoundStatement Binder::BindCopyTo(CopyStatement &stmt, CopyToType copy_to_type)\n \tcopy->write_empty_file = write_empty_file;\n \tcopy->return_type = return_type;\n \tcopy->preserve_order = preserve_order;\n+\tcopy->hive_file_pattern = hive_file_pattern;\n \n \tcopy->names = unique_column_names;\n \tcopy->expected_types = select_node.types;\n---\nsrc/planner/operator/logical_copy_to_file.cpp\n@@ -68,6 +68,7 @@ void LogicalCopyToFile::Serialize(Serializer &serializer) const {\n \tserializer.WritePropertyWithDefault(216, \"write_partition_columns\", write_partition_columns, true);\n \tserializer.WritePropertyWithDefault(217, \"write_empty_file\", write_empty_file, true);\n \tserializer.WritePropertyWithDefault(218, \"preserve_order\", preserve_order, PreserveOrderType::AUTOMATIC);\n+\tserializer.WritePropertyWithDefault(219, \"hive_file_pattern\", hive_file_pattern, true);\n }\n \n unique_ptr<LogicalOperator> LogicalCopyToFile::Deserialize(Deserializer &deserializer) {\n@@ -115,6 +116,7 @@ unique_ptr<LogicalOperator> LogicalCopyToFile::Deserialize(Deserializer &deseria\n \tauto write_empty_file = deserializer.ReadPropertyWithExplicitDefault(217, \"write_empty_file\", true);\n \tauto preserve_order =\n \t    deserializer.ReadPropertyWithExplicitDefault(218, \"preserve_order\", PreserveOrderType::AUTOMATIC);\n+\tauto hive_file_pattern = deserializer.ReadPropertyWithExplicitDefault(219, \"hive_file_pattern\", true);\n \n \tif (!has_serialize) {\n \t\t// If not serialized, re-bind with the copy info\n@@ -144,6 +146,7 @@ unique_ptr<LogicalOperator> LogicalCopyToFile::Deserialize(Deserializer &deseria\n \tresult->write_partition_columns = write_partition_columns;\n \tresult->write_empty_file = write_empty_file;\n \tresult->preserve_order = preserve_order;\n+\tresult->hive_file_pattern = hive_file_pattern;\n \n \treturn std::move(result);\n }\n---\ntest/sql/copy/parquet/writer/partition_without_hive.test\n@@ -0,0 +1,24 @@\n+# name: test/sql/copy/parquet/writer/partition_without_hive.test\n+# description: Test writing partitioned files WITHOUT hive partitioning\n+# group: [writer]\n+\n+require parquet\n+\n+statement ok\n+CREATE TABLE t1(part_key INT, val INT);\n+\n+statement ok\n+INSERT INTO t1 SELECT i%2, i FROM range(10) t(i);\n+\n+statement ok\n+COPY t1 TO '__TEST_DIR__/hive_filters' (FORMAT PARQUET, PARTITION_BY part_key, HIVE_FILE_PATTERN false, WRITE_PARTITION_COLUMNS true);\n+\n+query I\n+SELECT file.replace('__TEST_DIR__', '').replace('\\', '/') FROM GLOB('__TEST_DIR__/hive_filters/*.parquet') ORDER BY ALL\n+----\n+/hive_filters/data_0.parquet\n+/hive_filters/data_1.parquet\n+\n+query II\n+FROM '__TEST_DIR__/hive_filters/*.parquet' EXCEPT ALL FROM t1\n+----\n---\n"}], "ideal": "Title: Add support for the HIVE_FILE_PATTERN option - that allows partitioned files to be written without writing them to a hive-style directory structure\n\nBody: For example, the following query:\r\n\r\n```sql\r\nCREATE TABLE t1(part_key INT, val INT);\r\nINSERT INTO t1 SELECT i%2, i FROM range(10) t(i);\r\n\r\nCOPY t1 TO 'hive_filters' (FORMAT PARQUET, PARTITION_BY part_key, HIVE_FILE_PATTERN false, WRITE_PARTITION_COLUMNS true);\r\n```\r\n\r\nNow writes the files:\r\n\r\n```\r\nhive_filters/data_0.parquet\r\nhive_filters/data_1.parquet\r\n```\r\n\r\nInstead of:\r\n\r\n```\r\nhive_filters/part_key=0/data_0.parquet\r\nhive_filters/part_key=1/data_0.parquet\r\n```"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/Windows.yml\n@@ -254,8 +254,7 @@ jobs:\n  mingw:\n      name: MinGW (64 Bit)\n      runs-on: windows-2019\n-     #FIXME: add this back -- if: ${{ inputs.skip_tests != 'true' }}\n-     if: false\n+     if: ${{ inputs.skip_tests != 'true' }}\n      needs: win-release-64\n      steps:\n        - uses: actions/checkout@v4\n---\n"}], "ideal": "Title: Revert \"Skip MinGW, currently failing on main\"\n\nBody: This reverts commit 41a586f730dc732173755a4879ebbcc243339d77.\r\n\r\nThis should now be working, fixed likely via https://github.com/duckdb/duckdb/pull/17319"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/icu/third_party/icu/common/unistr.cpp\n@@ -1563,7 +1563,11 @@ UnicodeString::doAppend(const UChar *srcChars, int32_t srcStart, int32_t srcLeng\n   }\n \n   int32_t oldLength = length();\n-  int32_t newLength = oldLength + srcLength;\n+  int32_t newLength;\n+  if (uprv_add32_overflow(oldLength, srcLength, &newLength)) {\n+    setToBogus();\n+    return *this;\n+  }\n \n   // Check for append onto ourself\n   const UChar* oldArray = getArrayStart();\n---\n"}], "ideal": "Title: Fix potential vulnerable cloned function\n\nBody: Dear Development team,\r\n\r\nI identified another vulnerability in a clone function doAppend() in `extension/icu/third_party/icu/common/unistr.cpp` sourced from [unicode-org/icu](https://github.com/unicode-org/icu). These issues, originally reported in [CVE-2020-10531](https://nvd.nist.gov/vuln/detail/cve-2020-10531), were resolved in the repository via this commit https://github.com/unicode-org/icu/commit/b7d08bc04a4296982fcef8b6b8a354a9e4e7afca.\r\n\r\nThis PR applies the corresponding patch to prevent a potential integer overflow in this codebase.\r\n\r\nPlease review at your convenience. Thank you for your time and attention!"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "third_party/hyperloglog/sds.cpp\n@@ -201,7 +201,7 @@ void sdsclear(sds s) {\n sds sdsMakeRoomFor(sds s, size_t addlen) {\n     void *sh, *newsh;\n     size_t avail = sdsavail(s);\n-    size_t len, newlen;\n+    size_t len, newlen, reqlen;\n     char type, oldtype = s[-1] & SDS_TYPE_MASK;\n     int hdrlen;\n \n@@ -210,7 +210,8 @@ sds sdsMakeRoomFor(sds s, size_t addlen) {\n \n     len = sdslen(s);\n     sh = (char*)s-sdsHdrSize(oldtype);\n-    newlen = (len+addlen);\n+    reqlen = newlen = (len+addlen);\n+    assert(newlen > len);   /* Catch size_t overflow */\n     if (newlen < SDS_MAX_PREALLOC)\n         newlen *= 2;\n     else\n@@ -224,6 +225,7 @@ sds sdsMakeRoomFor(sds s, size_t addlen) {\n     if (type == SDS_TYPE_5) type = SDS_TYPE_8;\n \n     hdrlen = sdsHdrSize(type);\n+    assert(hdrlen + newlen + 1 > reqlen);  /* Catch size_t overflow */\n     if (oldtype==type) {\n         newsh = realloc(sh, hdrlen+newlen+1);\n         if (newsh == NULL) return NULL;\n---\n"}], "ideal": "Title: Fix potential vulnerable cloned function\n\nBody: Dear Development team,\r\n\r\nI identified a vulnerability in a clone function sdsMakeRoomFor() in `third_party/hyperloglog/sds.cpp` sourced from [redis/redis](https://github.com/redis/redis). These issues, originally reported in [CVE-2021-41099](https://nvd.nist.gov/vuln/detail/cve-2021-41099), were resolved in the repository via this commit https://github.com/redis/redis/commit/c6ad876774f3cc11e32681ea02a2eead00f2c521.\r\n\r\nThis PR applies the corresponding patch to fix the vulnerabilities in this codebase. \r\n\r\nPlease review at your convenience. Thank you for your time and attention!"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/parquet_extension.cpp\n@@ -303,8 +303,9 @@ unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyFunctionBi\n \t\t\t\t * interchangeably */\n \t\t\t\tbind_data->codec = duckdb_parquet::CompressionCodec::LZ4_RAW;\n \t\t\t} else {\n-\t\t\t\tthrow BinderException(\"Expected %s argument to be either [uncompressed, brotli, gzip, snappy, or zstd]\",\n-\t\t\t\t                      loption);\n+\t\t\t\tthrow BinderException(\n+\t\t\t\t    \"Expected %s argument to be any of [uncompressed, brotli, gzip, snappy, lz4, lz4_raw or zstd]\",\n+\t\t\t\t    loption);\n \t\t\t}\n \t\t} else if (loption == \"field_ids\") {\n \t\t\tif (option.second[0].type().id() == LogicalTypeId::VARCHAR &&\n---\nscripts/generate_c_api.py\n@@ -342,8 +342,75 @@ def create_function_typedef(function_obj):\n     return f'#define {function_name} {DUCKDB_EXT_API_VAR_NAME}.{function_name}\\n'\n \n \n+def headline_capitalize(s, i):\n+    if i > 0 and s in [\n+        \"a\",\n+        \"an\",\n+        \"the\",\n+        \"and\",\n+        \"but\",\n+        \"or\",\n+        \"nor\",\n+        \"for\",\n+        \"so\",\n+        \"yet\",\n+        \"about\",\n+        \"above\",\n+        \"across\",\n+        \"after\",\n+        \"against\",\n+        \"along\",\n+        \"among\",\n+        \"around\",\n+        \"at\",\n+        \"before\",\n+        \"behind\",\n+        \"below\",\n+        \"beneath\",\n+        \"beside\",\n+        \"between\",\n+        \"beyond\",\n+        \"by\",\n+        \"despite\",\n+        \"down\",\n+        \"during\",\n+        \"except\",\n+        \"for\",\n+        \"from\",\n+        \"in\",\n+        \"inside\",\n+        \"into\",\n+        \"like\",\n+        \"near\",\n+        \"of\",\n+        \"off\",\n+        \"on\",\n+        \"onto\",\n+        \"out\",\n+        \"outside\",\n+        \"over\",\n+        \"past\",\n+        \"since\",\n+        \"through\",\n+        \"throughout\",\n+        \"to\",\n+        \"toward\",\n+        \"under\",\n+        \"underneath\",\n+        \"until\",\n+        \"up\",\n+        \"upon\",\n+        \"with\",\n+        \"within\",\n+        \"without\",\n+    ]:\n+        return s\n+    else:\n+        return s.capitalize()\n+\n+\n def to_camel_case(snake_str):\n-    return \" \".join(x.capitalize() for x in snake_str.lower().split(\"_\"))\n+    return \" \".join(headline_capitalize(s, i) for i, s in enumerate(snake_str.lower().split(\"_\")))\n \n \n def parse_semver(version):\n---\nsrc/include/duckdb.h\n@@ -1610,7 +1610,7 @@ Returns the statement type of the statement to be executed\n DUCKDB_C_API duckdb_statement_type duckdb_prepared_statement_type(duckdb_prepared_statement statement);\n \n //===--------------------------------------------------------------------===//\n-// Bind Values To Prepared Statements\n+// Bind Values to Prepared Statements\n //===--------------------------------------------------------------------===//\n \n /*!\n---\nsrc/parser/transform/constraint/transform_constraint.cpp\n@@ -127,7 +127,7 @@ unique_ptr<Constraint> Transformer::TransformConstraint(duckdb_libpgquery::PGCon\n \t\tcolumn.SetCompressionType(CompressionTypeFromString(constraint.compression_name));\n \t\tif (column.CompressionType() == CompressionType::COMPRESSION_AUTO) {\n \t\t\tthrow ParserException(\"Unrecognized option for column compression, expected none, uncompressed, rle, \"\n-\t\t\t                      \"dictionary, pfor, bitpacking or fsst\");\n+\t\t\t                      \"dictionary, pfor, bitpacking, fsst, chimp, patas, zstd, alp, alprd or roaring\");\n \t\t}\n \t\treturn nullptr;\n \tcase duckdb_libpgquery::PG_CONSTR_FOREIGN:\n---\ntools/juliapkg/src/api.jl\n@@ -1829,7 +1829,7 @@ end\n \n \n # --------------------------------------------------------------------------------\n-# Bind Values To Prepared Statements\n+# Bind Values to Prepared Statements\n # --------------------------------------------------------------------------------\n \n \"\"\"\n---\n"}], "ideal": "Title: Some nitpicking fixes\n\nBody: Fixes #16601 and a bunch of other missing compression methods/nitpicking things"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/scalar/string/md5.cpp\n@@ -26,7 +26,7 @@ struct MD5Number128Operator {\n \t\tMD5Context context;\n \t\tcontext.Add(input);\n \t\tcontext.Finish(digest);\n-\t\treturn *reinterpret_cast<hugeint_t *>(digest);\n+\t\treturn *reinterpret_cast<uhugeint_t *>(digest);\n \t}\n };\n \n@@ -39,7 +39,7 @@ static void MD5Function(DataChunk &args, ExpressionState &state, Vector &result)\n static void MD5NumberFunction(DataChunk &args, ExpressionState &state, Vector &result) {\n \tauto &input = args.data[0];\n \n-\tUnaryExecutor::Execute<string_t, hugeint_t, MD5Number128Operator>(input, result, args.size());\n+\tUnaryExecutor::Execute<string_t, uhugeint_t, MD5Number128Operator>(input, result, args.size());\n }\n \n ScalarFunctionSet MD5Fun::GetFunctions() {\n@@ -51,8 +51,8 @@ ScalarFunctionSet MD5Fun::GetFunctions() {\n \n ScalarFunctionSet MD5NumberFun::GetFunctions() {\n \tScalarFunctionSet set(\"md5_number\");\n-\tset.AddFunction(ScalarFunction({LogicalType::VARCHAR}, LogicalType::HUGEINT, MD5NumberFunction));\n-\tset.AddFunction(ScalarFunction({LogicalType::BLOB}, LogicalType::HUGEINT, MD5NumberFunction));\n+\tset.AddFunction(ScalarFunction({LogicalType::VARCHAR}, LogicalType::UHUGEINT, MD5NumberFunction));\n+\tset.AddFunction(ScalarFunction({LogicalType::BLOB}, LogicalType::UHUGEINT, MD5NumberFunction));\n \treturn set;\n }\n \n---\ntest/sql/function/string/md5.test\n@@ -19,7 +19,7 @@ ab2a0d28de6b77ffdd6c72afead099ab\n query II\n select md5_number('hello'), md5_number_upper(NULL)\n ----\n--145191729247072235933599665601768308387\n+195090637673866227529774941829999903069\n NULL\n \n query II\n---\n"}], "ideal": "Title: md5_number: return UHUGEINT\n\nBody: Would fix #17285 \r\nPlease review and lmk whether it makes sense to make this change"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/InvokeCI.yml\n@@ -126,8 +126,8 @@ jobs:\n     if: ${{ inputs.git_ref == '' && always() }} \n     with:\n       is-success: ${{ needs.prepare-status.outputs.is-success }}\n-      target-branch: ${{ github.ref_name }}\n-      duckdb-sha: ${{ github.ref }}\n+      target-branch: ${{ github.ref }}\n+      duckdb-sha: ${{ github.sha }}\n       triggering-event: ${{ github.event_name }}\n       should-publish: 'true'\n \n@@ -138,7 +138,7 @@ jobs:\n     if: ${{ inputs.git_ref != '' && always() }}\n     with:\n       is-success: ${{ needs.prepare-status.outputs.is-success }}\n-      target-branch: ${{ github.ref_name }}\n-      duckdb-sha: ${{ inputs.git_ref }}\n+      target-branch: ${{ inputs.git_ref }}\n+      duckdb-sha: ${{ github.sha }}\n       triggering-event: ${{ github.event_name }}\n       should-publish: 'true'\n---\n.github/workflows/NotifyExternalRepositories.yml\n@@ -66,7 +66,7 @@ jobs:\n         if: ${{ github.repository == 'duckdb/duckdb' }}\n         run: |\n           export URL=https://api.github.com/repos/duckdb/duckdb-odbc/actions/workflows/Vendor.yml/dispatches\n-          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"duckdb-sha\": \"${{ inputs.duckdb-sha }}\"}}'\n+          export DATA='{\"ref\": \"${{ inputs.target-branch }}\", \"inputs\": {\"duckdb-sha\": \"${{ inputs.duckdb-sha }}\"}}'\n           curl -v -XPOST -u \"${PAT_USER}:${PAT_TOKEN}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" $URL --data \"$DATA\"\n \n   notify-jdbc-run:\n@@ -81,7 +81,7 @@ jobs:\n         if: ${{ github.repository == 'duckdb/duckdb' }}\n         run: |\n           export URL=https://api.github.com/repos/duckdb/duckdb-java/actions/workflows/Vendor.yml/dispatches\n-          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"duckdb-sha\": \"${{ inputs.duckdb-sha }}\"}}'\n+          export DATA='{\"ref\": \"${{ inputs.target-branch }}\", \"inputs\": {\"duckdb-sha\": \"${{ inputs.duckdb-sha }}\"}}'\n           curl -v -XPOST -u \"${PAT_USER}:${PAT_TOKEN}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" $URL --data \"$DATA\"\n \n   notify-nightly-build-status:\n@@ -95,5 +95,5 @@ jobs:\n         if: ${{ github.repository == 'duckdb/duckdb' }}\n         run: |\n           export URL=https://api.github.com/repos/duckdb/duckdb-build-status/actions/workflows/NightlyBuildsCheck.yml/dispatches\n-          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"event\": \"${{ inputs.triggering-event }}\", \"should_publish\": \"${{ inputs.should-publish }}\"}}'\n+          export DATA='{\"ref\": \"${{ inputs.target-branch }}\", \"inputs\": {\"event\": \"${{ inputs.triggering-event }}\", \"should_publish\": \"${{ inputs.should-publish }}\"}}'\n           curl -v -XPOST -u \"${PAT_USER}:${PAT_TOKEN}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" $URL --data \"$DATA\"\n---\n"}], "ideal": "Title: Pass commit ID to NotifyExternalRepositories.yml\n\nBody: There is a problem uncovered with ODBC/JDBC repos notifications. These notifications are expected to be dispatched only after all tests are passed to ensure that the exact commit of the imported DuckDB sources has the full set of extensions published.\r\n\r\nCurrently `github.ref` (for `main` branch) or `inputs.git_ref` (for custom branch) values are passed to `NotifyExternalRepositories.yml` in `duckdb-sha` parameter, for example: `refs/heads/main`. This causes the `Vendor.yml` on ODBC/JDBC side to checkout the latest commit on the `main` branch that does not necessary match the actual commit value with which `InvokeCI.yml` was run.\r\n\r\nProposed change uses `github.sha` instead assuming that it is available in `InvokeCI.yml` (at least correct commit ID is displayed in web UI for it). It also passes `github.ref` (or `input.git_ref`) as a `target-branch` parameter to `NotifyExternalRepositories.yml` instead the `github.ref_name`."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/table/range.cpp\n@@ -105,6 +105,7 @@ static void GenerateRangeParameters(DataChunk &input, idx_t row_id, RangeFunctio\n \tif (result.increment == 0) {\n \t\tthrow BinderException(\"interval cannot be 0!\");\n \t}\n+\tresult.empty_range = false;\n \tif (result.start > result.end && result.increment > 0) {\n \t\tresult.empty_range = true;\n \t}\n---\ntest/sql/function/list/generate_series.test\n@@ -211,3 +211,10 @@ query I\n SELECT range(NULL, NULL, NULL)\n ----\n NULL\n+\n+query II\n+SELECT * FROM (SELECT 1 UNION ALL SELECT 0 UNION ALL SELECT 2) AS _(x), generate_series(1, x) AS __(y) ORDER BY x, y\n+----\n+1\t1\n+2\t1\n+2\t2\n---\n"}], "ideal": "Title: Fix issue with empty ranges\n\nBody: Under certain circumstances, a single empty invocation of `range`/`generate_series` would result in an empty result for the entire `generate_series` join:\r\n\r\n```sql\r\nSELECT * FROM\r\n  (SELECT 1 UNION ALL SELECT 0 UNION ALL SELECT 2) AS _(x),\r\n  generate_series(1, x) AS __(y) ORDER BY x, y\r\n```\r\n\r\nWe have to reset the `empty_range` flag each time."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "data/json/format_string_key.json\n@@ -0,0 +1 @@\n+{\"%s\":42,\"%s\":43}\n---\nextension/json/json_functions/json_transform.cpp\n@@ -262,8 +262,8 @@ bool JSONTransform::GetStringVector(yyjson_val *vals[], const idx_t count, const\n \t\tif (!unsafe_yyjson_is_str(val)) {\n \t\t\tvalidity.SetInvalid(i);\n \t\t\tif (success && options.strict_cast && !unsafe_yyjson_is_str(val)) {\n-\t\t\t\toptions.error_message = StringUtil::Format(\"Unable to cast '%s' to \" + EnumUtil::ToString(target.id()),\n-\t\t\t\t                                           JSONCommon::ValToString(val, 50));\n+\t\t\t\toptions.error_message = StringUtil::Format(\n+\t\t\t\t    \"Unable to cast '%s' to %s\", JSONCommon::ValToString(val, 50), EnumUtil::ToString(target.id()));\n \t\t\t\toptions.object_index = i;\n \t\t\t\tsuccess = false;\n \t\t\t}\n@@ -424,8 +424,8 @@ bool JSONTransform::TransformObject(yyjson_val *objects[], yyjson_alc *alc, cons\n \t\t\t\tif (found_keys[col_idx]) {\n \t\t\t\t\tif (success && options.error_duplicate_key) {\n \t\t\t\t\t\toptions.error_message =\n-\t\t\t\t\t\t    StringUtil::Format(\"Duplicate key \\\"\" + string(key_ptr, key_len) + \"\\\" in object %s\",\n-\t\t\t\t\t\t                       JSONCommon::ValToString(objects[i], 50));\n+\t\t\t\t\t\t    StringUtil::Format(\"Object %s has duplicate key \\\"%s\\\"\",\n+\t\t\t\t\t\t                       JSONCommon::ValToString(objects[i], 50), string(key_ptr, key_len));\n \t\t\t\t\t\toptions.object_index = i;\n \t\t\t\t\t\tsuccess = false;\n \t\t\t\t\t}\n@@ -436,8 +436,8 @@ bool JSONTransform::TransformObject(yyjson_val *objects[], yyjson_alc *alc, cons\n \t\t\t\t}\n \t\t\t} else if (success && error_unknown_key && options.error_unknown_key) {\n \t\t\t\toptions.error_message =\n-\t\t\t\t    StringUtil::Format(\"Object %s has unknown key \\\"\" + string(key_ptr, key_len) + \"\\\"\",\n-\t\t\t\t                       JSONCommon::ValToString(objects[i], 50));\n+\t\t\t\t    StringUtil::Format(\"Object %s has unknown key \\\"%s\\\"\", JSONCommon::ValToString(objects[i], 50),\n+\t\t\t\t                       string(key_ptr, key_len));\n \t\t\t\toptions.object_index = i;\n \t\t\t\tsuccess = false;\n \t\t\t}\n@@ -453,8 +453,8 @@ bool JSONTransform::TransformObject(yyjson_val *objects[], yyjson_alc *alc, cons\n \t\t\t\tnested_vals[col_idx][i] = nullptr;\n \n \t\t\t\tif (success && options.error_missing_key) {\n-\t\t\t\t\toptions.error_message = StringUtil::Format(\"Object %s does not have key \\\"\" + names[col_idx] + \"\\\"\",\n-\t\t\t\t\t                                           JSONCommon::ValToString(objects[i], 50));\n+\t\t\t\t\toptions.error_message = StringUtil::Format(\"Object %s does not have key \\\"%s\\\"\",\n+\t\t\t\t\t                                           JSONCommon::ValToString(objects[i], 50), names[col_idx]);\n \t\t\t\t\toptions.object_index = i;\n \t\t\t\t\tsuccess = false;\n \t\t\t\t}\n@@ -750,7 +750,7 @@ static bool TransformObjectToMap(yyjson_val *objects[], yyjson_alc *alc, Vector\n \t// Transform keys\n \tif (!JSONTransform::Transform(keys, alc, MapVector::GetKeys(result), list_size, options, nullptr)) {\n \t\tthrow ConversionException(\n-\t\t    StringUtil::Format(options.error_message + \". Cannot default to NULL, because map keys cannot be NULL\"));\n+\t\t    StringUtil::Format(\"%s. Cannot default to NULL, because map keys cannot be NULL\", options.error_message));\n \t}\n \n \t// Transform values\n---\ntest/sql/json/issues/internal_issue4794.test\n@@ -0,0 +1,13 @@\n+# name: test/sql/json/issues/internal_issue4794.test\n+# description: Test internal issue 4794 - AFL++ issue: internal exception due to format string json key\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+pragma enable_verification\n+\n+statement error\n+FROM read_json('data/json/format_string_key.json');\n+----\n+Invalid Input Error\n---\n"}], "ideal": "Title: Properly format strings when throw JSON errors\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/4794\r\nJSON can contain pretty much any string, including format specifiers like `%s`. If we don't print these carefully, we run into an `InternalException` when trying to throw a descriptive error. With this PR, the error messages are now properly constructed."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/scalar/math/numeric.cpp\n@@ -1,3 +1,4 @@\n+#include \"duckdb/common/operator/decimal_cast_operators.hpp\"\n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/common/algorithm.hpp\"\n #include \"duckdb/common/likely.hpp\"\n@@ -566,6 +567,34 @@ struct RoundDecimalOperator {\n \t}\n };\n \n+struct RoundIntegerOperator {\n+\ttemplate <class TA, class TB, class TR>\n+\tstatic inline TR Operation(TA input, TB precision) {\n+\t\tif (precision < 0) {\n+\t\t\t//\tDo all the arithmetic at higher precision\n+\t\t\tusing POWERS_OF_TEN_CLASS = typename DecimalCastTraits<TA>::POWERS_OF_TEN_CLASS;\n+\t\t\tif (precision <= -POWERS_OF_TEN_CLASS::CACHED_POWERS_OF_TEN) {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t\tconst auto power_of_ten = POWERS_OF_TEN_CLASS::POWERS_OF_TEN[-precision];\n+\t\t\tauto addition = power_of_ten / 2;\n+\t\t\tif (input < 0) {\n+\t\t\t\taddition = -addition;\n+\t\t\t}\n+\t\t\taddition += input;\n+\t\t\taddition /= power_of_ten;\n+\t\t\tif (addition) {\n+\t\t\t\treturn UnsafeNumericCast<TR>(addition * power_of_ten);\n+\t\t\t} else {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t} else {\n+\t\t\t//\tRounding integers to higher precision is a NOP\n+\t\t\treturn input;\n+\t\t}\n+\t}\n+};\n+\n struct RoundPrecisionFunctionData : public FunctionData {\n \texplicit RoundPrecisionFunctionData(int32_t target_scale) : target_scale(target_scale) {\n \t}\n@@ -698,10 +727,6 @@ ScalarFunctionSet RoundFun::GetFunctions() {\n \t\tscalar_function_t round_func = nullptr;\n \t\tbind_scalar_function_t bind_func = nullptr;\n \t\tbind_scalar_function_t bind_prec_func = nullptr;\n-\t\tif (type.IsIntegral()) {\n-\t\t\t// no round for integral numbers\n-\t\t\tcontinue;\n-\t\t}\n \t\tswitch (type.id()) {\n \t\tcase LogicalTypeId::FLOAT:\n \t\t\tround_func = ScalarFunction::UnaryFunction<float, float, RoundOperator>;\n@@ -715,7 +740,31 @@ ScalarFunctionSet RoundFun::GetFunctions() {\n \t\t\tbind_func = BindGenericRoundFunctionDecimal<RoundDecimalOperator>;\n \t\t\tbind_prec_func = BindDecimalRoundPrecision;\n \t\t\tbreak;\n+\t\tcase LogicalTypeId::TINYINT:\n+\t\t\tround_func = ScalarFunction::NopFunction;\n+\t\t\tround_prec_func = ScalarFunction::BinaryFunction<int8_t, int32_t, int8_t, RoundIntegerOperator>;\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::SMALLINT:\n+\t\t\tround_func = ScalarFunction::NopFunction;\n+\t\t\tround_prec_func = ScalarFunction::BinaryFunction<int16_t, int32_t, int16_t, RoundIntegerOperator>;\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::INTEGER:\n+\t\t\tround_func = ScalarFunction::NopFunction;\n+\t\t\tround_prec_func = ScalarFunction::BinaryFunction<int32_t, int32_t, int32_t, RoundIntegerOperator>;\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::BIGINT:\n+\t\t\tround_func = ScalarFunction::NopFunction;\n+\t\t\tround_prec_func = ScalarFunction::BinaryFunction<int64_t, int32_t, int64_t, RoundIntegerOperator>;\n+\t\t\tbreak;\n+\t\tcase LogicalTypeId::HUGEINT:\n+\t\t\tround_func = ScalarFunction::NopFunction;\n+\t\t\tround_prec_func = ScalarFunction::BinaryFunction<hugeint_t, int32_t, hugeint_t, RoundIntegerOperator>;\n+\t\t\tbreak;\n \t\tdefault:\n+\t\t\tif (type.IsIntegral()) {\n+\t\t\t\t// no round for integral numbers\n+\t\t\t\tcontinue;\n+\t\t\t}\n \t\t\tthrow InternalException(\"Unimplemented numeric type for function \\\"floor\\\"\");\n \t\t}\n \t\tround.AddFunction(ScalarFunction({type}, type, round_func, bind_func));\n---\nsrc/common/operator/cast_operators.cpp\n@@ -1800,11 +1800,12 @@ struct HugeIntegerCastOperation {\n \t\t}\n \n \t\t// Get the first (left-most) digit of the decimals\n-\t\twhile (state.decimal_total_digits > 39) {\n-\t\t\tstate.decimal /= T::Operation::POWERS_OF_TEN[39];\n-\t\t\tstate.decimal_total_digits -= 39;\n+\t\tconstexpr auto MAX_DIGITS = T::Operation::CACHED_POWERS_OF_TEN - 1;\n+\t\twhile (state.decimal_total_digits > MAX_DIGITS) {\n+\t\t\tstate.decimal /= T::Operation::POWERS_OF_TEN[MAX_DIGITS];\n+\t\t\tstate.decimal_total_digits -= MAX_DIGITS;\n \t\t}\n-\t\tD_ASSERT((state.decimal_total_digits - 1) >= 0 && (state.decimal_total_digits - 1) <= 39);\n+\t\tD_ASSERT((state.decimal_total_digits - 1) >= 0 && (state.decimal_total_digits - 1) <= MAX_DIGITS);\n \t\tstate.decimal /= T::Operation::POWERS_OF_TEN[state.decimal_total_digits - 1];\n \n \t\tif (state.decimal >= 5) {\n---\nsrc/include/duckdb/common/types/cast_helpers.hpp\n@@ -23,7 +23,7 @@ namespace duckdb {\n //! NumericHelper is a static class that holds helper functions for integers/doubles\n class NumericHelper {\n public:\n-\tstatic constexpr uint8_t CACHED_POWERS_OF_TEN = 20;\n+\tstatic constexpr uint8_t CACHED_POWERS_OF_TEN = 19;\n \tstatic const int64_t POWERS_OF_TEN[CACHED_POWERS_OF_TEN];\n \tstatic const double DOUBLE_POWERS_OF_TEN[40];\n \n---\nsrc/include/duckdb/common/types/hugeint.hpp\n@@ -171,7 +171,8 @@ class Hugeint {\n \t\treturn upper_smaller | (upper_equal & lower_smaller_equals);\n \t}\n \n-\tstatic const hugeint_t POWERS_OF_TEN[40];\n+\tstatic constexpr uint8_t CACHED_POWERS_OF_TEN = 39;\n+\tstatic const hugeint_t POWERS_OF_TEN[CACHED_POWERS_OF_TEN];\n };\n \n template <>\n---\nsrc/include/duckdb/common/types/uhugeint.hpp\n@@ -160,7 +160,8 @@ class Uhugeint {\n \t\treturn upper_smaller | (upper_equal & lower_smaller_equals);\n \t}\n \n-\tstatic const uhugeint_t POWERS_OF_TEN[40];\n+\tstatic constexpr uint8_t CACHED_POWERS_OF_TEN = 39;\n+\tstatic const uhugeint_t POWERS_OF_TEN[CACHED_POWERS_OF_TEN];\n };\n \n template <>\n---\ntest/sql/function/numeric/test_round_integers.test\n@@ -0,0 +1,47 @@\n+# name: test/sql/function/numeric/test_round_integers.test\n+# description: Test the special cased integer rounding\n+# group: [numeric]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE zz AS\n+    SELECT\n+           CAST(i AS SMALLINT) AS id,\n+           CAST(i AS SMALLINT) AS si\n+    FROM generate_series(1, 1000) t(i);\n+\n+query I\n+SELECT ROUND(53) AS ag_column3\n+FROM zz\n+GROUP BY ag_column3\n+ORDER BY ag_column3\n+----\n+53\n+\n+query I\n+SELECT ROUND(53, si) AS ag_column3\n+FROM zz\n+GROUP BY ag_column3\n+ORDER BY ag_column3\n+----\n+53\n+\n+query I\n+SELECT ROUND(53, -si) AS ag_column3\n+FROM zz\n+GROUP BY ag_column3\n+ORDER BY ag_column3\n+----\n+0\n+50\n+100\n+\n+query I\n+select round(100::INTEGER, int)\n+from test_all_types();\n+----\n+0\n+100\n+NULL\n---\ntest/sql/window/test_custom_spooling.test_slow\n@@ -22,14 +22,13 @@ statement ok\n PRAGMA enable_verification\n \n # MODE is very unstable on this data\n-query IIII\n-SELECT SUM(s), SUM(q), SUM(m), ROUND(SUM(d), -4)\n+query III\n+SELECT SUM(s), SUM(q), SUM(m)\n FROM (\n \tSELECT\n \t\tSUM(l_extendedprice) OVER w AS s,\n \t\tQUANTILE(l_extendedprice, 0.5) OVER w AS q,\n \t\tMAD(l_extendedprice) OVER w AS m,\n-\t\tMODE(l_linenumber) OVER w AS d,\n \tFROM lineitem\n \tWINDOW w AS (\n \t\tPARTITION BY l_suppkey \n@@ -38,4 +37,4 @@ FROM (\n \t)\n ) t;\n ----\n-871183083803.67\t20718441622.70\t9873496927.55\t1050000.0\n+871183083803.67\t20718441622.70\t9873496927.55\n---\n"}], "ideal": "Title: Issue #17299: Integer Rounding\n\nBody: Add ROUND implementations for integer arguments.\r\n* Fix incorrect POWERS_OF_TEN size constants\r\n* Give up on MODE scaling test (too unstable/non-deterministic).\r\n* Switch incorrect random constant to correct class constant.\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/operator/cast_operators.cpp\n@@ -2311,7 +2311,7 @@ bool DoubleToDecimalCast(SRC input, DST &result, CastParameters &parameters, uin\n \tdouble value = input * NumericHelper::DOUBLE_POWERS_OF_TEN[scale];\n \tdouble roundedValue = round(value);\n \tif (roundedValue <= -NumericHelper::DOUBLE_POWERS_OF_TEN[width] ||\n-\t    roundedValue >= NumericHelper::DOUBLE_POWERS_OF_TEN[width]) {\n+\t    roundedValue >= NumericHelper::DOUBLE_POWERS_OF_TEN[width] || !Value::IsFinite(roundedValue)) {\n \t\tstring error = StringUtil::Format(\"Could not cast value %f to DECIMAL(%d,%d)\", input, width, scale);\n \t\tHandleCastError::AssignError(error, parameters);\n \t\treturn false;\n---\ntest/issues/general/test_17310.test\n@@ -0,0 +1,12 @@\n+# name: test/issues/general/test_17310.test\n+# description: Issue 17310 - Casting issue from float64 to decimal\n+# group: [general]\n+\n+statement ok\n+pragma enable_verification\n+\n+# should not error\n+query I\n+select try_cast('nan'::double as decimal(18,2));\n+----\n+NULL\n---\n"}], "ideal": "Title: Fix `try_cast` from NaN `double` to `decimal`\n\nBody: Fixes #17310\r\n\r\nThis should return `NULL`, not throw an error."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/common/table_column.hpp\n@@ -14,11 +14,15 @@\n namespace duckdb {\n \n struct TableColumn {\n+\tTableColumn() = default;\n \tTableColumn(string name_p, LogicalType type_p) : name(std::move(name_p)), type(std::move(type_p)) {\n \t}\n \n \tstring name;\n \tLogicalType type;\n+\n+\tvoid Serialize(Serializer &serializer) const;\n+\tstatic TableColumn Deserialize(Deserializer &deserializer);\n };\n \n using virtual_column_map_t = unordered_map<column_t, TableColumn>;\n---\nsrc/include/duckdb/storage/serialization/nodes.json\n@@ -1086,5 +1086,25 @@\n       }\n     ],\n     \"pointer_type\": \"none\"\n+  },\n+  {\n+    \"class\": \"TableColumn\",\n+    \"includes\": [\n+      \"duckdb/common/table_column.hpp\"\n+    ],\n+    \"members\": [\n+      {\n+        \"id\": 100,\n+        \"name\": \"name\",\n+        \"type\": \"string\"\n+      },\n+      {\n+        \"id\": 101,\n+        \"name\": \"type\",\n+        \"type\": \"LogicalType\"\n+      }\n+    ],\n+    \"pointer_type\": \"none\",\n+    \"constructor\": [\"name\", \"type\"]\n   }\n ]\n---\nsrc/storage/serialization/serialize_nodes.cpp\n@@ -34,6 +34,7 @@\n #include \"duckdb/parser/qualified_name.hpp\"\n #include \"duckdb/parser/parsed_data/exported_table_data.hpp\"\n #include \"duckdb/common/column_index.hpp\"\n+#include \"duckdb/common/table_column.hpp\"\n \n namespace duckdb {\n \n@@ -647,6 +648,18 @@ StrpTimeFormat StrpTimeFormat::Deserialize(Deserializer &deserializer) {\n \treturn result;\n }\n \n+void TableColumn::Serialize(Serializer &serializer) const {\n+\tserializer.WritePropertyWithDefault<string>(100, \"name\", name);\n+\tserializer.WriteProperty<LogicalType>(101, \"type\", type);\n+}\n+\n+TableColumn TableColumn::Deserialize(Deserializer &deserializer) {\n+\tauto name = deserializer.ReadPropertyWithDefault<string>(100, \"name\");\n+\tauto type = deserializer.ReadProperty<LogicalType>(101, \"type\");\n+\tTableColumn result(std::move(name), std::move(type));\n+\treturn result;\n+}\n+\n void TableFilterSet::Serialize(Serializer &serializer) const {\n \tserializer.WritePropertyWithDefault<map<idx_t, unique_ptr<TableFilter>>>(100, \"filters\", filters);\n }\n---\n"}], "ideal": "Title: Add serialization for new TableColumn type\n\nBody: Adds missing serialization callbacks"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/parser/transform/helpers/transform_cte.cpp\n@@ -45,10 +45,11 @@ void Transformer::TransformCTE(duckdb_libpgquery::PGWithClause &de_with_clause,\n \t\tauto info = make_uniq<CommonTableExpressionInfo>();\n \n \t\tauto &cte = *PGPointerCast<duckdb_libpgquery::PGCommonTableExpr>(cte_ele->data.ptr_value);\n-\n-\t\tauto key_target = PGPointerCast<duckdb_libpgquery::PGNode>(cte.recursive_keys->head->data.ptr_value);\n-\t\tif (key_target) {\n-\t\t\tTransformExpressionList(*cte.recursive_keys, info->key_targets);\n+\t\tif (cte.recursive_keys) {\n+\t\t\tauto key_target = PGPointerCast<duckdb_libpgquery::PGNode>(cte.recursive_keys->head->data.ptr_value);\n+\t\t\tif (key_target) {\n+\t\t\t\tTransformExpressionList(*cte.recursive_keys, info->key_targets);\n+\t\t\t}\n \t\t}\n \n \t\tif (cte.aliascolnames) {\n---\ntest/sql/cte/cte_issue_17311.test\n@@ -0,0 +1,21 @@\n+# name: test/sql/cte/cte_issue_17311.test\n+# description: Test for CTE issue #17311\n+# group: [cte]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE RECURSIVE VIEW nums (n) AS\n+    VALUES (1)\n+UNION ALL\n+    SELECT n+1 FROM nums WHERE n < 5;\n+\n+query I\n+FROM nums\n+----\n+1\n+2\n+3\n+4\n+5\n---\n"}], "ideal": "Title: Fix #17311: correctly check for presence of recursive keys in transformer\n\nBody: Fixes #17311\r\n\r\nThis snuck in in https://github.com/duckdb/duckdb/pull/12430"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/file_system.cpp\n@@ -442,6 +442,43 @@ void FileSystem::CreateDirectory(const string &directory, optional_ptr<FileOpene\n \tthrow NotImplementedException(\"%s: CreateDirectory is not implemented!\", GetName());\n }\n \n+void FileSystem::CreateDirectoriesRecursive(const string &path, optional_ptr<FileOpener> opener) {\n+\t// To avoid hitting directories we have no permission for when using allowed_directories + enable_external_access,\n+\t// we construct the list of directories to be created depth-first. This avoids calling DirectoryExists on a parent\n+\t// dir that is not in the allowed_directories list\n+\n+\tauto sep = PathSeparator(path);\n+\tvector<string> dirs_to_create;\n+\n+\tstring current_prefix = path;\n+\n+\tStringUtil::RTrim(current_prefix, sep);\n+\n+\t// Strip directories from the path until we hit a directory that exists\n+\twhile (!current_prefix.empty() && !DirectoryExists(current_prefix)) {\n+\t\tauto found = current_prefix.find_last_of(sep);\n+\n+\t\t// Push back the root dir\n+\t\tif (found == string::npos || found == 0) {\n+\t\t\tdirs_to_create.push_back(current_prefix);\n+\t\t\tcurrent_prefix = \"\";\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\t// Add the directory to the directories to be created\n+\t\tdirs_to_create.push_back(current_prefix.substr(found, current_prefix.size() - found));\n+\n+\t\t// Update the current prefix to remove the current dir\n+\t\tcurrent_prefix = current_prefix.substr(0, found);\n+\t}\n+\n+\t// Create the directories one by one\n+\tfor (vector<string>::reverse_iterator riter = dirs_to_create.rbegin(); riter != dirs_to_create.rend(); ++riter) {\n+\t\tcurrent_prefix += *riter;\n+\t\tCreateDirectory(current_prefix);\n+\t}\n+}\n+\n void FileSystem::RemoveDirectory(const string &directory, optional_ptr<FileOpener> opener) {\n \tthrow NotImplementedException(\"%s: RemoveDirectory is not implemented!\", GetName());\n }\n---\nsrc/include/duckdb/common/file_system.hpp\n@@ -156,6 +156,8 @@ class FileSystem {\n \tDUCKDB_API virtual bool DirectoryExists(const string &directory, optional_ptr<FileOpener> opener = nullptr);\n \t//! Create a directory if it does not exist\n \tDUCKDB_API virtual void CreateDirectory(const string &directory, optional_ptr<FileOpener> opener = nullptr);\n+\t//! Helper function that uses DirectoryExists and CreateDirectory to ensure all directories in path are created\n+\tDUCKDB_API virtual void CreateDirectoriesRecursive(const string &path, optional_ptr<FileOpener> opener = nullptr);\n \t//! Recursively remove a directory and all files in it\n \tDUCKDB_API virtual void RemoveDirectory(const string &directory, optional_ptr<FileOpener> opener = nullptr);\n \n---\nsrc/main/extension/extension_install.cpp\n@@ -118,19 +118,7 @@ string ExtensionHelper::ExtensionDirectory(DatabaseInstance &db, FileSystem &fs)\n \tstring extension_directory = GetExtensionDirectoryPath(db, fs);\n \t{\n \t\tif (!fs.DirectoryExists(extension_directory)) {\n-\t\t\tauto sep = fs.PathSeparator(extension_directory);\n-\t\t\tauto splits = StringUtil::Split(extension_directory, sep);\n-\t\t\tD_ASSERT(!splits.empty());\n-\t\t\tstring extension_directory_prefix;\n-\t\t\tif (StringUtil::StartsWith(extension_directory, sep)) {\n-\t\t\t\textension_directory_prefix = sep; // this is swallowed by Split otherwise\n-\t\t\t}\n-\t\t\tfor (auto &split : splits) {\n-\t\t\t\textension_directory_prefix = extension_directory_prefix + split + sep;\n-\t\t\t\tif (!fs.DirectoryExists(extension_directory_prefix)) {\n-\t\t\t\t\tfs.CreateDirectory(extension_directory_prefix);\n-\t\t\t\t}\n-\t\t\t}\n+\t\t\tfs.CreateDirectoriesRecursive(extension_directory);\n \t\t}\n \t}\n \tD_ASSERT(fs.DirectoryExists(extension_directory));\n@@ -560,9 +548,6 @@ ExtensionHelper::InstallExtensionInternal(DatabaseInstance &db, FileSystem &fs,\n #ifdef DUCKDB_DISABLE_EXTENSION_LOAD\n \tthrow PermissionException(\"Installing external extensions is disabled through a compile time flag\");\n #else\n-\tif (!db.config.options.enable_external_access) {\n-\t\tthrow PermissionException(\"Installing extensions is disabled through configuration\");\n-\t}\n \n \tauto extension_name = ApplyExtensionAlias(fs.ExtractBaseName(extension));\n \tstring local_extension_path = fs.JoinPath(local_path, extension_name + \".duckdb_extension\");\n---\ntest/sql/extensions/allowed_directories_install.test\n@@ -0,0 +1,28 @@\n+# name: test/sql/extensions/allowed_directories_install.test\n+# description: Test extension installation with allowed_directories setting\n+# group: [extensions]\n+\n+statement ok\n+set extension_directory='__TEST_DIR__/extension_dir'\n+\n+statement ok\n+SET allowed_directories=['__TEST_DIR__', 'http://', 'https://'];\n+\n+statement ok\n+SET enable_external_access=false;\n+\n+# error messsage `Failed to download extension` means duckdb\n+statement error\n+INSTALL bogus FROM 'http://not.existent';\n+----\n+Failed to download extension\n+\n+# now switch the extension directory to one that is not in the allowed_directories list\n+statement ok\n+set extension_directory='/tmp'\n+\n+# duckdb will throw permission error because /tmp is not allowed\n+statement error\n+INSTALL bogus FROM 'http://not.existent';\n+----\n+Permission Error: Cannot access directory\n\\ No newline at end of file\n---\n"}], "ideal": "Title: Allow installing extensions with external access allowlist\n\nBody: https://github.com/duckdb/duckdb/pull/14568 added a mechanism to add certain files and dirs to an allowlist to allow some IO even when `enable_external_access` is false.\r\n\r\nThis PR aims to extend that to also allow installing extensions.\r\n\r\nWith this PR you can:\r\n```SQL\r\nSET extension_directory= 'my_extension_dir';\r\nSET allowed_directories=['my_extension_dir', 'http://', 'https://'];\r\nSET enable_external_access=false;\r\ninstall my_ext;\r\n```\r\n\r\nNote that when using absolute paths EITHER the extension directory needs to exists already OR the allowed directories should contain the path to the parent dir of the extension dir. This is because to create the extension directory, `FileSystem::DirectoryExists` needs to be called on the directory in which the extension directory will sit. So for example:\r\n\r\n```SET extension_directory= '/some/path/to/extensions';```\r\nwhere `/some/path/to/extensions` does not exists yet will only work with:\r\n```\r\nSET allowed_directories=['/some/path/to', 'http://', 'https://'];\r\n```"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/optimizer/filter_combiner.hpp\n@@ -53,7 +53,8 @@ class FilterCombiner {\n \n \tvoid GenerateFilters(const std::function<void(unique_ptr<Expression> filter)> &callback);\n \tbool HasFilters();\n-\tTableFilterSet GenerateTableScanFilters(const LogicalGet &get, vector<FilterPushdownResult> &pushdown_results);\n+\tTableFilterSet GenerateTableScanFilters(const vector<ColumnIndex> &column_ids,\n+\t                                        vector<FilterPushdownResult> &pushdown_results);\n \n \tFilterPushdownResult TryPushdownGenericExpression(LogicalGet &get, Expression &expr);\n \n@@ -68,8 +69,8 @@ class FilterCombiner {\n \n \tFilterPushdownResult TryPushdownConstantFilter(TableFilterSet &table_filters, const vector<ColumnIndex> &column_ids,\n \t                                               column_t column_id, vector<ExpressionValueInformation> &info_list);\n-\tFilterPushdownResult TryPushdownExpression(const LogicalGet &get, TableFilterSet &table_filters,\n-\t                                           const vector<ColumnIndex> &column_ids, Expression &expr);\n+\tFilterPushdownResult TryPushdownExpression(TableFilterSet &table_filters, const vector<ColumnIndex> &column_ids,\n+\t                                           Expression &expr);\n \tFilterPushdownResult TryPushdownPrefixFilter(TableFilterSet &table_filters, const vector<ColumnIndex> &column_ids,\n \t                                             Expression &expr);\n \tFilterPushdownResult TryPushdownLikeFilter(TableFilterSet &table_filters, const vector<ColumnIndex> &column_ids,\n---\nsrc/optimizer/filter_combiner.cpp\n@@ -603,7 +603,7 @@ FilterPushdownResult FilterCombiner::TryPushdownOrClause(TableFilterSet &table_f\n \treturn FilterPushdownResult::PUSHED_DOWN_PARTIALLY;\n }\n \n-FilterPushdownResult FilterCombiner::TryPushdownExpression(const LogicalGet &get, TableFilterSet &table_filters,\n+FilterPushdownResult FilterCombiner::TryPushdownExpression(TableFilterSet &table_filters,\n                                                            const vector<ColumnIndex> &column_ids, Expression &expr) {\n \tauto pushdown_result = TryPushdownPrefixFilter(table_filters, column_ids, expr);\n \tif (pushdown_result != FilterPushdownResult::NO_PUSHDOWN) {\n@@ -624,10 +624,9 @@ FilterPushdownResult FilterCombiner::TryPushdownExpression(const LogicalGet &get\n \treturn FilterPushdownResult::NO_PUSHDOWN;\n }\n \n-TableFilterSet FilterCombiner::GenerateTableScanFilters(const LogicalGet &get,\n+TableFilterSet FilterCombiner::GenerateTableScanFilters(const vector<ColumnIndex> &column_ids,\n                                                         vector<FilterPushdownResult> &pushdown_results) {\n \tTableFilterSet table_filters;\n-\tauto &column_ids = get.GetColumnIds();\n \t//! First, we figure the filters that have constant expressions that we can push down to the table scan\n \tfor (auto &constant_value : constant_values) {\n \t\tauto expr_id = constant_value.first;\n@@ -637,7 +636,7 @@ TableFilterSet FilterCombiner::GenerateTableScanFilters(const LogicalGet &get,\n \t//! Here we look for LIKE or IN filters\n \tfor (idx_t rem_fil_idx = 0; rem_fil_idx < remaining_filters.size(); rem_fil_idx++) {\n \t\tauto &remaining_filter = remaining_filters[rem_fil_idx];\n-\t\tauto pushdown_result = TryPushdownExpression(get, table_filters, column_ids, *remaining_filter);\n+\t\tauto pushdown_result = TryPushdownExpression(table_filters, column_ids, *remaining_filter);\n \t\tif (pushdown_result == FilterPushdownResult::PUSHED_DOWN_FULLY) {\n \t\t\t// the filter has been pushed down entirely - we can prune it\n \t\t\tremaining_filters.erase_at(rem_fil_idx--);\n---\nsrc/optimizer/pushdown/pushdown_get.cpp\n@@ -52,7 +52,7 @@ unique_ptr<LogicalOperator> FilterPushdown::PushdownGet(unique_ptr<LogicalOperat\n \n \t//! We generate the table filters that will be executed during the table scan\n \tvector<FilterPushdownResult> pushdown_results;\n-\tget.table_filters = combiner.GenerateTableScanFilters(get, pushdown_results);\n+\tget.table_filters = combiner.GenerateTableScanFilters(get.GetColumnIds(), pushdown_results);\n \n \tGenerateFilters();\n \n---\n"}], "ideal": "Title: take the column ids from the logical get, don't require a LogicalGet \u2026\n\nBody: This PR is a follow up to #17213 \r\n\r\nThe prototype of the `GenerateTableScanFilters` changed in a way that made it too restrictive to be adapted to by users of the old version.\r\n\r\nIn both Delta and Iceberg we rely on this method for the `ComplexFilterPushdown`\r\n(I've also removed the `LogicalGet &` from the `TryPushdownExpression` as it was not used entirely there)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "third_party/mbedtls/include/mbedtls_wrapper.hpp\n@@ -70,6 +70,7 @@ class AESStateMBEDTLS : public duckdb::EncryptionState {\n \t\t                          duckdb::idx_t out_len) override;\n \t\tDUCKDB_API size_t Finalize(duckdb::data_ptr_t out, duckdb::idx_t out_len, duckdb::data_ptr_t tag, duckdb::idx_t tag_len) override;\n \n+\t\tDUCKDB_API static void GenerateRandomDataStatic(duckdb::data_ptr_t data, duckdb::idx_t len);\n \t\tDUCKDB_API void GenerateRandomData(duckdb::data_ptr_t data, duckdb::idx_t len) override;\n \t\tDUCKDB_API void FinalizeGCM(duckdb::data_ptr_t tag, duckdb::idx_t tag_len);\n \t\tDUCKDB_API const mbedtls_cipher_info_t *GetCipher(size_t key_len);\n---\nthird_party/mbedtls/mbedtls_wrapper.cpp\n@@ -255,7 +255,7 @@ MbedTlsWrapper::AESStateMBEDTLS::~AESStateMBEDTLS() {\n \t}\n }\n \n-void MbedTlsWrapper::AESStateMBEDTLS::GenerateRandomData(duckdb::data_ptr_t data, duckdb::idx_t len) {\n+void MbedTlsWrapper::AESStateMBEDTLS::GenerateRandomDataStatic(duckdb::data_ptr_t data, duckdb::idx_t len) {\n \tduckdb::RandomEngine random_engine(duckdb::Timestamp::GetCurrentTimestamp().value);\n \twhile (len != 0) {\n \t\tconst auto random_integer = random_engine.NextRandomInteger();\n@@ -266,6 +266,11 @@ void MbedTlsWrapper::AESStateMBEDTLS::GenerateRandomData(duckdb::data_ptr_t data\n \t}\n }\n \n+\n+void MbedTlsWrapper::AESStateMBEDTLS::GenerateRandomData(duckdb::data_ptr_t data, duckdb::idx_t len) {\n+\tGenerateRandomDataStatic(data, len);\n+}\n+\n void MbedTlsWrapper::AESStateMBEDTLS::InitializeEncryption(duckdb::const_data_ptr_t iv, duckdb::idx_t iv_len, const std::string *key) {\n \tmode = ENCRYPT;\n \n---\nthird_party/thrift/thrift/transport/TTransport.h\n@@ -23,6 +23,7 @@\n #include \"thrift/Thrift.h\"\n #include \"thrift/transport/TTransportException.h\"\n #include <memory>\n+#include <cstdint>\n #include <string>\n \n namespace duckdb_apache {\n---\n"}], "ideal": "Title: Generating random data for mbedtls without key\n\nBody: For generating random data with mbedtls we don't actually need to initialise an AES state, which means we also do not need a key. However, now, we require to initialise an encryption state, and if no proper key is given this throws an error.\r\n\r\nThis PR fixes this behaviour by making a static generate random data function that can be called independently of the mbedtls AES state."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/icu/icu-dateadd.cpp\n@@ -12,6 +12,17 @@\n \n namespace duckdb {\n \n+static duckdb::unique_ptr<FunctionData> ICUBindIntervalMonths(ClientContext &context, ScalarFunction &bound_function,\n+                                                              vector<duckdb::unique_ptr<Expression>> &arguments) {\n+\tauto result = ICUDateFunc::Bind(context, bound_function, arguments);\n+\tauto &info = result->Cast<ICUDateFunc::BindData>();\n+\tTZCalendar calendar(*info.calendar, info.cal_setting);\n+\tif (!calendar.SupportsIntervals()) {\n+\t\tthrow NotImplementedException(\"INTERVALs do not work with 13 month calendars. Try using DATE_DIFF instead.\");\n+\t}\n+\treturn std::move(result);\n+}\n+\n struct ICUCalendarAdd {\n \ttemplate <class TA, class TB, class TR>\n \tstatic inline TR Operation(TA left, TB right, TZCalendar &calendar_p) {\n@@ -228,7 +239,7 @@ struct ICUDateAdd : public ICUDateFunc {\n \ttemplate <typename TA, typename TR, typename OP>\n \tinline static ScalarFunction GetUnaryDateFunction(const LogicalTypeId &left_type,\n \t                                                  const LogicalTypeId &result_type) {\n-\t\treturn ScalarFunction({left_type}, result_type, ExecuteUnary<TA, TR, OP>, Bind);\n+\t\treturn ScalarFunction({left_type}, result_type, ExecuteUnary<TA, TR, OP>, ICUBindIntervalMonths);\n \t}\n \n \ttemplate <typename TA, typename TB, typename TR, typename OP>\n@@ -247,7 +258,8 @@ struct ICUDateAdd : public ICUDateFunc {\n \ttemplate <typename TA, typename TB, typename TR, typename OP>\n \tinline static ScalarFunction GetBinaryDateFunction(const LogicalTypeId &left_type, const LogicalTypeId &right_type,\n \t                                                   const LogicalTypeId &result_type) {\n-\t\treturn ScalarFunction({left_type, right_type}, result_type, ExecuteBinary<TA, TB, TR, OP>, Bind);\n+\t\treturn ScalarFunction({left_type, right_type}, result_type, ExecuteBinary<TA, TB, TR, OP>,\n+\t\t                      ICUBindIntervalMonths);\n \t}\n \n \ttemplate <typename TA, typename TB, typename OP>\n---\nextension/icu/include/icu-datefunc.hpp\n@@ -12,7 +12,6 @@\n \n #include \"duckdb/common/enums/date_part_specifier.hpp\"\n #include \"duckdb/planner/expression/bound_function_expression.hpp\"\n-#include \"unicode/calendar.h\"\n #include \"tz_calendar.hpp\"\n \n namespace duckdb {\n---\nextension/icu/include/tz_calendar.hpp\n@@ -8,7 +8,7 @@\n \n #pragma once\n \n-#include \"icu-datefunc.hpp\"\n+#include \"unicode/calendar.h\"\n #include \"duckdb/common/string_util.hpp\"\n \n namespace duckdb {\n@@ -18,7 +18,8 @@ using CalendarPtr = duckdb::unique_ptr<icu::Calendar>;\n struct TZCalendar {\n \tTZCalendar(icu::Calendar &calendar_p, const string &cal_setting)\n \t    : calendar(CalendarPtr(calendar_p.clone())),\n-\t      is_gregorian(cal_setting.empty() || StringUtil::CIEquals(cal_setting, \"gregorian\")) {\n+\t      is_gregorian(cal_setting.empty() || StringUtil::CIEquals(cal_setting, \"gregorian\")),\n+\t      supports_intervals(calendar->getMaximum(UCAL_MONTH) < 12) { // 0-based\n \t}\n \n \ticu::Calendar *GetICUCalendar() {\n@@ -27,9 +28,13 @@ struct TZCalendar {\n \tbool IsGregorian() const {\n \t\treturn is_gregorian;\n \t}\n+\tbool SupportsIntervals() const {\n+\t\treturn supports_intervals;\n+\t}\n \n \tCalendarPtr calendar;\n-\tbool is_gregorian;\n+\tconst bool is_gregorian;\n+\tconst bool supports_intervals;\n };\n \n } // namespace duckdb\n---\ntest/sql/timezone/icu_calendar_operations.test\n@@ -6,22 +6,35 @@ require icu\n \n # Normalise the testing locale\n statement ok\n-SET Calendar = 'hebrew';\n+SET TimeZone = 'UTC';\n+\n+# Test intervals for lunar calendars\n+foreach cal hebrew coptic ethiopic\n \n statement ok\n-SET TimeZone = 'UTC';\n+SET Calendar = '${cal}';\n \n-query I\n-SELECT AGE(timestamptz '2020-01-01' + interval (30) day, timestamptz '2020-01-01')\n+statement error\n+select age('2020-01-01'::timestamptz);\n ----\n-1 month\n+INTERVALs do not work with 13 month calendars. Try using DATE_DIFF instead.\n \n-query I\n-SELECT AGE(timestamptz '2020-06-01' + interval (230) day, timestamptz '2020-06-01')\n+statement error\n+select '2020-01-01'::timestamptz + interval '380 days'\n ----\n-7 months 23 days\n+INTERVALs do not work with 13 month calendars. Try using DATE_DIFF instead.\n \n-query I\n-SELECT AGE(timestamptz '2021-06-01', timestamptz '2020-06-01' + interval (230) day)\n+statement error\n+select '2021-01-01'::timestamptz - interval '380 days'\n ----\n-5 months 7 days\n+INTERVALs do not work with 13 month calendars. Try using DATE_DIFF instead.\n+\n+statement error\n+select \n+\t'2020-01-01'::timestamptz as start_date,\n+\t'2021-01-01'::timestamptz as end_date,\n+\tage(end_date, start_date) as delta;\n+----\n+INTERVALs do not work with 13 month calendars. Try using DATE_DIFF instead.\n+\n+endloop\n---\n"}], "ideal": "Title: Internal #4532: 13 Month Intervals \n\nBody: * Disable interval arithmetic for 13 month calendars.\r\n\r\nfixes: duckdblabs/duckdb-internal#4532"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/operator/cast_operators.cpp\n@@ -2316,7 +2316,8 @@ bool DoubleToDecimalCast(SRC input, DST &result, CastParameters &parameters, uin\n \t\tHandleCastError::AssignError(error, parameters);\n \t\treturn false;\n \t}\n-\tresult = Cast::Operation<SRC, DST>(static_cast<SRC>(value));\n+\t// For some reason PG does not use statistical rounding here (even though it _does_ for integers...)\n+\tresult = Cast::Operation<SRC, DST>(static_cast<SRC>(roundedValue));\n \treturn true;\n }\n \n---\ntest/sql/cast/float_decimal_cast.test\n@@ -0,0 +1,17 @@\n+# name: test/sql/cast/float_decimal_cast.test\n+# description: Rounding half up in float => decimal casts\n+# group: [cast]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+# PG does NOT use statistical (\"Banker's\") rounding for floating point => decimal\n+\n+foreach src FLOAT DOUBLE\n+\n+query II\n+select 1.35::${src}::decimal(3, 1), 1.45::${src}::decimal(3, 1)\n+----\n+1.4\t1.5\n+\n+endloop\n---\ntest/sql/join/iejoin/iejoin_projection_maps.test\n@@ -27,7 +27,7 @@ query IIIIII\n SELECT SUM(id) AS id, SUM(id2) AS id2, SUM(id3) AS id3, SUM(value) AS sum_value, SUM(one_min_value) AS sum_one_min_value, sum_value + sum_one_min_value AS sum\n FROM df\n ----\n-252652\t29774\t17657\t2497.951\t2502.045\t4999.996\n+252652\t29774\t17657\t2498.192\t2502.191\t5000.383\n \n statement ok\n PRAGMA enable_verification\n@@ -38,6 +38,8 @@ foreach prefer False True\n statement ok\n PRAGMA prefer_range_joins=${prefer};\n \n+# mode output_hash\n+\n query I\n SELECT id2, id3, id3_right, sum(value * value_right) as value\n FROM (\n@@ -51,7 +53,7 @@ FROM (\n GROUP BY ALL\n ORDER BY ALL\n ----\n-660 values hashing to 787fff9f7e1d4749631eac7194d6ed44\n+660 values hashing to fe2237dbeb18fe3400d5323bcab26dd2\n \n endloop\n \n---\n"}], "ideal": "Title: Issue #16563: FLOAT to DECIMAL\n\nBody: * Use half up rounding instead of statistical rounding for these casts\r\n* Our double arithmetic for integral conversions still has some warts when values get large..."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/table/table_scan.cpp\n@@ -404,14 +404,20 @@ void ExtractConjunctionAnd(ConjunctionAndFilter &filter, BoundColumnRefExpressio\n \tvector<reference<InFilter>> in_filters;\n \n \tfor (idx_t i = 0; i < filter.child_filters.size(); i++) {\n-\t\tif (filter.child_filters[i]->filter_type == TableFilterType::CONSTANT_COMPARISON) {\n-\t\t\tauto &comparison = filter.child_filters[i]->Cast<ConstantFilter>();\n+\t\tauto &child_filter = *filter.child_filters[i];\n+\t\tswitch (child_filter.filter_type) {\n+\t\tcase TableFilterType::CONSTANT_COMPARISON: {\n+\t\t\tauto &comparison = child_filter.Cast<ConstantFilter>();\n \t\t\tcomparisons.push_back(comparison);\n-\t\t\tcontinue;\n+\t\t\tbreak;\n \t\t}\n-\n-\t\tif (filter.child_filters[i]->filter_type == TableFilterType::OPTIONAL_FILTER) {\n-\t\t\tauto &optional_filter = filter.child_filters[i]->Cast<OptionalFilter>();\n+\t\tcase TableFilterType::CONJUNCTION_AND: {\n+\t\t\tauto &conjunction = child_filter.Cast<ConjunctionAndFilter>();\n+\t\t\tExtractConjunctionAnd(conjunction, bound_ref, expressions);\n+\t\t\tbreak;\n+\t\t}\n+\t\tcase TableFilterType::OPTIONAL_FILTER: {\n+\t\t\tauto &optional_filter = child_filter.Cast<OptionalFilter>();\n \t\t\tif (!optional_filter.child_filter) {\n \t\t\t\treturn;\n \t\t\t}\n@@ -421,11 +427,13 @@ void ExtractConjunctionAnd(ConjunctionAndFilter &filter, BoundColumnRefExpressio\n \t\t\t}\n \t\t\tauto &in_filter = optional_filter.child_filter->Cast<InFilter>();\n \t\t\tin_filters.push_back(in_filter);\n-\t\t\tcontinue;\n+\t\t\tbreak;\n+\t\t}\n+\t\tdefault:\n+\t\t\t// Not yet supported: filter types than CONSTANT_COMPARISON/IN_FILTER/CONJUNCTION_AND in CONJUNCTION_AND.\n+\t\t\texpressions.clear();\n+\t\t\treturn;\n \t\t}\n-\n-\t\t// No support for other filter types than CONSTANT_COMPARISON and IN_FILTER in CONJUNCTION_AND yet.\n-\t\treturn;\n \t}\n \n \t// No support for other CONJUNCTION_AND cases yet.\n---\ntest/sql/index/art/issues/test_art_internal_issue_4742.test\n@@ -0,0 +1,70 @@\n+# name: test/sql/index/art/issues/test_art_internal_issue_4742.test\n+# description: Test that Index Scan is triggered even when there is a nested conjunction filter on the index column\n+# group: [issues]\n+\n+# needs this vector size otherwise the sample is somehow not repeatable\n+require vector_size 2048\n+\n+statement ok\n+create or replace table test as select 9223372036854776 + range * 9223372036854776 i from range(100);\n+\n+statement ok\n+create index my_index on test(i);\n+\n+# this already worked properly, uses an index scan\n+query II\n+explain analyze select i from test SEMI JOIN (select i from test using sample reservoir(10) repeatable (42)) USING (i);\n+----\n+analyzed_plan\t<REGEX>:.*Index Scan.*\n+\n+# should return exactly 10 values, as we join with a sample of size 10 of itself\n+query I\n+select count(*) from test SEMI JOIN (select i from test using sample reservoir(10) repeatable (42)) USING (i);\n+----\n+10\n+\n+# given the seed, the return values should be stable\n+query I\n+select i from test SEMI JOIN (select i from test using sample reservoir(10) repeatable (42)) USING (i) order by all;\n+----\n+36893488147419104\n+138350580552821640\n+184467440737095520\n+249031044995078952\n+295147905179352832\n+433498485732174472\n+682529530727253424\n+700976274800962976\n+839326855353784616\n+903890459611768048\n+\n+# this did not work properly before the fix: doing the same join with a table\n+statement ok\n+create or replace table sample as select i from test using sample reservoir(10) repeatable (42);\n+\n+# should also use an index scan now\n+query II\n+explain analyze select i from test SEMI JOIN sample USING (i);\n+----\n+analyzed_plan\t<REGEX>:.*Index Scan.*\n+\n+# again should be 10 values\n+query I\n+select count(*) from test SEMI JOIN sample USING (i);\n+----\n+10\n+\n+# values should be exactly the same as before\n+query I\n+select i from test SEMI JOIN sample USING (i) order by all;\n+----\n+36893488147419104\n+138350580552821640\n+184467440737095520\n+249031044995078952\n+295147905179352832\n+433498485732174472\n+682529530727253424\n+700976274800962976\n+839326855353784616\n+903890459611768048\n---\n"}], "ideal": "Title: Extract expressions from nested conjunction AND for index scan\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/4742\r\n\r\nWe did not recurse into nested conjunction `AND` expressions, causing us to bail on using an Index Scan, even though this would be much faster than the Sequential Scan."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/icu/icu-timezone.cpp\n@@ -165,8 +165,7 @@ struct ICUFromNaiveTimestamp : public ICUDateFunc {\n \t\t}\n \t\tif (input.context->config.disable_timestamptz_casts) {\n \t\t\tthrow BinderException(\"Casting from TIMESTAMP to TIMESTAMP WITH TIME ZONE without an explicit time zone \"\n-\t\t\t                      \"has been disabled  - use \\\"AT TIME ZONE ...\\\"\",\n-\t\t\t                      LogicalTypeIdToString(source.id()));\n+\t\t\t                      \"has been disabled  - use \\\"AT TIME ZONE ...\\\"\");\n \t\t}\n \n \t\tauto cast_data = make_uniq<CastData>(make_uniq<BindData>(*input.context));\n---\n"}], "ideal": "Title: Internal #4772: Timestamp Error Parameter\n\nBody: * Remove bogus extra parameter in the exception text.\r\n\r\nfixes: duckdblabs/duckdb-internal#4772"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/operator/logical_get.cpp\n@@ -10,6 +10,7 @@\n #include \"duckdb/common/serializer/serializer.hpp\"\n #include \"duckdb/common/serializer/deserializer.hpp\"\n #include \"duckdb/parser/tableref/table_function_ref.hpp\"\n+#include \"duckdb/common/multi_file/multi_file_reader.hpp\"\n \n namespace duckdb {\n \n@@ -152,6 +153,11 @@ column_t LogicalGet::GetAnyColumn() const {\n \t\t// return the rowid column if the projection supports it\n \t\treturn COLUMN_IDENTIFIER_ROW_ID;\n \t}\n+\tentry = virtual_columns.find(MultiFileReader::COLUMN_IDENTIFIER_FILE_ROW_NUMBER);\n+\tif (entry != virtual_columns.end()) {\n+\t\t// return the file row number column if the projection supports it\n+\t\treturn MultiFileReader::COLUMN_IDENTIFIER_FILE_ROW_NUMBER;\n+\t}\n \t// otherwise return the first column\n \treturn 0;\n }\n---\n"}], "ideal": "Title: Let `LogicalGet::GetAnyColumn` get the file row number\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/4665"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "test/sql/aggregate/aggregates/histogram_table_function.test\n@@ -28,7 +28,7 @@ SELECT * FROM histogram_values(integers, i, bin_count := 2)\n statement error\n SELECT * FROM histogram_values(integers, k)\n ----\n-not found in FROM clause\n+<REGEX>:.*Binder Error.*not found in FROM clause.*\n \n statement ok\n INSERT INTO integers FROM range(127)\n@@ -84,13 +84,13 @@ SELECT * FROM histogram_values(integers, (i%2)::VARCHAR)\n 1\t64\n \n # FIXME: there are some minor rounding problems on ARM64 with the below tests\n-mode skip\n+# mode skip\n \n # varchar does not work with equi-width-bins\n statement error\n SELECT * FROM histogram_values(integers, (i%2)::VARCHAR, technique := 'equi-width')\n ----\n-Unsupported\n+<REGEX>:.*Binder Error.*Unsupported type \"VARCHAR\" for equi_width_bins.*\n \n # but it works with equi-height\n query II\n@@ -100,19 +100,9 @@ SELECT COUNT(*), AVG(count)  FROM histogram_values(integers, i::VARCHAR, techniq\n \n # histogram with ranges\n query II\n-SELECT bin, count FROM histogram(integers, i::VARCHAR, technique := 'equi-height')\n+SELECT COUNT(bin), AVG(count) FROM histogram(integers, i::VARCHAR, technique := 'equi-height')\n ----\n-x <= 109\t13\n-109 < x <= 120\t13\n-120 < x <= 19\t13\n-19 < x <= 30\t13\n-30 < x <= 42\t14\n-42 < x <= 53\t12\n-53 < x <= 65\t13\n-65 < x <= 77\t13\n-77 < x <= 88\t13\n-88 < x <= 99999999\t13\n-\n+10\t13\n \n # booleans\n statement ok\n---\n"}], "ideal": "Title: Histogram table function test\n\nBody: This PR should fix the issue https://github.com/duckdblabs/duckdb-internal/issues/4199\r\n\r\nTested [here](https://github.com/hmeriann/duckdb/actions/runs/14665571107)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/Windows.yml\n@@ -254,7 +254,8 @@ jobs:\n  mingw:\n      name: MinGW (64 Bit)\n      runs-on: windows-2019\n-     if: ${{ inputs.skip_tests != 'true' }}\n+     #FIXME: add this back -- if: ${{ inputs.skip_tests != 'true' }}\n+     if: false\n      needs: win-release-64\n      steps:\n        - uses: actions/checkout@v4\n---\nsrc/parser/transform/expression/transform_interval.cpp\n@@ -148,7 +148,9 @@ unique_ptr<ParsedExpression> Transformer::TransformInterval(duckdb_libpgquery::P\n \t// now push the operation\n \tvector<unique_ptr<ParsedExpression>> children;\n \tchildren.push_back(std::move(expr));\n-\treturn make_uniq<FunctionExpression>(fname, std::move(children));\n+\tauto result = make_uniq<FunctionExpression>(fname, std::move(children));\n+\tSetQueryLocation(*result, node.location);\n+\treturn std::move(result);\n }\n \n } // namespace duckdb\n---\n"}], "ideal": "Title: Add root's query_location also to TransformInterval\n\nBody: Minor fix that allows propagating interval query_location information.\r\n\r\nThis makes that:\r\n```sql\r\nSELECT location, count(*)\r\nFROM (\r\nSELECT location, count(*)\r\nFROM (\r\n    SELECT try_cast(\r\n         string_split(string_split(unnest(string_split(json_serialize_sql('select interval 1 day')::VARCHAR, 'query_location\":')), ',')[1],'}')[1]\r\n         as UINT64\r\n    ) as location OFFSET 1\r\n) GROUP BY location, ORDER by location;\r\n```\r\nreturns now\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502       location       \u2502 count_star() \u2502\r\n\u2502        uint64        \u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                   16 \u2502            1 \u2502\r\n\u2502 18446744073709551615 \u2502            5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAlso disable MinGW CI, it has been failing for a while.\r\nPython failure is failing also without this PR, to be addressed separately."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/window/window_value_function.cpp\n@@ -293,15 +293,20 @@ void WindowLeadLagExecutor::EvaluateInternal(WindowExecutorGlobalState &gstate,\n \t\tframes.resize(1);\n \t\tauto &frame = frames[0];\n \t\tfor (idx_t i = 0; i < count; ++i, ++row_idx) {\n+\t\t\tint64_t offset = 1;\n+\t\t\tif (wexpr.offset_expr) {\n+\t\t\t\tif (leadlag_offset.CellIsNull(i)) {\n+\t\t\t\t\tFlatVector::SetNull(result, i, true);\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\toffset = leadlag_offset.GetCell<int64_t>(i);\n+\t\t\t}\n+\n \t\t\t// (1) compute the ROW_NUMBER of the own row\n \t\t\tframe = FrameBounds(frame_begin[i], frame_end[i]);\n \t\t\tconst auto own_row = glstate.row_tree->Rank(frame.start, frame.end, row_idx) - 1;\n \t\t\t// (2) adjust the row number by adding or subtracting an offset\n \t\t\tauto val_idx = NumericCast<int64_t>(own_row);\n-\t\t\tint64_t offset = 1;\n-\t\t\tif (wexpr.offset_expr) {\n-\t\t\t\toffset = leadlag_offset.GetCell<int64_t>(i);\n-\t\t\t}\n \t\t\tif (wexpr.GetExpressionType() == ExpressionType::WINDOW_LEAD) {\n \t\t\t\tval_idx = AddOperatorOverflowCheck::Operation<int64_t, int64_t, int64_t>(val_idx, offset);\n \t\t\t} else {\n@@ -344,6 +349,12 @@ void WindowLeadLagExecutor::EvaluateInternal(WindowExecutorGlobalState &gstate,\n \tfor (idx_t i = 0; i < count;) {\n \t\tint64_t offset = 1;\n \t\tif (wexpr.offset_expr) {\n+\t\t\tif (leadlag_offset.CellIsNull(i)) {\n+\t\t\t\tFlatVector::SetNull(result, i, true);\n+\t\t\t\t++i;\n+\t\t\t\t++row_idx;\n+\t\t\t\tcontinue;\n+\t\t\t}\n \t\t\toffset = leadlag_offset.GetCell<int64_t>(i);\n \t\t}\n \t\tint64_t val_idx = (int64_t)row_idx;\n---\ntest/sql/window/test_lead_lag.test\n@@ -105,3 +105,33 @@ ORDER BY 2, 1\n 2024-01-01\t7\t1000\tlate\t2024-03-01\tdispatched\n 2024-02-01\t7\t1000\tdispatched\tNULL\tNULL\n 2024-03-01\t7\t1000\tdispatched\tNULL\tNULL\n+\n+# test lad and lead when offset is null\n+statement ok\n+CREATE TABLE issue17266(c1 INT, c2 SMALLINT, c3 BITSTRING);\n+\n+statement ok\n+INSERT INTO issue17266 VALUES\n+(0, null, null),\n+(1, 32767, '101'),\n+(2, -32767, '101'),\n+(3, 0, '000'),\n+(4, null, null);\n+\n+query IIII\n+SELECT c1, c3, c2, LAG(c3, c2, BITSTRING'010101010') OVER (PARTITION BY c1 ORDER BY c3) FROM issue17266 ORDER BY c1;\n+----\n+0\tNULL\tNULL\tNULL\n+1\t101\t32767\t010101010\n+2\t101\t-32767\t010101010\n+3\t000\t0\t000\n+4\tNULL\tNULL\tNULL\n+\n+query IIII\n+SELECT c1, c3, c2, LEAD(c3, c2, BITSTRING'010101010') OVER (PARTITION BY c1 ORDER BY c3) FROM issue17266 ORDER BY c1;\n+----\n+0\tNULL\tNULL\tNULL\n+1\t101\t32767\t010101010\n+2\t101\t-32767\t010101010\n+3\t000\t0\t000\n+4\tNULL\tNULL\tNULL\n\\ No newline at end of file\n---\ntest/sql/window/test_leadlag_orderby.test\n@@ -28,3 +28,51 @@ ORDER BY inside DESC, i\n 3\t10\t1\t0\t2\n 0\t0\t0\tNULL\tNULL\n 1\t7\t0\tNULL\t0\n+\n+# test lad and lead when offset is null\n+statement ok\n+CREATE TABLE issue17266(c1 INT, c2 SMALLINT, c3 BITSTRING);\n+\n+statement ok\n+INSERT INTO issue17266 VALUES\n+(0, null, null),\n+(1, 32767, '101'),\n+(2, -32767, '101'),\n+(3, 0, '000'),\n+(4, 1, '010'),\n+(5, 0, '110'),\n+(6, null, null);\n+\n+query IIII\n+SELECT\n+\tc1,\n+        c3,\n+        c2,\n+        LAG(c3, c2 ORDER BY c1, BITSTRING'010101010') OVER (PARTITION BY c1 ORDER BY c3)\n+FROM issue17266\n+ORDER BY c1;\n+----\n+0\tNULL\tNULL\tNULL\n+1\t101\t32767\tNULL\n+2\t101\t-32767\tNULL\n+3\t000\t0\t000\n+4\t010\t1\tNULL\n+5\t110\t0\t110\n+6\tNULL\tNULL\tNULL\n+\n+query IIII\n+SELECT\n+\tc1,\n+        c3,\n+        c2,\n+        LEAD(c3, c2 ORDER BY c1, BITSTRING'010101010') OVER (PARTITION BY c1 ORDER BY c3)\n+FROM issue17266\n+ORDER BY c1;\n+----\n+0\tNULL\tNULL\tNULL\n+1\t101\t32767\tNULL\n+2\t101\t-32767\tNULL\n+3\t000\t0\t000\n+4\t010\t1\tNULL\n+5\t110\t0\t110\n+6\tNULL\tNULL\tNULL\n---\n"}], "ideal": "Title: Fix #17266\uff1athe result of lad/lead when the offset is null\n\nBody: fix the result of lad/lead when the offset is null\r\ncompatibility with PG"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/physical_plan/plan_set_operation.cpp\n@@ -101,19 +101,6 @@ PhysicalOperator &PhysicalPlanGenerator::CreatePlan(LogicalSetOperation &op) {\n \t\tthrow InternalException(\"Unexpected operator type for set operation\");\n \t}\n \n-\t// if the ALL specifier is not given, we have to ensure distinct results. Hence, push a GROUP BY ALL\n-\tif (!op.setop_all) { // no ALL, use distinct semantics\n-\t\tauto &types = result->GetTypes();\n-\t\tvector<unique_ptr<Expression>> groups, aggregates /* left empty */;\n-\t\tfor (idx_t i = 0; i < types.size(); i++) {\n-\t\t\tgroups.push_back(make_uniq<BoundReferenceExpression>(types[i], i));\n-\t\t}\n-\t\tauto &group_by = Make<PhysicalHashAggregate>(context, op.types, std::move(aggregates), std::move(groups),\n-\t\t                                             result->estimated_cardinality);\n-\t\tgroup_by.children.push_back(*result);\n-\t\tresult = group_by;\n-\t}\n-\n \tD_ASSERT(result);\n \treturn *result;\n }\n---\nsrc/parser/transform/statement/transform_select_node.cpp\n@@ -154,6 +154,11 @@ unique_ptr<QueryNode> Transformer::TransformSelectInternal(duckdb_libpgquery::PG\n \t\tdefault:\n \t\t\tthrow InternalException(\"Unexpected setop type\");\n \t\t}\n+\n+\t\tif (!stmt.all) {\n+\t\t\tresult.modifiers.push_back(make_uniq<DistinctModifier>());\n+\t\t}\n+\n \t\tif (stmt.sampleOptions) {\n \t\t\tthrow ParserException(\"SAMPLE clause is only allowed in regular SELECT statements\");\n \t\t}\n---\ntest/sql/binder/test_duplicate_in_set_operation.test\n@@ -0,0 +1,47 @@\n+# name: test/sql/binder/test_duplicate_set_operation.test\n+# description: Test whether duplicate can be performed correctly in set operation\n+# group: [binder]\n+\n+statement ok\n+create table t1(col1 varchar collate nocase);\n+\n+statement ok\n+create table t2(col1 varchar collate nocase);\n+\n+statement ok\n+insert into t1 values ('a');\n+\n+statement ok\n+insert into t2 values ('A');\n+\n+statement ok\n+pragma enable_verification;\n+\n+query I\n+select upper(col1)\n+from (\n+\tselect *\n+\tfrom t1\n+\tunion\n+\tselect *\n+\tfrom t2\n+) d;\n+----\n+A\n+\n+\n+statement ok\n+pragma explain_output='physical_only';\n+\n+# Ensure that duplicate is performed correctly in the HASH_GROUP_BY operator\n+query II\n+explain select upper(col1)\n+from (\n+\tselect *\n+\tfrom t1\n+\tunion\n+\tselect *\n+\tfrom t2\n+) d;\n+----\n+physical_plan\t<REGEX>:.* Aggregates: .*\\\"first\\\"\\(\\#1\\).*\n\\ No newline at end of file\n---\n"}], "ideal": "Title: Fix #17251: Eliminate duplicates after a set operation using set sema\u2026\n\nBody: Currently, the set operation using set semantics does not consider the impact of collation when eliminating duplicates. This can be fixed by adding a DistinctModifier in the transform process.\r\n\r\nFixes https://github.com/duckdb/duckdb/issues/17251\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/optimizer/remove_unused_columns.cpp\n@@ -56,9 +56,17 @@ void RemoveUnusedColumns::VisitOperator(LogicalOperator &op) {\n \tswitch (op.type) {\n \tcase LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY: {\n \t\t// aggregate\n-\t\tif (!everything_referenced) {\n+\t\tauto &aggr = op.Cast<LogicalAggregate>();\n+\t\t// if there is more than one grouping set, the group by most likely has a rollup or cube\n+\t\t// If there is an equality join underneath the aggregate, this can change the groups to avoid unused columns\n+\t\t// This causes the duplicate eliminator to ignore functionality provided by grouping sets\n+\t\tbool new_root = false;\n+\t\tif (aggr.grouping_sets.size() > 1) {\n+\t\t\t;\n+\t\t\tnew_root = true;\n+\t\t}\n+\t\tif (!everything_referenced && !new_root) {\n \t\t\t// FIXME: groups that are not referenced need to stay -> but they don't need to be scanned and output!\n-\t\t\tauto &aggr = op.Cast<LogicalAggregate>();\n \t\t\tClearUnusedExpressions(aggr.expressions, aggr.aggregate_index);\n \t\t\tif (aggr.expressions.empty() && aggr.groups.empty()) {\n \t\t\t\t// removed all expressions from the aggregate: push a COUNT(*)\n@@ -70,7 +78,7 @@ void RemoveUnusedColumns::VisitOperator(LogicalOperator &op) {\n \t\t}\n \n \t\t// then recurse into the children of the aggregate\n-\t\tRemoveUnusedColumns remove(binder, context);\n+\t\tRemoveUnusedColumns remove(binder, context, new_root);\n \t\tremove.VisitOperatorExpressions(op);\n \t\tremove.VisitOperator(*op.children[0]);\n \t\treturn;\n---\ntest/sql/optimizer/test_duplicate_groups_optimizer.test\n@@ -0,0 +1,121 @@\n+# name: test/sql/optimizer/test_duplicate_groups_optimizer.test\n+# description: Test Duplicate Groups optimizer\n+# group: [optimizer]\n+\n+statement ok\n+create table t1(col1 int, col2 int);\n+\n+statement ok\n+create table t2(col3 int);\n+\n+statement ok\n+insert into t1 values (1, 1);\n+\n+statement ok\n+insert into t2 values (1);\n+\n+statement ok\n+pragma enable_verification;\n+\n+query III\n+select\n+\tcol1,\n+\tcol2,\n+\tcol3\n+from t1\n+\tjoin t2\n+\ton t1.col1 = t2.col3\n+group by rollup(col1, col2, col3) order by 1, 2 ,3;\n+----\n+1\t1\t1\n+1\t1\tNULL\n+1\tNULL\tNULL\n+NULL\tNULL\tNULL\n+\n+\n+query III\n+select\n+\tcol1,\n+\tcol2,\n+\tcol3\n+from t1\n+\tjoin t2\n+\ton t1.col1 = t2.col3\n+group by cube(col1, col2, col3) order by 1, 2 ,3;\n+----\n+1\t1\t1\n+1\t1\tNULL\n+1\tNULL\t1\n+1\tNULL\tNULL\n+NULL\t1\t1\n+NULL\t1\tNULL\n+NULL\tNULL\t1\n+NULL\tNULL\tNULL\n+\n+\n+query III\n+select\n+      col1,\n+      col2,\n+      col3\n+  from t1\n+      join t2\n+      on t1.col1 = t2.col3\n+  group by grouping sets (col1, col2, col3), (col1, col2), (col1) order by 1, 2 ,3;\n+----\n+1\t1\t1\n+1\t1\tNULL\n+1\t1\tNULL\n+\n+statement ok\n+pragma explain_output='optimized_only';\n+\n+statement ok\n+pragma disable_verification;\n+\n+# make sure there is only one group and unused columns/duplicate groups still\n+# works.\n+# if unused columns/duplicate groups combo breaks, each group will be on a separate line\n+query II\n+explain select\n+\tcol1,\n+\tcol3\n+from t1\n+\tjoin t2\n+\ton t1.col1 = t2.col3\n+group by col1, col3;\n+----\n+logical_opt\t<REGEX>:.*Groups: col1.*\n+\n+\n+statement ok\n+create table t3 (a int, b int, c int);\n+\n+statement ok\n+insert into t3 values\n+(1, 1, 1),\n+(1, 2, 2),\n+(1, 1, 1),\n+(1, 2, 1);\n+\n+query III\n+select * from t3 group by cube(a, b, c) order by all;\n+----\n+1\t1\t1\n+1\t1\tNULL\n+1\t2\t1\n+1\t2\t2\n+1\t2\tNULL\n+1\tNULL\t1\n+1\tNULL\t2\n+1\tNULL\tNULL\n+NULL\t1\t1\n+NULL\t1\tNULL\n+NULL\t2\t1\n+NULL\t2\t2\n+NULL\t2\tNULL\n+NULL\tNULL\t1\n+NULL\tNULL\t2\n+NULL\tNULL\tNULL\n+\n+\n---\n"}], "ideal": "Title: BUGFIX: do not perform unused columns optimization in presence of multiple grouping sets\n\nBody: If we remove unused columns in the group by when there are multiple grouping sets (i.e more than 1), the duplicate groups optimizer will then optimize away possible groups created by functions like rollup and cube. \r\n\r\nFixes https://github.com/duckdb/duckdb/issues/17175\r\nFixes https://github.com/duckdblabs/duckdb-internal/issues/4711"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/storage/caching_file_system.cpp\n@@ -245,7 +245,9 @@ BufferHandle CachingFileHandle::TryReadFromCache(data_ptr_t &buffer, idx_t nr_by\n \t\t\tif (result.IsValid()) {\n \t\t\t\treturn result;\n \t\t\t}\n-\t\t\tcontinue;\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unknown CachedFileRangeOverlap\");\n \t\t}\n \t\t++it;\n \t}\n@@ -257,9 +259,6 @@ BufferHandle CachingFileHandle::TryReadFromFileRange(const unique_ptr<StorageLoc\n                                                      CachedFileRange &file_range, data_ptr_t &buffer, idx_t nr_bytes,\n                                                      idx_t location) {\n \tD_ASSERT(file_range.GetOverlap(nr_bytes, location) == CachedFileRangeOverlap::FULL);\n-\tif (file_range.block_handle->IsUnloaded()) {\n-\t\treturn BufferHandle(); // Purely in-memory (for now)\n-\t}\n \tauto result = external_file_cache.GetBufferManager().Pin(file_range.block_handle);\n \tif (result.IsValid()) {\n \t\tbuffer = result.Ptr() + (location - file_range.location);\n@@ -297,6 +296,8 @@ BufferHandle CachingFileHandle::TryInsertFileRange(BufferHandle &pin, data_ptr_t\n \t\t\t// Since we have the write lock here, we can do some cleanup\n \t\t\tit = ranges.erase(it);\n \t\t\tcontinue;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unknown CachedFileRangeOverlap\");\n \t\t}\n \t\tif (break_loop) {\n \t\t\tbreak;\n---\nsrc/storage/standard_buffer_manager.cpp\n@@ -366,6 +366,10 @@ BufferHandle StandardBufferManager::Pin(shared_ptr<BlockHandle> &handle) {\n \t\t\t// now we can actually load the current block\n \t\t\tD_ASSERT(handle->Readers() == 0);\n \t\t\tbuf = handle->Load(std::move(reusable_buffer));\n+\t\t\tif (!buf.IsValid()) {\n+\t\t\t\treservation.Resize(0);\n+\t\t\t\treturn buf; // Buffer was destroyed (e.g., due to DestroyBufferUpon::Eviction)\n+\t\t\t}\n \t\t\tauto &memory_charge = handle->GetMemoryCharge(lock);\n \t\t\tmemory_charge = std::move(reservation);\n \t\t\t// in the case of a variable sized block, the buffer may be smaller than a full block.\n---\n"}], "ideal": "Title: Return invalid `BufferHandle` upon loading a destroyed `BlockHandle`\n\nBody: Blocks can be destroyed at any time if they are flagged with `DestroyBufferUpon::EVICTION` when they are not pinned. If we try to pin them later, they should return an invalid `BufferHandle` instead of running into an `InternalException`.\r\n\r\nI found this myself while playing around when doing some Parquet joins with low memory settings."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "README.md\n@@ -15,7 +15,7 @@\n \n ## DuckDB\n \n-DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/stable/guides/sql_features/friendly_sql).\n+DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/stable/sql/dialect/friendly_sql.html).\n \n DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/stable/clients/cli/overview) and has clients for [Python](https://duckdb.org/docs/stable/clients/python/overview), [R](https://duckdb.org/docs/stable/clients/r), [Java](https://duckdb.org/docs/stable/clients/java), [Wasm](https://duckdb.org/docs/stable/clients/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdb.org/docs/stable/clients/r#duckplyr-dplyr-api).\n \n---\n"}], "ideal": "Title: Update Friendlier SQL link.md\n\nBody: updated the friendlier sql link "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/parquet/column_writer.cpp\n@@ -399,7 +399,7 @@ ParquetColumnSchema ColumnWriter::FillParquetSchema(vector<duckdb_parquet::Schem\n \t\trepeated_element.__isset.num_children = true;\n \t\trepeated_element.__isset.type = false;\n \t\trepeated_element.__isset.repetition_type = true;\n-\t\trepeated_element.name = is_list ? \"list\" : \"array\";\n+\t\trepeated_element.name = \"list\";\n \t\tschemas.push_back(std::move(repeated_element));\n \n \t\tParquetColumnSchema list_column(name, type, max_define, max_repeat, schema_idx, 0);\n@@ -494,7 +494,7 @@ ColumnWriter::CreateWriterRecursive(ClientContext &context, ParquetWriter &write\n \t}\n \tif (type.id() == LogicalTypeId::LIST || type.id() == LogicalTypeId::ARRAY) {\n \t\tauto is_list = type.id() == LogicalTypeId::LIST;\n-\t\tpath_in_schema.push_back(is_list ? \"list\" : \"array\");\n+\t\tpath_in_schema.push_back(\"list\");\n \t\tauto child_writer = CreateWriterRecursive(context, writer, parquet_schemas, schema.children[0], path_in_schema);\n \t\tif (is_list) {\n \t\t\treturn make_uniq<ListColumnWriter>(writer, schema, std::move(path_in_schema), std::move(child_writer),\n---\n"}], "ideal": "Title: Switch to always using list identifier instead of array\n\nBody: Partially resolves #17238 \r\n\r\nAfter this change duckdb arrays are readable from other tools without adding additional nesting and are consistent with the spec. \r\n\r\nThis does not add preserving array size information. In both cases duckdb is able to read it correctly.\r\n\r\n\r\n```python3\r\nimport duckdb\r\nimport polars as pl\r\n\r\nduckdb.sql(\"COPY (SELECT [1, 2, 3]::INTEGER[3] as sample_arr) TO 'duckdb_out.parquet'\")\r\nprint(pl.read_parquet(\"duckdb_out.parquet\"))\r\n```\r\n\r\nOutput prior to change:\r\n```\r\nshape: (1, 1)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 sample_arr      \u2502\r\n\u2502 ---             \u2502\r\n\u2502 list[struct[1]] \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 [{1}, {2}, {3}] \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nOutput after change:\r\n```\r\nshape: (1, 1)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 sample_arr \u2502\r\n\u2502 ---        \u2502\r\n\u2502 list[i32]  \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 [1, 2, 3]  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/main/database_manager.cpp\n@@ -167,16 +167,22 @@ void DatabaseManager::GetDatabaseType(ClientContext &context, AttachInfo &info,\n \t\tDBPathAndType::CheckMagicBytes(fs, info.path, options.db_type);\n \t}\n \n-\t// If we are loading a database type from an extension, then we need to check if that extension is loaded.\n-\tif (!options.db_type.empty()) {\n-\t\tif (!Catalog::TryAutoLoad(context, options.db_type)) {\n-\t\t\t// FIXME: Here it might be preferable to use an AutoLoadOrThrow kind of function\n-\t\t\t// so that either there will be success or a message to throw, and load will be\n-\t\t\t// attempted only once respecting the auto-loading options\n-\t\t\tExtensionHelper::LoadExternalExtension(context, options.db_type);\n-\t\t}\n+\tif (options.db_type.empty()) {\n \t\treturn;\n \t}\n+\n+\tif (config.storage_extensions.find(options.db_type) != config.storage_extensions.end()) {\n+\t\t// If the database type is already registered, we don't need to load it again.\n+\t\treturn;\n+\t}\n+\n+\t// If we are loading a database type from an extension, then we need to check if that extension is loaded.\n+\tif (!Catalog::TryAutoLoad(context, options.db_type)) {\n+\t\t// FIXME: Here it might be preferable to use an AutoLoadOrThrow kind of function\n+\t\t// so that either there will be success or a message to throw, and load will be\n+\t\t// attempted only once respecting the auto-loading options\n+\t\tExtensionHelper::LoadExternalExtension(context, options.db_type);\n+\t}\n }\n \n const string &DatabaseManager::GetDefaultDatabase(ClientContext &context) {\n---\n"}], "ideal": "Title: Dont try to load extension if storage type is already registered\n\nBody: This fixes an issue that occurs when an extension provides a storage type that is not named the same as the extension. \r\n\r\nEven if the extension was loaded, we would still fall into the `ExtensionHelper::LoadExternalExtension` path and try to load an extension with the name of the storage type, throwing an error. Now we first check if a storage type with the target name has already been made available by a previously loaded extension."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/function/scalar/generic/constant_or_null.cpp\n@@ -42,7 +42,10 @@ static void ConstantOrNullFunction(DataChunk &args, ExpressionState &state, Vect\n \t\tcase VectorType::CONSTANT_VECTOR: {\n \t\t\tif (ConstantVector::IsNull(args.data[idx])) {\n \t\t\t\t// input is constant null, return constant null\n-\t\t\t\tresult.Reference(info.value);\n+\t\t\t\tresult.SetVectorType(VectorType::CONSTANT_VECTOR);\n+\t\t\t\tauto &result_mask = ConstantVector::Validity(result);\n+\t\t\t\tauto &input_mask = ConstantVector::Validity(args.data[idx]);\n+\t\t\t\tresult_mask.Initialize(input_mask);\n \t\t\t\tConstantVector::SetNull(result, true);\n \t\t\t\treturn;\n \t\t\t}\n---\ntest/sql/join/inner/not_between_is_null.test\n@@ -0,0 +1,22 @@\n+# name: test/sql/join/inner/not_between_is_null.test\n+# description: Test INNER JOIN with NOT BETWEEN and IS NULL conditions\n+# group: [inner]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE t1(c0 INT);\n+CREATE TABLE t2(c0 INT);\n+\n+statement ok\n+INSERT INTO t1(c0) VALUES (-18), (NULL);\n+\n+statement ok\n+INSERT INTO t2(c0) VALUES (NULL);\n+\n+query II\n+SELECT * FROM t1 INNER JOIN t2 ON ((t1.c0 NOT BETWEEN t2.c0 AND t2.c0) IS NULL);\n+----\n+-18\tNULL\n+NULL\tNULL\n\\ No newline at end of file\n---\n"}], "ideal": "Title: Share null mask with constant null arg vector\n\nBody: Fixes https://github.com/duckdb/duckdb/issues/17217\r\n\r\nWhen the constant_or_null function encounters a constant NULL input, it will set the null mask of result vector for row 0 to invalid.\r\n\r\nIf the null mask is shared with other function arg vector (due to a previous Combine() call), modifying the null mask for row 0 would incorrectly affect the arg vector. This leads to wrong query result.\r\n\r\nShare the null mask with the constant NULL arg vector should fix the issue.\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/NotifyExternalRepositories.yml\n@@ -72,6 +72,7 @@ jobs:\n   notify-jdbc-run:\n     name: Run JDBC Vendor\n     runs-on: ubuntu-latest\n+    if: ${{ inputs.is-success == 'true' }}\n     env:\n       PAT_USER: ${{ secrets.PAT_USERNAME }}\n       PAT_TOKEN: ${{ secrets.PAT_TOKEN }}\n---\n"}], "ideal": "Title: Only notify JDBC when all runs are successful\n\nBody: This change adds the check to JDBC notification workflow that all nightly CI runs are sucessful. The check is added the same way it is used for ODBC notifications.\r\n\r\nThis check is necessary to make sure, that JDBC/ODBC only imports engine sources for the commit for which the whole set of extensions is built and available."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/storage/temporary_file_manager.cpp\n@@ -5,6 +5,7 @@\n #include \"duckdb/parallel/task_scheduler.hpp\"\n #include \"duckdb/storage/buffer/temporary_file_information.hpp\"\n #include \"duckdb/storage/standard_buffer_manager.hpp\"\n+#include \"duckdb/main/database.hpp\"\n #include \"zstd.h\"\n \n namespace duckdb {\n@@ -268,6 +269,9 @@ void TemporaryFileHandle::CreateFileIfNotExists(TemporaryFileLock &) {\n \t}\n \tauto &fs = FileSystem::GetFileSystem(db);\n \tauto open_flags = FileFlags::FILE_FLAGS_READ | FileFlags::FILE_FLAGS_WRITE | FileFlags::FILE_FLAGS_FILE_CREATE;\n+\tif (db.config.options.use_direct_io) {\n+\t\topen_flags |= FileFlags::FILE_FLAGS_DIRECT_IO;\n+\t}\n \thandle = fs.OpenFile(path, open_flags);\n }\n \n---\n"}], "ideal": "Title: Direct IO for temp files\n\nBody: It adds direct IO for temp files (if the flag is set), which is possible since they are all multiples of 4096. We should experiment with this more at some point on high-throughput storage devices."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/InvokeCI.yml\n@@ -70,7 +70,7 @@ jobs:\n       override_git_describe: ${{ inputs.override_git_describe }}\n       git_ref: ${{ inputs.git_ref }}\n       skip_tests: ${{ inputs.skip_tests }}\n-\n+    \n   Wasm:\n     uses: ./.github/workflows/Wasm.yml\n     secrets: inherit\n@@ -89,6 +89,7 @@ jobs:\n \n   prepare-status:\n     runs-on: ubuntu-latest\n+    if: always()\n     needs:\n       - osx\n       - linux-release\n@@ -113,9 +114,9 @@ jobs:\n                 \"${{ needs.R.result }}\" == \"success\" && \\\n                 \"${{ needs.Wasm.result }}\" == \"success\" && \\\n                 \"${{ needs.static-libraries.result }}\" == \"success\" ]]; then\n-            echo \"success='true'\" >> $GITHUB_OUTPUT\n+            echo \"success=true\" >> $GITHUB_OUTPUT\n           else\n-            echo \"success='false'\" >> $GITHUB_OUTPUT\n+            echo \"success=false\" >> $GITHUB_OUTPUT\n           fi\n     \n   notify-external-repos-main:\n---\n.github/workflows/NotifyExternalRepositories.yml\n@@ -72,6 +72,7 @@ jobs:\n   notify-jdbc-run:\n     name: Run JDBC Vendor\n     runs-on: ubuntu-latest\n+    if: ${{ inputs.is-success == 'true' }}\n     env:\n       PAT_USER: ${{ secrets.PAT_USERNAME }}\n       PAT_TOKEN: ${{ secrets.PAT_TOKEN }}\n@@ -94,5 +95,5 @@ jobs:\n         if: ${{ github.repository == 'duckdb/duckdb' }}\n         run: |\n           export URL=https://api.github.com/repos/duckdb/duckdb-build-status/actions/workflows/NightlyBuildsCheck.yml/dispatches\n-          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"branch\": \"${{ inputs.target-branch }}\", \"event\": \"${{ inputs.triggering-event }}\", \"should_publish\": \"${{ inputs.should-publish }}\"}}'\n+          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"event\": \"${{ inputs.triggering-event }}\", \"should_publish\": \"${{ inputs.should-publish }}\"}}'\n           curl -v -XPOST -u \"${PAT_USER}:${PAT_TOKEN}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" $URL --data \"$DATA\"\n---\n"}], "ideal": "Title: Fix [InvokeCI / NotifyExternalRepository] GitHub Actions has encountered an internal error when running your job.\n\nBody: GitHub Actions didn't provide more details in the [error message](https://github.com/duckdb/duckdb/actions/runs/14607342802/job/40992045763), so I ran some test workflows.\r\n\r\nWe are not expecting the `InvokeCI / NotifyExternalRepository` would skip `Run ODBC Vendor` job and fail `Run JDBC Vendor` and `Run Nightly Build Status` jobs.\r\n\r\nThe issue was in the boolean values passed as string inputs to the called `NotifyExternalRepository` workflow, where they were interpreted as booleans ([here](https://github.com/hmeriann/duckdb/blob/main/.github/workflows/NotifyExternalRepositories.yml#L60)).\r\n\r\nThis PR fixes that and adds missing `if` conditions to both workflows: to the `prepare-status:` job in `InvokeCI` (that job creates a flag to trigger or not the ODBC/JDBC Vendors) and to the `notify-jdbc-run` job in `NotifyExternalRepositories` (to make it have the same behaviour as `notify-odbc-run`)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/persistent/physical_batch_insert.cpp\n@@ -649,6 +649,7 @@ SinkFinalizeType PhysicalBatchInsert::Finalize(Pipeline &pipeline, Event &event,\n \n \t\tauto &optimistic_writer = data_table.GetOptimisticWriter(context);\n \t\toptimistic_writer.Merge(*writer);\n+\t\toptimistic_writer.FinalFlush();\n \t\tmemory_manager.FinalCheck();\n \t\treturn SinkFinalizeType::READY;\n \t}\n---\ntest/sql/storage/optimistic_write/optimistic_write_multi_statement.test_slow\n@@ -0,0 +1,27 @@\n+# name: test/sql/storage/optimistic_write/optimistic_write_multi_statement.test_slow\n+# description: Test multiple appends across statements within the same transaction\n+# group: [optimistic_write]\n+\n+load __TEST_DIR__/optimistic_write_multi_statement.db\n+\n+statement ok\n+SET force_compression='uncompressed';\n+\n+statement ok\n+CREATE TABLE main_tbl (v INT);\n+\n+statement ok\n+BEGIN\n+\n+foreach file_size 130815 68295 68490\n+\n+statement ok\n+CREATE OR REPLACE TEMPORARY TABLE insert_tbl AS FROM range(${file_size});\n+\n+statement ok\n+INSERT INTO main_tbl FROM insert_tbl;\n+\n+endloop\n+\n+statement ok\n+COMMIT\n\\ No newline at end of file\n---\ntest/sql/storage/optimistic_write/optimistic_write_multi_statement_string.test_slow\n@@ -0,0 +1,29 @@\n+# name: test/sql/storage/optimistic_write/optimistic_write_multi_statement_string.test_slow\n+# description: Test multiple appends across statements within the same transaction\n+# group: [optimistic_write]\n+\n+require parquet\n+\n+load __TEST_DIR__/optimistic_write_multi_statement_string.db\n+\n+statement ok\n+SET force_compression='uncompressed';\n+\n+statement ok\n+CREATE TABLE main_tbl (v VARCHAR);\n+\n+statement ok\n+BEGIN\n+\n+foreach file_size 130815 125018 125774 77995\n+\n+statement ok\n+COPY (SELECT uuid()::VARCHAR FROM range(${file_size})) TO '__TEST_DIR__/file.parquet' (ROW_GROUP_SIZE 100000000);\n+\n+statement ok\n+INSERT INTO main_tbl FROM '__TEST_DIR__/file.parquet';\n+\n+endloop\n+\n+statement ok\n+COMMIT\n---\n"}], "ideal": "Title: Explicitly flush the thread-local optimistic writer in `PhysicalBatchInsert` when finalizing\n\nBody: This fixes an issue that could pop up when inserting from specific row group sizes with `SET preserve_insertion_order=true`"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/InvokeCI.yml\n@@ -113,9 +113,9 @@ jobs:\n                 \"${{ needs.R.result }}\" == \"success\" && \\\n                 \"${{ needs.Wasm.result }}\" == \"success\" && \\\n                 \"${{ needs.static-libraries.result }}\" == \"success\" ]]; then\n-            echo \"success=true\" >> $GITHUB_OUTPUT\n+            echo \"success='true'\" >> $GITHUB_OUTPUT\n           else\n-            echo \"success=false\" >> $GITHUB_OUTPUT\n+            echo \"success='false'\" >> $GITHUB_OUTPUT\n           fi\n     \n   notify-external-repos-main:\n@@ -128,7 +128,7 @@ jobs:\n       target-branch: ${{ github.ref_name }}\n       duckdb-sha: ${{ github.ref }}\n       triggering-event: ${{ github.event_name }}\n-      should-publish: true\n+      should-publish: 'true'\n \n   notify-specific-branch-on-external-repos:\n     uses: ./.github/workflows/NotifyExternalRepositories.yml\n@@ -140,4 +140,4 @@ jobs:\n       target-branch: ${{ github.ref_name }}\n       duckdb-sha: ${{ inputs.git_ref }}\n       triggering-event: ${{ github.event_name }}\n-      should-publish: true\n\\ No newline at end of file\n+      should-publish: 'true'\n---\n.github/workflows/NotifyExternalRepositories.yml\n@@ -18,12 +18,12 @@ on:\n         type: 'string'\n       should-publish:\n         description: 'Should the called workflow push updates or not'\n-        default: false\n-        type: 'boolean'\n+        default: 'false'\n+        type: 'string'\n       is-success:\n         description: 'True, if all the builds in InvokeCI had succeeded'\n-        default: false\n-        type: 'boolean'\n+        default: 'false'\n+        type: 'string'\n   workflow_dispatch:\n     inputs:\n       duckdb-sha:\n@@ -42,12 +42,12 @@ on:\n         type: 'string'\n       should-publish:\n         description: 'Should the called workflow push updates'\n-        default: false\n-        type: 'boolean'\n+        default: 'false'\n+        type: 'string'\n       is-success:\n         description: 'True, if all the builds in InvokeCI had succeeded'\n-        default: false\n-        type: 'boolean'\n+        default: 'false'\n+        type: 'string'\n \n concurrency:\n   group: ${{ github.workflow }}\n@@ -57,7 +57,7 @@ jobs:\n   notify-odbc-run:\n     name: Run ODBC Vendor\n     runs-on: ubuntu-latest\n-    if: ${{ inputs.is-success }}\n+    if: ${{ inputs.is-success == 'true' }}\n     env:\n       PAT_USER: ${{ secrets.PAT_USERNAME }}\n       PAT_TOKEN: ${{ secrets.PAT_TOKEN }}\n@@ -94,5 +94,5 @@ jobs:\n         if: ${{ github.repository == 'duckdb/duckdb' }}\n         run: |\n           export URL=https://api.github.com/repos/duckdb/duckdb-build-status/actions/workflows/NightlyBuildsCheck.yml/dispatches\n-          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"branch\": \"${{ inputs.target-branch }}\", \"event\": \"${{ inputs.triggering_event }}\", \"should_publish\": \"${{ inputs.should_publish }}\"}}'\n+          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"branch\": \"${{ inputs.target-branch }}\", \"event\": \"${{ inputs.triggering-event }}\", \"should_publish\": \"${{ inputs.should-publish }}\"}}'\n           curl -v -XPOST -u \"${PAT_USER}:${PAT_TOKEN}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" $URL --data \"$DATA\"\n---\n"}], "ideal": "Title: Fix [InvokeCI / NotifyExternalRepository] Unexpected value 'true'\n\nBody: [InvokeCI](https://github.com/duckdb/duckdb/actions/runs/14584113147/job/40919219118) keeps skipping the external repos notification, which is more likely because of not matching types of the `should-publish.\r\n\r\nThe `.github/workflows/NotifyExternalRepositories.yml` was expecting `boolean` values passed as the inputs from the `.github/workflows/InvokeCI.yml`, but the `prepare-status` job set a string value into a `is-success` GH outputs variable.\r\n\r\nAlso we don't want to notify the Vendor when the `InvokeCI` hadn't succeeded, so the `.github/workflows/NotifyExternalRepositories.yml` should only trigger the `Vendor.yml` run on `if: ${{ inputs.is-success == 'true' }}`\r\n\r\nThis PR fixes mentioned problems and corrects the inputs names passed to the `NightlyBuildCheck.yml`."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/function_list.cpp\n@@ -122,6 +122,7 @@ static const StaticFunctionDefinition core_functions[] = {\n \tDUCKDB_AGGREGATE_FUNCTION(BoolOrFun),\n \tDUCKDB_SCALAR_FUNCTION(CanCastImplicitlyFun),\n \tDUCKDB_SCALAR_FUNCTION(CardinalityFun),\n+\tDUCKDB_SCALAR_FUNCTION(CastToTypeFun),\n \tDUCKDB_SCALAR_FUNCTION(CbrtFun),\n \tDUCKDB_SCALAR_FUNCTION_SET(CeilFun),\n \tDUCKDB_SCALAR_FUNCTION_SET_ALIAS(CeilingFun),\n---\nextension/core_functions/include/core_functions/scalar/generic_functions.hpp\n@@ -185,4 +185,14 @@ struct IsHistogramOtherBinFun {\n \tstatic ScalarFunction GetFunction();\n };\n \n+struct CastToTypeFun {\n+\tstatic constexpr const char *Name = \"cast_to_type\";\n+\tstatic constexpr const char *Parameters = \"param,type\";\n+\tstatic constexpr const char *Description = \"Casts the first argument to the type of the second argument\";\n+\tstatic constexpr const char *Example = \"cast_to_type('42', NULL::INTEGER)\";\n+\tstatic constexpr const char *Categories = \"\";\n+\n+\tstatic ScalarFunction GetFunction();\n+};\n+\n } // namespace duckdb\n---\nextension/core_functions/scalar/generic/CMakeLists.txt\n@@ -3,6 +3,7 @@ add_library_unity(\n   OBJECT\n   alias.cpp\n   binning.cpp\n+  cast_to_type.cpp\n   can_implicitly_cast.cpp\n   current_setting.cpp\n   hash.cpp\n---\nextension/core_functions/scalar/generic/cast_to_type.cpp\n@@ -0,0 +1,29 @@\n+#include \"core_functions/scalar/generic_functions.hpp\"\n+#include \"duckdb/planner/expression/bound_cast_expression.hpp\"\n+\n+namespace duckdb {\n+\n+static void CastToTypeFunction(DataChunk &args, ExpressionState &state, Vector &result) {\n+\tthrow InternalException(\"CastToType function cannot be executed directly\");\n+}\n+\n+unique_ptr<Expression> BindCastToTypeFunction(FunctionBindExpressionInput &input) {\n+\tauto &return_type = input.children[1]->return_type;\n+\tif (return_type.id() == LogicalTypeId::UNKNOWN) {\n+\t\t// parameter - unknown return type\n+\t\tthrow ParameterNotResolvedException();\n+\t}\n+\tif (return_type.id() == LogicalTypeId::SQLNULL) {\n+\t\tthrow InvalidInputException(\"cast_to_type cannot be used to cast to NULL\");\n+\t}\n+\treturn BoundCastExpression::AddCastToType(input.context, std::move(input.children[0]), return_type);\n+}\n+\n+ScalarFunction CastToTypeFun::GetFunction() {\n+\tauto fun = ScalarFunction({LogicalType::ANY, LogicalType::ANY}, LogicalType::ANY, CastToTypeFunction);\n+\tfun.null_handling = FunctionNullHandling::SPECIAL_HANDLING;\n+\tfun.bind_expression = BindCastToTypeFunction;\n+\treturn fun;\n+}\n+\n+} // namespace duckdb\n---\nextension/core_functions/scalar/generic/functions.json\n@@ -119,5 +119,12 @@\n         \"description\": \"Whether or not the provided value is the histogram \\\"other\\\" bin (used for values not belonging to any provided bin)\",\n         \"example\": \"is_histogram_other_bin(v)\",\n         \"type\": \"scalar_function\"\n+    },\n+    {\n+        \"name\": \"cast_to_type\",\n+        \"parameters\": \"param,type\",\n+        \"description\": \"Casts the first argument to the type of the second argument\",\n+        \"example\": \"cast_to_type('42', NULL::INTEGER)\",\n+        \"type\": \"scalar_function\"\n     }\n ]\n---\nsrc/include/duckdb/main/extension_entries.hpp\n@@ -120,6 +120,7 @@ static constexpr ExtensionFunctionEntry EXTENSION_FUNCTIONS[] = {\n     {\"broadcast\", \"inet\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"can_cast_implicitly\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"cardinality\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n+    {\"cast_to_type\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"cbrt\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"ceil\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n     {\"ceiling\", \"core_functions\", CatalogType::SCALAR_FUNCTION_ENTRY},\n---\ntest/sql/function/generic/cast_to_type.test\n@@ -0,0 +1,56 @@\n+# name: test/sql/function/generic/cast_to_type.test\n+# description: Test cast_to_type function\n+# group: [generic]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query I\n+SELECT cast_to_type('  42', NULL::INT)\n+----\n+42\n+\n+statement error\n+SELECT cast_to_type('hello', NULL::INT)\n+----\n+Conversion Error\n+\n+statement ok\n+CREATE OR REPLACE MACRO try_trim_null(s) AS CASE WHEN typeof(s)=='VARCHAR' THEN cast_to_type(nullif(trim(s::VARCHAR), ''), s) ELSE s END;\n+\n+query III\n+SELECT try_trim_null(42) as trim_int, try_trim_null('  col  ') as trim_varchar, try_trim_null('') as trim_empty;\n+----\n+42\tcol\tNULL\n+\n+statement ok\n+create table tbl(i int, v varchar);\n+\n+statement ok\n+insert into tbl values (42, ' hello '), (100, '   ');\n+\n+query II\n+SELECT try_trim_null(COLUMNS(*)) FROM tbl\n+----\n+42\thello\n+100\tNULL\n+\n+# prepared statements\n+statement ok\n+PREPARE v1 AS SELECT cast_to_type(' 42', ?);\n+\n+query I\n+EXECUTE v1(NULL::INT)\n+----\n+42\n+\n+query I\n+EXECUTE v1(NULL::VARCHAR)\n+----\n+ 42\n+\n+# cast to NULL\n+statement error\n+SELECT cast_to_type(42, NULL);\n+----\n+cannot be used to cast to NULL\n---\n"}], "ideal": "Title: Add support for the cast_to_type function, that allows generating a cast from an expression to the type of another column\n\nBody: This PR adds the `cast_to_type` function. This function is transformed into a cast from the first input argument to the type of the second input argument. For example:\r\n\r\n```sql\r\nSELECT cast_to_type('42', NULL::INT) AS res;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  res  \u2502\r\n\u2502 int32 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  42   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThis function is primarily useful in macros, as it allows us to maintain types. This helps with making generic macros (that operate on different types). For example, here is a macro that adds to a number if the input is an integer:\r\n\r\n```sql\r\nCREATE TABLE tbl(i INT, s VARCHAR);\r\nINSERT INTO tbl VALUES (42, 'hello world');\r\n\r\nCREATE MACRO conditional_add(col, nr) AS\r\n    CASE WHEN typeof(col) == 'INTEGER'\r\n    THEN cast_to_type(col::INTEGER + nr, col)\r\n    ELSE col END;\r\nSELECT conditional_add(COLUMNS(*), 100) FROM tbl;\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   i   \u2502      s      \u2502\r\n\u2502 int32 \u2502   varchar   \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502  142  \u2502 hello world \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThe way this works is that the `CASE` statement needs to return the same type in all code paths. We can perform the addition on any input column by adding a cast to the desired type - but we need to cast the result of the addition back to the source type to make the binding work. Without the cast we end up with the following error message:\r\n\r\n```sql\r\n\r\nCREATE MACRO broken_conditional_add(col, nr) AS\r\n    CASE WHEN typeof(col) == 'INTEGER'\r\n    THEN col::INTEGER + nr\r\n    ELSE col END;\r\nSELECT broken_conditional_add(COLUMNS(*), 100) FROM tbl;\r\n-- Cannot mix values of type VARCHAR and INTEGER in CASE expression - an explicit cast is required\r\n```\r\n\r\nSo we need a way of dynamically casting to the type of a column - which this PR adds using the `cast_to_type` function."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/storage/caching_file_system.hpp\n@@ -79,6 +79,8 @@ struct CachingFileHandle {\n \tOpenFileInfo path;\n \t//! Flags used to open the file\n \tFileOpenFlags flags;\n+\t//! Whether to validate the cache entry\n+\tbool validate;\n \t//! The associated CachedFile with cached ranges\n \tCachedFile &cached_file;\n \n@@ -113,8 +115,6 @@ class CachingFileSystem {\n \tFileSystem &file_system;\n \t//! The External File Cache that caches the files\n \tExternalFileCache &external_file_cache;\n-\t//! Whether to validate cache entries\n-\tbool validate;\n };\n \n } // namespace duckdb\n---\nsrc/storage/caching_file_system.cpp\n@@ -11,8 +11,7 @@\n namespace duckdb {\n \n CachingFileSystem::CachingFileSystem(FileSystem &file_system_p, DatabaseInstance &db)\n-    : file_system(file_system_p), external_file_cache(ExternalFileCache::Get(db)), validate(true) {\n-\t// TODO: \"validate\" defaults to true (for now)\n+    : file_system(file_system_p), external_file_cache(ExternalFileCache::Get(db)) {\n }\n \n CachingFileSystem::~CachingFileSystem() {\n@@ -29,8 +28,15 @@ unique_ptr<CachingFileHandle> CachingFileSystem::OpenFile(const OpenFileInfo &pa\n CachingFileHandle::CachingFileHandle(CachingFileSystem &caching_file_system_p, const OpenFileInfo &path_p,\n                                      FileOpenFlags flags_p, CachedFile &cached_file_p)\n     : caching_file_system(caching_file_system_p), external_file_cache(caching_file_system.external_file_cache),\n-      path(path_p), flags(flags_p), cached_file(cached_file_p), position(0) {\n-\tif (!external_file_cache.IsEnabled() || caching_file_system.validate) {\n+      path(path_p), flags(flags_p), validate(true), cached_file(cached_file_p), position(0) {\n+\tif (path.extended_info) {\n+\t\tconst auto &open_options = path.extended_info->options;\n+\t\tconst auto validate_entry = open_options.find(\"validate_external_file_cache\");\n+\t\tif (validate_entry != open_options.end()) {\n+\t\t\tvalidate = BooleanValue::Get(validate_entry->second);\n+\t\t}\n+\t}\n+\tif (!external_file_cache.IsEnabled() || validate) {\n \t\t// If caching is disabled, or if we must validate cache entries, we always have to open the file\n \t\tGetFileHandle();\n \t\treturn;\n@@ -54,7 +60,7 @@ FileHandle &CachingFileHandle::GetFileHandle() {\n \t\tversion_tag = caching_file_system.file_system.GetVersionTag(*file_handle);\n \n \t\tauto guard = cached_file.lock.GetExclusiveLock();\n-\t\tif (!cached_file.IsValid(guard, caching_file_system.validate, version_tag, last_modified, current_time)) {\n+\t\tif (!cached_file.IsValid(guard, validate, version_tag, last_modified, current_time)) {\n \t\t\tcached_file.Ranges(guard).clear(); // Invalidate entire cache\n \t\t}\n \t\tcached_file.FileSize(guard) = file_handle->GetFileSize();\n@@ -143,15 +149,15 @@ string CachingFileHandle::GetPath() const {\n }\n \n idx_t CachingFileHandle::GetFileSize() {\n-\tif (file_handle || caching_file_system.validate) {\n+\tif (file_handle || validate) {\n \t\treturn GetFileHandle().GetFileSize();\n \t}\n \tauto guard = cached_file.lock.GetSharedLock();\n \treturn cached_file.FileSize(guard);\n }\n \n time_t CachingFileHandle::GetLastModifiedTime() {\n-\tif (file_handle || caching_file_system.validate) {\n+\tif (file_handle || validate) {\n \t\tGetFileHandle();\n \t\treturn last_modified;\n \t}\n@@ -160,7 +166,7 @@ time_t CachingFileHandle::GetLastModifiedTime() {\n }\n \n bool CachingFileHandle::CanSeek() {\n-\tif (file_handle || caching_file_system.validate) {\n+\tif (file_handle || validate) {\n \t\treturn GetFileHandle().CanSeek();\n \t}\n \tauto guard = cached_file.lock.GetSharedLock();\n@@ -172,15 +178,15 @@ bool CachingFileHandle::IsRemoteFile() const {\n }\n \n bool CachingFileHandle::OnDiskFile() {\n-\tif (file_handle || caching_file_system.validate) {\n+\tif (file_handle || validate) {\n \t\treturn GetFileHandle().OnDiskFile();\n \t}\n \tauto guard = cached_file.lock.GetSharedLock();\n \treturn cached_file.OnDiskFile(guard);\n }\n \n const string &CachingFileHandle::GetVersionTag(const unique_ptr<StorageLockKey> &guard) {\n-\tif (file_handle || caching_file_system.validate) {\n+\tif (file_handle || validate) {\n \t\tGetFileHandle();\n \t\treturn version_tag;\n \t}\n---\n"}], "ideal": "Title: Add `ExternalFileCache` validation as option for `ExtendedOpenFileInfo`\n\nBody: This PR adds functionality to bypass the built-in cache validation for `ExternalFileCache` by adding the key `\"validate_external_file_cache\"` with value `false`, to the `ExtendedOpenFileInfo` of `OpenFileInfo`. This is not exposed to users, so it can only be used internally. This avoids doing a `HEAD` request for files that have been queried before.\r\n\r\nThe Iceberg/Delta extensions should be able to set this to `false` safely.  cc @Tmonster "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/planner/expression_binder/base_select_binder.cpp\n@@ -101,9 +101,9 @@ BindResult BaseSelectBinder::BindGroup(ParsedExpression &expr, idx_t depth, idx_\n \tif (it != info.collated_groups.end()) {\n \t\t// This is an implicitly collated group, so we need to refer to the first() aggregate\n \t\tconst auto &aggr_index = it->second;\n-\t\tauto uncollated_first_expression =\n-\t\t    make_uniq<BoundColumnRefExpression>(expr.GetName(), node.aggregates[aggr_index]->return_type,\n-\t\t                                        ColumnBinding(node.aggregate_index, aggr_index), depth);\n+\t\tconst auto return_type = node.aggregates[aggr_index]->return_type;\n+\t\tauto uncollated_first_expression = make_uniq<BoundColumnRefExpression>(\n+\t\t    expr.GetName(), return_type, ColumnBinding(node.aggregate_index, aggr_index), depth);\n \n \t\tif (node.groups.grouping_sets.size() <= 1) {\n \t\t\t// if there are no more than two grouping sets, you can return the uncollated first expression.\n@@ -117,10 +117,10 @@ BindResult BaseSelectBinder::BindGroup(ParsedExpression &expr, idx_t depth, idx_\n \t\tauto collated_group_expression = make_uniq<BoundColumnRefExpression>(\n \t\t    expr.GetName(), group->return_type, ColumnBinding(node.group_index, group_index), depth);\n \n-\t\tauto sql_null = make_uniq<BoundConstantExpression>(Value(LogicalType::VARCHAR));\n+\t\tauto sql_null = make_uniq<BoundConstantExpression>(Value(return_type));\n \t\tauto when_expr = make_uniq<BoundOperatorExpression>(ExpressionType::OPERATOR_IS_NULL, LogicalType::BOOLEAN);\n \t\twhen_expr->children.push_back(std::move(collated_group_expression));\n-\t\tauto then_expr = make_uniq<BoundConstantExpression>(Value(LogicalType::VARCHAR));\n+\t\tauto then_expr = make_uniq<BoundConstantExpression>(Value(return_type));\n \t\tauto else_expr = std::move(uncollated_first_expression);\n \t\tauto case_expr =\n \t\t    make_uniq<BoundCaseExpression>(std::move(when_expr), std::move(then_expr), std::move(else_expr));\n---\n"}], "ideal": "Title: Fix grouping feature with interval type\n\nBody: Fix #17179 "}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/types/selection_vector.cpp\n@@ -28,6 +28,10 @@ string SelectionVector::ToString(idx_t count) const {\n \treturn result;\n }\n \n+void SelectionVector::Sort(idx_t count) {\n+\tstd::sort(sel_vector, sel_vector + count);\n+}\n+\n void SelectionVector::Print(idx_t count) const {\n \tPrinter::Print(ToString(count));\n }\n---\nsrc/execution/expression_executor/execute_conjunction.cpp\n@@ -130,6 +130,9 @@ idx_t ExpressionExecutor::Select(const BoundConjunctionExpression &expr, Express\n \t\t\t\tcurrent_sel = false_sel;\n \t\t\t}\n \t\t}\n+\t\tif (true_sel) {\n+\t\t\ttrue_sel->Sort(result_count);\n+\t\t}\n \n \t\t// adapt runtime statistics\n \t\tstate.adaptive_filter->EndFilter(filter_state);\n---\nsrc/include/duckdb/common/types/selection_vector.hpp\n@@ -116,6 +116,7 @@ struct SelectionVector {\n \t\treturn sel_vector;\n \t}\n \tvoid Verify(idx_t count, idx_t vector_size) const;\n+\tvoid Sort(idx_t count);\n \n private:\n \tsel_t *sel_vector;\n---\ntest/sql/conjunction/or_comparison.test\n@@ -48,3 +48,9 @@ SELECT pk FROM tab0 WHERE col0 < 84 OR col0 < 8 ;\n 5\n 7\n 8\n+\n+query I\n+select pk from tab0 where col0 = 37 or col0 = 86\n+----\n+0\n+1\n---\n"}], "ideal": "Title: Fix #17170: sort selection result in OR expression\n\nBody: This pr try to fix #17170 , sort selection when execute CONJUNCTION_OR"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/storage/local_storage.cpp\n@@ -161,7 +161,7 @@ ErrorData LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGr\n \t\t}\n \t\tmock_chunk.SetCardinality(chunk);\n \t\t// append this chunk to the indexes of the table\n-\t\terror = DataTable::AppendToIndexes(index_list, nullptr, mock_chunk, start_row, index_append_mode);\n+\t\terror = DataTable::AppendToIndexes(index_list, delete_indexes, mock_chunk, start_row, index_append_mode);\n \t\tif (error.HasError()) {\n \t\t\treturn false;\n \t\t}\n---\ntest/sql/index/art/constraints/test_art_eager_update.test_slow\n@@ -0,0 +1,50 @@\n+# name: test/sql/index/art/constraints/test_art_eager_update.test_slow\n+# description: Test ART BIGINT key type\n+# group: [constraints]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE OR REPLACE TABLE t (\n+\ts UUID,\n+\tpayload STRUCT(k VARCHAR)[],\n+\tp STRUCT(e VARCHAR, s DOUBLE),\n+\tpayload2 STRUCT(x INT)[],\n+\tother VARCHAR\n+);\n+\n+statement ok\n+CREATE OR REPLACE TABLE t2 (\n+\ts UUID,\n+\tpayload STRUCT(k VARCHAR)[],\n+\tp STRUCT(e VARCHAR, s DOUBLE),\n+\tpayload2 STRUCT(x INT)[]\n+);\n+\n+statement ok\n+INSERT INTO t(s, other) (\n+\tSELECT s, SUBSTRING(s::VARCHAR, 1, 8) AS other\n+\tFROM (SELECT uuid() AS s FROM range(300000))\n+);\n+\n+statement ok\n+INSERT INTO t2 (\n+\tSELECT\n+\t\ts,\n+\t\t[struct_pack(k := 'aaaa'), struct_pack(k := 'rrrr')],\n+\t\tstruct_pack(e := 'ddd', s := -9.0),\n+\t\t[struct_pack(x := 99), struct_pack(x := 100), struct_pack(x := 101)]\n+\tFROM t WHERE rowid % 2 = 0\n+);\n+\n+statement ok\n+ALTER TABLE t ADD PRIMARY KEY (s);\n+\n+statement ok\n+ALTER TABLE t2 ADD PRIMARY KEY (s);\n+\n+statement ok\n+UPDATE t\n+SET payload = t2.payload, p = t2.p, payload2 = t2.payload2\n+FROM t2 WHERE t.s = t2.s;\n---\n"}], "ideal": "Title: [Fix] Pass delete indexes when committing updates\n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/4682"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/core_functions_extension.cpp\n@@ -3,52 +3,11 @@\n \n #include \"core_functions/function_list.hpp\"\n #include \"duckdb/main/extension_util.hpp\"\n-#include \"duckdb/function/register_function_list_helper.hpp\"\n-#include \"duckdb/parser/parsed_data/create_aggregate_function_info.hpp\"\n-#include \"duckdb/parser/parsed_data/create_scalar_function_info.hpp\"\n \n namespace duckdb {\n \n-template <class T>\n-static void FillExtraInfo(const StaticFunctionDefinition &function, T &info) {\n-\tinfo.internal = true;\n-\tinfo.alias_of = function.alias_of;\n-\tFillFunctionDescriptions(function, info);\n-\tinfo.on_conflict = OnCreateConflict::ALTER_ON_CONFLICT;\n-}\n-\n void LoadInternal(DuckDB &db) {\n-\tauto functions = StaticFunctionDefinition::GetFunctionList();\n-\tfor (idx_t i = 0; functions[i].name; i++) {\n-\t\tauto &function = functions[i];\n-\t\tif (function.get_function || function.get_function_set) {\n-\t\t\t// scalar function\n-\t\t\tScalarFunctionSet result;\n-\t\t\tif (function.get_function) {\n-\t\t\t\tresult.AddFunction(function.get_function());\n-\t\t\t} else {\n-\t\t\t\tresult = function.get_function_set();\n-\t\t\t}\n-\t\t\tresult.name = function.name;\n-\t\t\tCreateScalarFunctionInfo info(result);\n-\t\t\tFillExtraInfo(function, info);\n-\t\t\tExtensionUtil::RegisterFunction(*db.instance, std::move(info));\n-\t\t} else if (function.get_aggregate_function || function.get_aggregate_function_set) {\n-\t\t\t// aggregate function\n-\t\t\tAggregateFunctionSet result;\n-\t\t\tif (function.get_aggregate_function) {\n-\t\t\t\tresult.AddFunction(function.get_aggregate_function());\n-\t\t\t} else {\n-\t\t\t\tresult = function.get_aggregate_function_set();\n-\t\t\t}\n-\t\t\tresult.name = function.name;\n-\t\t\tCreateAggregateFunctionInfo info(result);\n-\t\t\tFillExtraInfo(function, info);\n-\t\t\tExtensionUtil::RegisterFunction(*db.instance, std::move(info));\n-\t\t} else {\n-\t\t\tthrow InternalException(\"Do not know how to register function of this type\");\n-\t\t}\n-\t}\n+\tFunctionList::RegisterExtensionFunctions(*db.instance, CoreFunctionList::GetFunctionList());\n }\n \n void CoreFunctionsExtension::Load(DuckDB &db) {\n---\nextension/core_functions/function_list.cpp\n@@ -405,7 +405,7 @@ static const StaticFunctionDefinition core_functions[] = {\n \tFINAL_FUNCTION\n };\n \n-const StaticFunctionDefinition *StaticFunctionDefinition::GetFunctionList() {\n+const StaticFunctionDefinition *CoreFunctionList::GetFunctionList() {\n \treturn core_functions;\n }\n \n---\nextension/core_functions/include/core_functions/function_list.hpp\n@@ -1,34 +1,18 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb/core_functions/function_list.hpp\n+// extension/core_functions/include/core_functions/function_list.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n \n #pragma once\n \n-#include \"duckdb.hpp\"\n+#include \"duckdb/function/function_list.hpp\"\n \n namespace duckdb {\n \n-typedef ScalarFunction (*get_scalar_function_t)();\n-typedef ScalarFunctionSet (*get_scalar_function_set_t)();\n-typedef AggregateFunction (*get_aggregate_function_t)();\n-typedef AggregateFunctionSet (*get_aggregate_function_set_t)();\n-\n-struct StaticFunctionDefinition {\n-\tconst char *name;\n-\tconst char *alias_of;\n-\tconst char *parameters;\n-\tconst char *description;\n-\tconst char *example;\n-\tconst char *categories;\n-\tget_scalar_function_t get_function;\n-\tget_scalar_function_set_t get_function_set;\n-\tget_aggregate_function_t get_aggregate_function;\n-\tget_aggregate_function_set_t get_aggregate_function_set;\n-\n+struct CoreFunctionList {\n \tstatic const StaticFunctionDefinition *GetFunctionList();\n };\n \n---\nsrc/function/register_function_list.cpp\n@@ -4,18 +4,51 @@\n #include \"duckdb/function/register_function_list_helper.hpp\"\n #include \"duckdb/parser/parsed_data/create_aggregate_function_info.hpp\"\n #include \"duckdb/parser/parsed_data/create_scalar_function_info.hpp\"\n+#include \"duckdb/main/extension_util.hpp\"\n \n namespace duckdb {\n \n-template <class T>\n+struct MainRegisterContext {\n+\tMainRegisterContext(Catalog &catalog, CatalogTransaction transaction) : catalog(catalog), transaction(transaction) {\n+\t}\n+\n+\tCatalog &catalog;\n+\tCatalogTransaction transaction;\n+};\n+\n+struct MainRegister {\n+\ttemplate <class T>\n+\tstatic void FillExtraInfo(T &info) {\n+\t}\n+\n+\ttemplate <class T>\n+\tstatic void RegisterFunction(MainRegisterContext &context, T &info) {\n+\t\tcontext.catalog.CreateFunction(context.transaction, info);\n+\t}\n+};\n+\n+struct ExtensionRegister {\n+\ttemplate <class T>\n+\tstatic void FillExtraInfo(T &info) {\n+\t\tinfo.on_conflict = OnCreateConflict::ALTER_ON_CONFLICT;\n+\t}\n+\n+\ttemplate <class T>\n+\tstatic void RegisterFunction(DatabaseInstance &db, T &info) {\n+\t\tExtensionUtil::RegisterFunction(db, std::move(info));\n+\t}\n+};\n+\n+template <class OP, class T>\n static void FillExtraInfo(const StaticFunctionDefinition &function, T &info) {\n \tinfo.internal = true;\n \tinfo.alias_of = function.alias_of;\n \tFillFunctionDescriptions(function, info);\n+\tOP::FillExtraInfo(info);\n }\n \n-static void RegisterFunctionList(Catalog &catalog, CatalogTransaction transaction,\n-                                 const StaticFunctionDefinition *functions) {\n+template <class OP, class REGISTER_CONTEXT>\n+static void RegisterFunctionList(REGISTER_CONTEXT &context, const StaticFunctionDefinition *functions) {\n \tfor (idx_t i = 0; functions[i].name; i++) {\n \t\tauto &function = functions[i];\n \t\tif (function.get_function || function.get_function_set) {\n@@ -28,8 +61,8 @@ static void RegisterFunctionList(Catalog &catalog, CatalogTransaction transactio\n \t\t\t}\n \t\t\tresult.name = function.name;\n \t\t\tCreateScalarFunctionInfo info(result);\n-\t\t\tFillExtraInfo(function, info);\n-\t\t\tcatalog.CreateFunction(transaction, info);\n+\t\t\tFillExtraInfo<OP>(function, info);\n+\t\t\tOP::RegisterFunction(context, info);\n \t\t} else if (function.get_aggregate_function || function.get_aggregate_function_set) {\n \t\t\t// aggregate function\n \t\t\tAggregateFunctionSet result;\n@@ -40,16 +73,21 @@ static void RegisterFunctionList(Catalog &catalog, CatalogTransaction transactio\n \t\t\t}\n \t\t\tresult.name = function.name;\n \t\t\tCreateAggregateFunctionInfo info(result);\n-\t\t\tFillExtraInfo(function, info);\n-\t\t\tcatalog.CreateFunction(transaction, info);\n+\t\t\tFillExtraInfo<OP>(function, info);\n+\t\t\tOP::RegisterFunction(context, info);\n \t\t} else {\n \t\t\tthrow InternalException(\"Do not know how to register function of this type\");\n \t\t}\n \t}\n }\n \n+void FunctionList::RegisterExtensionFunctions(DatabaseInstance &db, const StaticFunctionDefinition *functions) {\n+\tRegisterFunctionList<ExtensionRegister>(db, functions);\n+}\n+\n void FunctionList::RegisterFunctions(Catalog &catalog, CatalogTransaction transaction) {\n-\tRegisterFunctionList(catalog, transaction, FunctionList::GetInternalFunctionList());\n+\tMainRegisterContext context(catalog, transaction);\n+\tRegisterFunctionList<MainRegister>(context, FunctionList::GetInternalFunctionList());\n }\n \n } // namespace duckdb\n---\nsrc/include/duckdb/function/function_list.hpp\n@@ -36,6 +36,7 @@ struct CatalogTransaction;\n struct FunctionList {\n \tstatic const StaticFunctionDefinition *GetInternalFunctionList();\n \tstatic void RegisterFunctions(Catalog &catalog, CatalogTransaction transaction);\n+\tstatic void RegisterExtensionFunctions(DatabaseInstance &db, const StaticFunctionDefinition *functions);\n };\n \n } // namespace duckdb\n---\n"}], "ideal": "Title: Unify function list\n\nBody: Changes copied from https://github.com/Mytherin/duckdb/tree/unifyfunctionlist.\r\nApplied fix to `FunctionList::RegisterExtensionFunctions`"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/gzip_file_system.cpp\n@@ -120,7 +120,7 @@ void MiniZStreamWrapper::Initialize(CompressedFile &file, bool write) {\n \t} else {\n \t\tidx_t data_start = GZIP_HEADER_MINSIZE;\n \t\tauto read_count = file.child_handle->Read(gzip_hdr, GZIP_HEADER_MINSIZE);\n-\t\tGZipFileSystem::VerifyGZIPHeader(gzip_hdr, NumericCast<idx_t>(read_count));\n+\t\tGZipFileSystem::VerifyGZIPHeader(gzip_hdr, NumericCast<idx_t>(read_count), &file);\n \t\t// Skip over the extra field if necessary\n \t\tif (gzip_hdr[3] & GZIP_FLAG_EXTRA) {\n \t\t\tuint8_t gzip_xlen[2];\n@@ -157,7 +157,7 @@ bool MiniZStreamWrapper::Read(StreamData &sd) {\n \t\tauto body_ptr = sd.in_buff_start + GZIP_FOOTER_SIZE;\n \t\tuint8_t gzip_hdr[GZIP_HEADER_MINSIZE];\n \t\tmemcpy(gzip_hdr, body_ptr, GZIP_HEADER_MINSIZE);\n-\t\tGZipFileSystem::VerifyGZIPHeader(gzip_hdr, GZIP_HEADER_MINSIZE);\n+\t\tGZipFileSystem::VerifyGZIPHeader(gzip_hdr, GZIP_HEADER_MINSIZE, nullptr);\n \t\tbody_ptr += GZIP_HEADER_MINSIZE;\n \t\tif (gzip_hdr[3] & GZIP_FLAG_EXTRA) {\n \t\t\tauto xlen = NumericCast<idx_t>((uint8_t)*body_ptr | (uint8_t) * (body_ptr + 1) << 8);\n@@ -306,19 +306,22 @@ class GZipFile : public CompressedFile {\n \tGZipFileSystem gzip_fs;\n };\n \n-void GZipFileSystem::VerifyGZIPHeader(uint8_t gzip_hdr[], idx_t read_count) {\n+void GZipFileSystem::VerifyGZIPHeader(uint8_t gzip_hdr[], idx_t read_count, optional_ptr<CompressedFile> source_file) {\n+\t// include the filename in the error message if known\n+\tstring file_info = source_file ? \": \" + source_file->path : \"\";\n+\n \t// check for incorrectly formatted files\n \tif (read_count != GZIP_HEADER_MINSIZE) {\n-\t\tthrow IOException(\"Input is not a GZIP stream\");\n+\t\tthrow IOException(\"Input is not a GZIP stream\" + file_info);\n \t}\n \tif (gzip_hdr[0] != 0x1F || gzip_hdr[1] != 0x8B) { // magic header\n-\t\tthrow IOException(\"Input is not a GZIP stream\");\n+\t\tthrow IOException(\"Input is not a GZIP stream\" + file_info);\n \t}\n \tif (gzip_hdr[2] != GZIP_COMPRESSION_DEFLATE) { // compression method\n-\t\tthrow IOException(\"Unsupported GZIP compression method\");\n+\t\tthrow IOException(\"Unsupported GZIP compression method\" + file_info);\n \t}\n \tif (gzip_hdr[3] & GZIP_FLAG_UNSUPPORTED) {\n-\t\tthrow IOException(\"Unsupported GZIP archive\");\n+\t\tthrow IOException(\"Unsupported GZIP archive\" + file_info);\n \t}\n }\n \n@@ -360,7 +363,7 @@ string GZipFileSystem::UncompressGZIPString(const char *data, idx_t size) {\n \t}\n \tmemcpy(gzip_hdr, body_ptr, GZIP_HEADER_MINSIZE);\n \tbody_ptr += GZIP_HEADER_MINSIZE;\n-\tGZipFileSystem::VerifyGZIPHeader(gzip_hdr, GZIP_HEADER_MINSIZE);\n+\tGZipFileSystem::VerifyGZIPHeader(gzip_hdr, GZIP_HEADER_MINSIZE, nullptr);\n \n \tif (gzip_hdr[3] & GZIP_FLAG_EXTRA) {\n \t\tthrow IOException(\"Extra field in a GZIP stream unsupported\");\n---\nsrc/include/duckdb/common/gzip_file_system.hpp\n@@ -24,7 +24,7 @@ class GZipFileSystem : public CompressedFileSystem {\n \t}\n \n \t//! Verifies that a buffer contains a valid GZIP header\n-\tstatic void VerifyGZIPHeader(uint8_t gzip_hdr[], idx_t read_count);\n+\tstatic void VerifyGZIPHeader(uint8_t gzip_hdr[], idx_t read_count, optional_ptr<CompressedFile> source_file);\n \tstatic bool CheckIsZip(const char *length, idx_t size);\n \n \t//! Consumes a byte stream as a gzip string, returning the decompressed string\n---\ntest/sql/copy/csv/zstd_crash.test\n@@ -25,7 +25,7 @@ Attempting to open a compressed file, but the compression type is not supported\n statement error\n COPY test_zst FROM 'data/csv/broken/test.csv.zst' (COMPRESSION GZIP);\n ----\n-Input is not a GZIP stream\n+Input is not a GZIP stream: data/csv/broken/test.csv.zst\n \n statement error\n COPY test_zst FROM 'data/csv/broken/test.csv.zst' (COMPRESSION NONE);\n---\n"}], "ideal": "Title: Add filename to GZIP stream error\n\nBody: When querying a large list of compressed files using a wildcard, and some of those files are invalid (e.g. 0 bytes), it can be quite difficult to figure out which one is causing the problem.\r\n\r\nThis PR proposes to add the (first) invalid filename to the error message when possible.\r\n\r\nBefore:\r\n```sql\r\nselect * from read_csv('s3://marco-crunchy-data/tmp/*.csv.gz');\r\nIO Error: Input is not a GZIP stream\r\n```\r\n\r\nAfter:\r\n```sql\r\nselect * from read_csv('s3://marco-crunchy-data/tmp/*.csv.gz');\r\nIO Error: Input is not a GZIP stream: s3://marco-crunchy-data/tmp/faulty.csv.gz\r\n```"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/InvokeCI.yml\n@@ -103,7 +103,7 @@ jobs:\n     steps:\n       - id: set-output\n         shell: bash\n-        run:\n+        run: |\n           if [[ \"${{ needs.osx.result }}\" == \"success\" && \\\n                 \"${{ needs.linux-release.result }}\" == \"success\" && \\\n                 \"${{ needs.linux-release.result }}\" == \"success\" && \\\n---\n"}], "ideal": "Title: [InvokeCI] Add missing pipe to run instruction\n\nBody: The latest InvokeCI run had failed because of missing pipe in the `prepare-status` step's `run:` instruction.\r\nThis PR add a missing pipe and it should fix the [\"unexpected end of command\" issue](https://github.com/duckdb/duckdb/actions/runs/14505429008/job/40705456895#step:2:1)."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/core_functions/aggregate/holistic/approximate_quantile.cpp\n@@ -6,8 +6,6 @@\n #include \"duckdb/common/serializer/serializer.hpp\"\n #include \"duckdb/common/serializer/deserializer.hpp\"\n \n-#include <algorithm>\n-#include <cmath>\n #include <stdlib.h>\n \n namespace duckdb {\n@@ -17,6 +15,50 @@ struct ApproxQuantileState {\n \tidx_t pos;\n };\n \n+struct ApproxQuantileCoding {\n+\ttemplate <typename INPUT_TYPE, typename SAVE_TYPE>\n+\tstatic SAVE_TYPE Encode(const INPUT_TYPE &input) {\n+\t\treturn Cast::template Operation<INPUT_TYPE, SAVE_TYPE>(input);\n+\t}\n+\n+\ttemplate <typename SAVE_TYPE, typename TARGET_TYPE>\n+\tstatic bool Decode(const SAVE_TYPE &source, TARGET_TYPE &target) {\n+\t\t// The result is approximate, so clamp instead of overflowing.\n+\t\tif (TryCast::Operation(source, target, false)) {\n+\t\t\treturn true;\n+\t\t} else if (source < 0) {\n+\t\t\ttarget = NumericLimits<TARGET_TYPE>::Minimum();\n+\t\t} else {\n+\t\t\ttarget = NumericLimits<TARGET_TYPE>::Maximum();\n+\t\t}\n+\t\treturn false;\n+\t}\n+};\n+\n+template <>\n+double ApproxQuantileCoding::Encode(const dtime_tz_t &input) {\n+\treturn Encode<uint64_t, double>(input.sort_key());\n+}\n+\n+template <>\n+bool ApproxQuantileCoding::Decode(const double &source, dtime_tz_t &target) {\n+\tuint64_t sort_key;\n+\tconst auto decoded = Decode<double, uint64_t>(source, sort_key);\n+\tif (decoded) {\n+\t\t//\tWe can invert the sort key because its offset was not touched.\n+\t\tauto offset = dtime_tz_t::decode_offset(sort_key);\n+\t\tauto micros = dtime_tz_t::decode_micros(sort_key);\n+\t\tmicros -= int64_t(dtime_tz_t::encode_offset(offset) * dtime_tz_t::OFFSET_MICROS);\n+\t\ttarget = dtime_tz_t(dtime_t(micros), offset);\n+\t} else if (source < 0) {\n+\t\ttarget = Value::MinimumValue(LogicalTypeId::TIME_TZ).GetValue<dtime_tz_t>();\n+\t} else {\n+\t\ttarget = Value::MaximumValue(LogicalTypeId::TIME_TZ).GetValue<dtime_tz_t>();\n+\t}\n+\n+\treturn decoded;\n+}\n+\n struct ApproximateQuantileBindData : public FunctionData {\n \tApproximateQuantileBindData() {\n \t}\n@@ -73,7 +115,7 @@ struct ApproxQuantileOperation {\n \n \ttemplate <class INPUT_TYPE, class STATE, class OP>\n \tstatic void Operation(STATE &state, const INPUT_TYPE &input, AggregateUnaryInput &unary_input) {\n-\t\tauto val = Cast::template Operation<INPUT_TYPE, SAVE_TYPE>(input);\n+\t\tauto val = ApproxQuantileCoding::template Encode<INPUT_TYPE, SAVE_TYPE>(input);\n \t\tif (!Value::DoubleIsFinite(val)) {\n \t\t\treturn;\n \t\t}\n@@ -121,15 +163,8 @@ struct ApproxQuantileScalarOperation : public ApproxQuantileOperation {\n \t\tstate.h->compress();\n \t\tauto &bind_data = finalize_data.input.bind_data->template Cast<ApproximateQuantileBindData>();\n \t\tD_ASSERT(bind_data.quantiles.size() == 1);\n-\t\t// The result is approximate, so clamp instead of overflowing.\n \t\tconst auto source = state.h->quantile(bind_data.quantiles[0]);\n-\t\tif (TryCast::Operation(source, target, false)) {\n-\t\t\treturn;\n-\t\t} else if (source < 0) {\n-\t\t\ttarget = NumericLimits<TARGET_TYPE>::Minimum();\n-\t\t} else {\n-\t\t\ttarget = NumericLimits<TARGET_TYPE>::Maximum();\n-\t\t}\n+\t\tApproxQuantileCoding::Decode(source, target);\n \t}\n };\n \n@@ -281,7 +316,9 @@ struct ApproxQuantileListOperation : public ApproxQuantileOperation {\n \t\tentry.length = bind_data.quantiles.size();\n \t\tfor (size_t q = 0; q < entry.length; ++q) {\n \t\t\tconst auto &quantile = bind_data.quantiles[q];\n-\t\t\trdata[ridx + q] = Cast::template Operation<SAVE_TYPE, CHILD_TYPE>(state.h->quantile(quantile));\n+\t\t\tconst auto &source = state.h->quantile(quantile);\n+\t\t\tauto &target = rdata[ridx + q];\n+\t\t\tApproxQuantileCoding::Decode(source, target);\n \t\t}\n \n \t\tListVector::SetListSize(finalize_data.result, entry.offset + entry.length);\n---\ntest/sql/aggregate/aggregates/test_approx_quantile.test\n@@ -112,6 +112,12 @@ SELECT CASE\n ----\n 1\n \n+# TIMETZ Support. Note this is not exact because we use DOUBLEs internally.\n+query I\n+SELECT approx_quantile('1:02:03.000000+05:30'::TIMETZ, 0.5);\n+----\n+01:02:42+05:30:39\n+\n # List versions\n query I\n SELECT [\n---\ntest/sql/show_select/test_summarize.test\n@@ -64,6 +64,12 @@ SELECT range::TIME AS range from range('2024-01-01'::DATE, '2024-04-10'::DATE, I\n ----\n range\tTIME\t00:00:00\t23:00:00\t28\t11:30:00\tNULL\t05:24:35.480769\t11:28:55.400975\t17:30:41.666667\t2400\t0.00\n \n+# TIMETZ quantiles. Note these are fuzzy because we use DOUBLEs in the sketch.\n+query IIIIIIIIIIII\n+SUMMARIZE (SELECT '1:02:03.000000+05:30'::TIMETZ AS ttz);\n+----\n+ttz\tTIME WITH TIME ZONE\t01:02:03+05:30\t01:02:03+05:30\t1\t19:32:03+00\tNULL\t01:02:42+05:30:39\t01:02:42+05:30:39\t01:02:42+05:30:39\t1\t0.00\n+\n statement ok\n SUMMARIZE VALUES (1.0),(6754950520);\n \n---\n"}], "ideal": "Title: Issue #17115: TimeTZ Approximate Quantile\n\nBody: * Fix implementation to actually work...\r\n* Add tests.\r\n\r\nfixes: duckdb/duckdb#17115\r\nfixes: duckdblabs/duckdb-internal#4669"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/multi_file/multi_file_reader.cpp\n@@ -335,13 +335,11 @@ MultiFileReader::InitializeGlobalState(ClientContext &context, const MultiFileOp\n \treturn nullptr;\n }\n \n-ReaderInitializeType MultiFileReader::CreateMapping(ClientContext &context, MultiFileReaderData &reader_data,\n-                                                    const vector<MultiFileColumnDefinition> &global_columns,\n-                                                    const vector<ColumnIndex> &global_column_ids,\n-                                                    optional_ptr<TableFilterSet> filters,\n-                                                    const OpenFileInfo &initial_file,\n-                                                    const MultiFileReaderBindData &bind_data,\n-                                                    const virtual_column_map_t &virtual_columns) {\n+ReaderInitializeType MultiFileReader::CreateMapping(\n+    ClientContext &context, MultiFileReaderData &reader_data, const vector<MultiFileColumnDefinition> &global_columns,\n+    const vector<ColumnIndex> &global_column_ids, optional_ptr<TableFilterSet> filters,\n+    const OpenFileInfo &initial_file, const MultiFileReaderBindData &bind_data,\n+    const virtual_column_map_t &virtual_columns, optional_ptr<MultiFileReaderGlobalState> global_state) {\n \tMultiFileColumnMapper column_mapper(context, *this, reader_data, global_columns, global_column_ids, filters,\n \t                                    initial_file, bind_data, virtual_columns);\n \treturn column_mapper.CreateMapping();\n---\nsrc/include/duckdb/common/multi_file/multi_file_reader.hpp\n@@ -109,7 +109,8 @@ struct MultiFileReader {\n \tCreateMapping(ClientContext &context, MultiFileReaderData &reader_data,\n \t              const vector<MultiFileColumnDefinition> &global_columns, const vector<ColumnIndex> &global_column_ids,\n \t              optional_ptr<TableFilterSet> filters, const OpenFileInfo &initial_file,\n-\t              const MultiFileReaderBindData &bind_data, const virtual_column_map_t &virtual_columns);\n+\t              const MultiFileReaderBindData &bind_data, const virtual_column_map_t &virtual_columns,\n+\t              optional_ptr<MultiFileReaderGlobalState> global_state);\n \n \t//! Finalize the reading of a chunk - applying any constants that are required\n \tDUCKDB_API virtual void FinalizeChunk(ClientContext &context, const MultiFileBindData &bind_data,\n@@ -182,7 +183,8 @@ struct MultiFileReader {\n \t\tFinalizeBind(reader_data, bind_data.file_options, bind_data.reader_bind, global_columns, global_column_ids,\n \t\t             context, global_state);\n \t\treturn CreateMapping(context, reader_data, global_columns, global_column_ids, table_filters,\n-\t\t                     bind_data.file_list->GetFirstFile(), bind_data.reader_bind, bind_data.virtual_columns);\n+\t\t                     bind_data.file_list->GetFirstFile(), bind_data.reader_bind, bind_data.virtual_columns,\n+\t\t                     global_state);\n \t}\n \n \ttemplate <class BIND_DATA>\n---\n"}], "ideal": "Title: [Dev] Add `global_state `to MFR `CreateMapping`\n\nBody: @Mytherin context is the `schema.name-mapping.default` we talked about.\r\nWe can't access the `IcebergMetadata` with just a `MultiFileReader`\r\n\r\n\r\nPerhaps we can abuse FinalizeBind for this, since it *does* have access to the global_state and is executed directly before `CreateMapping`..."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/multi_file/multi_file_column_mapper.cpp\n@@ -647,11 +647,13 @@ MultiFileColumnMapper::EvaluateConstantFilters(ResultColumnMapping &mapping,\n \t\t//! FIXME: this does not check for filters against struct fields that are not present in the file\n \t\tauto global_column_id = global_column_ids[global_index].GetPrimaryIndex();\n \t\tValue constant_value;\n-\t\tauto virtual_it = virtual_columns.find(global_column_ids[global_index].GetPrimaryIndex());\n+\t\tauto virtual_it = virtual_columns.find(global_column_id);\n \t\tif (virtual_it != virtual_columns.end()) {\n \t\t\tauto &virtual_column = virtual_it->second;\n \t\t\tif (virtual_column.name == \"filename\") {\n \t\t\t\tconstant_value = Value(reader_data.reader->GetFileName());\n+\t\t\t} else if (global_column_id == MultiFileReader::COLUMN_IDENTIFIER_FILE_INDEX) {\n+\t\t\t\tconstant_value = Value::UBIGINT(reader_data.reader->file_list_idx.GetIndex());\n \t\t\t} else {\n \t\t\t\tthrow InternalException(\"Unrecognized virtual column found: %s\", virtual_column.name);\n \t\t\t}\n---\nsrc/common/multi_file/multi_file_reader.cpp\n@@ -18,6 +18,7 @@ namespace duckdb {\n \n constexpr column_t MultiFileReader::COLUMN_IDENTIFIER_FILENAME;\n constexpr column_t MultiFileReader::COLUMN_IDENTIFIER_FILE_ROW_NUMBER;\n+constexpr column_t MultiFileReader::COLUMN_IDENTIFIER_FILE_INDEX;\n constexpr int32_t MultiFileReader::ORDINAL_FIELD_ID;\n constexpr int32_t MultiFileReader::FILENAME_FIELD_ID;\n constexpr int32_t MultiFileReader::ROW_ID_FIELD_ID;\n@@ -261,6 +262,7 @@ void MultiFileReader::GetVirtualColumns(ClientContext &context, MultiFileReaderB\n \t\tbind_data.filename_idx = COLUMN_IDENTIFIER_FILENAME;\n \t\tresult.insert(make_pair(COLUMN_IDENTIFIER_FILENAME, TableColumn(\"filename\", LogicalType::VARCHAR)));\n \t}\n+\tresult.insert(make_pair(COLUMN_IDENTIFIER_FILE_INDEX, TableColumn(\"file_index\", LogicalType::UBIGINT)));\n }\n \n void MultiFileReader::FinalizeBind(MultiFileReaderData &reader_data, const MultiFileOptions &file_options,\n@@ -284,11 +286,16 @@ void MultiFileReader::FinalizeBind(MultiFileReaderData &reader_data, const Multi\n \t\tauto &col_id = global_column_ids[i];\n \t\tauto column_id = col_id.GetPrimaryIndex();\n \t\tif ((options.filename_idx.IsValid() && column_id == options.filename_idx.GetIndex()) ||\n-\t\t    column_id == MultiFileReader::COLUMN_IDENTIFIER_FILENAME) {\n+\t\t    column_id == COLUMN_IDENTIFIER_FILENAME) {\n \t\t\t// filename\n \t\t\treader_data.constant_map.Add(global_idx, Value(filename));\n \t\t\tcontinue;\n \t\t}\n+\t\tif (column_id == COLUMN_IDENTIFIER_FILE_INDEX) {\n+\t\t\t// filename\n+\t\t\treader_data.constant_map.Add(global_idx, Value::UBIGINT(reader_data.reader->file_list_idx.GetIndex()));\n+\t\t\tcontinue;\n+\t\t}\n \t\tif (IsVirtualColumn(column_id)) {\n \t\t\tcontinue;\n \t\t}\n---\nsrc/include/duckdb/common/multi_file/multi_file_reader.hpp\n@@ -34,6 +34,7 @@ struct MultiFileReader {\n public:\n \tstatic constexpr column_t COLUMN_IDENTIFIER_FILENAME = UINT64_C(9223372036854775808);\n \tstatic constexpr column_t COLUMN_IDENTIFIER_FILE_ROW_NUMBER = UINT64_C(9223372036854775809);\n+\tstatic constexpr column_t COLUMN_IDENTIFIER_FILE_INDEX = UINT64_C(9223372036854775810);\n \t// Reserved field id used for the \"_file\" field according to the iceberg spec (used for file_row_number)\n \tstatic constexpr int32_t ORDINAL_FIELD_ID = 2147483645;\n \t// Reserved field id used for the \"_pos\" field according to the iceberg spec (used for file_row_number)\n---\ntest/sql/copy/parquet/parquet_virtual_columns.test\n@@ -4,6 +4,26 @@\n \n require parquet\n \n+# file_index\n+query I\n+SELECT file_index FROM 'data/parquet-testing/glob/t1.parquet'\n+----\n+0\n+\n+query III\n+SELECT file_index, i, j FROM read_parquet(['data/parquet-testing/glob/t1.parquet', 'data/parquet-testing/glob/t2.parquet', 'data/parquet-testing/glob2/t1.parquet'])\n+----\n+0\t1\ta\n+1\t2\tb\n+2\t3\tc\n+\n+query III\n+SELECT file_index, i, j\n+FROM read_parquet(['data/parquet-testing/glob/t1.parquet', 'data/parquet-testing/glob/t2.parquet', 'data/parquet-testing/glob2/t1.parquet'])\n+WHERE file_index=1\n+----\n+1\t2\tb\n+\n # Filename without the filename option\n statement ok\n select filename from 'data/parquet-testing/glob/t1.parquet'\n---\n"}], "ideal": "Title: Add file_index virtual column to the multi file reader that returns the file index of the read file\n\nBody: This PR adds the `file_index` virtual column to the multi file reader. This is an index of the file into the file list of the multi file reader. For example:\r\n\r\n```sql\r\nD SELECT file_index, filename FROM read_parquet(['data/parquet-testing/glob/t1.parquet', 'data/parquet-testing/glob/t2.parquet', 'data/parquet-testing/glob2/t1.parquet']);\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 file_index \u2502               filename                \u2502\r\n\u2502   uint64   \u2502                varchar                \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502          0 \u2502 data/parquet-testing/glob/t1.parquet  \u2502\r\n\u2502          1 \u2502 data/parquet-testing/glob/t2.parquet  \u2502\r\n\u2502          2 \u2502 data/parquet-testing/glob2/t1.parquet \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/execution/operator/persistent/physical_copy_to_file.cpp\n@@ -241,6 +241,7 @@ class CopyToFunctionLocalState : public LocalSinkState {\n \t}\n \tunique_ptr<GlobalFunctionData> global_state;\n \tunique_ptr<LocalFunctionData> local_state;\n+\tidx_t total_rows_copied = 0;\n \n \t//! Buffers the tuples in partitions before writing\n \tunique_ptr<HivePartitionedColumnData> part_buffer;\n@@ -520,7 +521,7 @@ SinkResultType PhysicalCopyToFile::Sink(ExecutionContext &context, DataChunk &ch\n \t\t// if we are only writing the file when there are rows to write we need to initialize here\n \t\tg.Initialize(context.client, *this);\n \t}\n-\tg.rows_copied += chunk.size();\n+\tl.total_rows_copied += chunk.size();\n \n \tif (partition_output) {\n \t\tl.AppendToPartition(context, *this, g, chunk);\n@@ -557,6 +558,11 @@ SinkResultType PhysicalCopyToFile::Sink(ExecutionContext &context, DataChunk &ch\n SinkCombineResultType PhysicalCopyToFile::Combine(ExecutionContext &context, OperatorSinkCombineInput &input) const {\n \tauto &g = input.global_state.Cast<CopyToFunctionGlobalState>();\n \tauto &l = input.local_state.Cast<CopyToFunctionLocalState>();\n+\tif (l.total_rows_copied == 0) {\n+\t\t// no rows copied\n+\t\treturn SinkCombineResultType::FINISHED;\n+\t}\n+\tg.rows_copied += l.total_rows_copied;\n \n \tif (partition_output) {\n \t\t// flush all remaining partitions\n---\n"}], "ideal": "Title: Copy To File: avoid calling Combine for threads that have not written any rows\n\nBody: This fixes a race condition when `write_empty_file` is set"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "benchmark/micro/optimizer/topn_large_optimization.benchmark\n@@ -0,0 +1,16 @@\n+# name: benchmark/micro/optimizer/topn_large_optimization.benchmark\n+# description: Benchmark of top n optimization\n+# group: [optimizer]\n+\n+name TopN Optimization\n+group micro\n+subgroup optimizer\n+\n+load\n+CREATE TABLE integers AS SELECT range a FROM range(100000000) order by hash(a);\n+\n+run\n+select * from integers order by a limit 500;\n+select * from integers order by a limit 5000;\n+select * from integers order by a limit 50000;\n+select * from integers order by a limit 12800000;\n---\nsrc/optimizer/topn_optimizer.cpp\n@@ -29,6 +29,15 @@ bool TopN::CanOptimize(LogicalOperator &op) {\n \n \t\tauto child_op = op.children[0].get();\n \n+\t\tauto constant_limit = static_cast<double>(limit.limit_val.GetConstantValue());\n+\t\tauto child_card = static_cast<double>(child_op->estimated_cardinality);\n+\n+\t\t// if the child cardinality is not 98 times more than the\n+\t\tbool limit_is_large = constant_limit > 5000;\n+\t\tif (constant_limit > child_card * 0.007 && limit_is_large) {\n+\t\t\treturn false;\n+\t\t}\n+\n \t\twhile (child_op->type == LogicalOperatorType::LOGICAL_PROJECTION) {\n \t\t\tD_ASSERT(!child_op->children.empty());\n \t\t\tchild_op = child_op->children[0].get();\n---\ntest/optimizer/topn/topn_optimizer.test\n@@ -59,3 +59,17 @@ WITH cte1 AS (SELECT range%2000 a,\n SELECT * FROM cte3 LIMIT 4;\n ----\n logical_opt\t<REGEX>:.*TOP_N.*\n+\n+\n+statement ok\n+create table large_table as select range a from range(100000);\n+\n+query II\n+EXPLAIN select * from large_table order by a limit 100;\n+----\n+logical_opt\t<REGEX>:.*TOP_N.*\n+\n+query II\n+EXPLAIN select * from large_table order by a limit 10000;\n+----\n+logical_opt\t<!REGEX>:.*TOP_N.*\n---\n"}], "ideal": "Title: Only trigger TopN rewrite relatively small limits compared to the table size. \n\nBody: Fixes https://github.com/duckdblabs/duckdb-internal/issues/4363\r\nFixes https://github.com/duckdb/duckdb/issues/16527\r\n\r\nTopN isn't always faster. The below experiments show that as the table cardinality increases/as the limit increases, TopN performance ends up suffering from nLog(n) operations performed one tuple at a time, and sorting the whole table becomes faster. The plots show without optimization (i.e sorting the whole table) as having relatively constant performance, while TopN performance degrades as the limit increases. \r\n\r\nLooking at the plots, we can see that sorting the table is constant regardless of the limit. However, once the limit value is >= (0.007)*(table_cardinality), the topn optimization takes more time that a normal sort.\r\n\r\n![no-top_n_opt](https://github.com/user-attachments/assets/6a47e66f-1a5f-4cf7-9ae7-3826e2f5706b)\r\n\r\nThe same plot with the changes from this PR that disables the TopN optimization once the limit value is >= 0.007 * table_cardinality (and the limit is > 5000).\r\n![with_topn_opt](https://github.com/user-attachments/assets/23da04b0-5b90-4502-b5b4-b064f3162330)\r\n\r\n\r\nWith this PR, we now observe the following runtimes\r\n\r\n| Limit    | This branch | v1.2.2 |\r\n|----------|-------------|--------|\r\n| 500      | 0.029       | 0.027  |\r\n| 5000     | 0.036       | 0.040  |\r\n| 50000    | 0.103       | 0.124  |\r\n| 12800000 | 1.116       | 33.019 |\r\n\r\n\r\nCC @lnkuiper (I think you still have a patch you want to add?)"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/operator/cast_operators.cpp\n@@ -1462,7 +1462,7 @@ string_t CastFromUUID::Operation(hugeint_t input, Vector &vector) {\n //===--------------------------------------------------------------------===//\n template <>\n bool TryCastToUUID::Operation(string_t input, hugeint_t &result, Vector &result_vector, CastParameters &parameters) {\n-\treturn UUID::FromString(input.GetString(), result);\n+\treturn UUID::FromString(input.GetString(), result, parameters.strict);\n }\n \n //===--------------------------------------------------------------------===//\n---\nsrc/common/types/uuid.cpp\n@@ -6,7 +6,7 @@ namespace duckdb {\n //////////////////\n // Base UUID\n //////////////////\n-bool BaseUUID::FromString(const string &str, hugeint_t &result) {\n+bool BaseUUID::FromString(const string &str, hugeint_t &result, bool strict) {\n \tauto hex2char = [](char ch) -> unsigned char {\n \t\tif (ch >= '0' && ch <= '9') {\n \t\t\treturn UnsafeNumericCast<unsigned char>(ch - '0');\n@@ -34,6 +34,17 @@ bool BaseUUID::FromString(const string &str, hugeint_t &result) {\n \t\treturn false;\n \t}\n \n+\tif (strict) {\n+\t\t// 32 characters and 4 hyphens\n+\t\tif (str.length() != 36) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tconst auto c_str = str.c_str();\n+\t\tif (c_str[8] != '-' || c_str[13] != '-' || c_str[18] != '-' || c_str[23] != '-') {\n+\t\t\treturn false;\n+\t\t}\n+\t}\n+\n \tresult.lower = 0;\n \tresult.upper = 0;\n \tsize_t count = 0;\n---\nsrc/include/duckdb/common/types/uuid.hpp\n@@ -21,7 +21,7 @@ class BaseUUID {\n public:\n \tconstexpr static const uint8_t STRING_SIZE = 36;\n \t//! Convert a uuid string to a hugeint object\n-\tstatic bool FromString(const string &str, hugeint_t &result);\n+\tstatic bool FromString(const string &str, hugeint_t &result, bool strict = false);\n \t//! Convert a uuid string to a hugeint object\n \tstatic bool FromCString(const char *str, idx_t len, hugeint_t &result) {\n \t\treturn FromString(string(str, 0, len), result);\n---\ntest/sql/json/issues/issue16684.test\n@@ -0,0 +1,26 @@\n+# name: test/sql/json/issues/issue16684.test\n+# description: Test issue 16684 - When using read_json to read data, it always converts the md5 string to uuid format.\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+copy (select '00000000000000000000000000000000' md5) to '__TEST_DIR__/issue16684.json'\n+\n+# should be varchar, not uuid (no hyphens)\n+query II\n+select md5, typeof(md5) from '__TEST_DIR__/issue16684.json'\n+----\n+00000000000000000000000000000000\tVARCHAR\n+\n+statement ok\n+copy (select '00000000-0000-0000-0000-000000000000' id) to '__TEST_DIR__/issue16684.json'\n+\n+# if we add hyphens we get a uuid\n+query II\n+select id, typeof(id) from '__TEST_DIR__/issue16684.json'\n+----\n+00000000-0000-0000-0000-000000000000\tUUID\n---\n"}], "ideal": "Title: Strict UUID cast\n\nBody: Requires hyphens to be present for the cast to succeed. Fixes #16684"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "CMakeLists.txt\n@@ -100,6 +100,10 @@ if(${FORCE_COLORED_OUTPUT})\n   endif()\n endif()\n \n+if (DUCKDB_EXPLICIT_PLATFORM)\n+    add_definitions(-DDUCKDB_CUSTOM_PLATFORM=${DUCKDB_EXPLICIT_PLATFORM})\n+endif()\n+\n option (WASM_ENABLED \"Are DuckDB-Wasm extensions build enabled\" FALSE)\n if (DEFINED ENV{WASM_EXTENSIONS})\n      set(WASM_ENABLED \"$ENV{WASM_EXTENSIONS}\")\n---\nsrc/include/duckdb/common/platform.hpp\n@@ -37,7 +37,7 @@ namespace duckdb {\n std::string DuckDBPlatform() { // NOLINT: allow definition in header\n #if defined(DUCKDB_CUSTOM_PLATFORM)\n \treturn DUCKDB_QUOTE_DEFINE(DUCKDB_CUSTOM_PLATFORM);\n-#endif\n+#else\n #if defined(DUCKDB_WASM_VERSION)\n \t// DuckDB-Wasm requires CUSTOM_PLATFORM to be defined\n \tstatic_assert(0, \"DUCKDB_WASM_VERSION should rely on CUSTOM_PLATFORM being provided\");\n@@ -84,6 +84,7 @@ std::string DuckDBPlatform() { // NOLINT: allow definition in header\n \tpostfix = \"_mingw\";\n #endif\n \treturn os + \"_\" + arch + postfix;\n+#endif\n }\n \n } // namespace duckdb\n---\n"}], "ideal": "Title: platform.hpp: Propagate DUCKDB_EXPLICIT_PLATFORM, avoid early return\n\nBody: Two fixes emerged after https://github.com/duckdb/duckdb/pull/16956:\r\n1. add remapping between DUCKDB_EXPLICIT_PLATFORM and DUCKDB_CUSTOM_PLATFORM, this might use a clean-up, I think only one should be necessary, but starting with baby steps\r\n2. preprocessor logic do not compose with early returns (!)\r\n\r\nRaised by @Tishj in https://github.com/duckdb/duckdb-avro/pull/21, but this is relevant in general since it meant there was actually no way to short circuit the erorr.\r\n\r\nNow that this is in place, I would consider expanding the detection so that behaviour is uniform between unsupported platforms (eg. also `__FreeBSD__` and other might just error out at compile time if platform is not explicitly provided)."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/transaction/duck_transaction_manager.hpp\n@@ -20,7 +20,7 @@ struct UndoBufferProperties;\n //! CleanupInfo collects transactions awaiting cleanup.\n //! This ensures we can clean up after releasing the transaction lock.\n struct DuckCleanupInfo {\n-\t//! All transaction in a cleanup info share the same lowest_start_time.\n+\t//! All transactions in a cleanup info share the same lowest_start_time.\n \ttransaction_t lowest_start_time;\n \tvector<unique_ptr<DuckTransaction>> transactions;\n \n@@ -123,9 +123,14 @@ class DuckTransactionManager : public TransactionManager {\n \tatomic<idx_t> last_uncommitted_catalog_version = {TRANSACTION_ID_START};\n \tidx_t last_committed_version = 0;\n \n+\t//! Only one cleanup can be active at any time.\n \tmutex cleanup_lock;\n-\tqueue<unique_ptr<DuckCleanupInfo>> cleanup_queue;\n+\t//! Changes to the cleanup queue must be synchronized.\n \tmutex cleanup_queue_lock;\n+\t//! Cleanups have to happen in-order.\n+\t//! E.g., if one transaction drops a table, and another creates a table,\n+\t//! inverting the cleanup order can result in catalog errors.\n+\tqueue<unique_ptr<DuckCleanupInfo>> cleanup_queue;\n \n protected:\n \tvirtual void OnCommitCheckpointDecision(const CheckpointDecision &decision, DuckTransaction &transaction) {\n---\nsrc/storage/data_table.cpp\n@@ -365,7 +365,6 @@ void DataTable::VerifyIndexBuffers() {\n \t});\n }\n \n-// Lock?\n void DataTable::CleanupAppend(transaction_t lowest_transaction, idx_t start, idx_t count) {\n \trow_groups->CleanupAppend(lowest_transaction, start, count);\n }\n---\n"}], "ideal": "Title: Remove misleading lock comment in data table\n\nBody: Just a small follow-up to the review comment in https://github.com/duckdb/duckdb/pull/17034."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "scripts/list_vcpkg_registry_packages.py\n@@ -0,0 +1,33 @@\n+import argparse\n+import requests\n+\n+parser = argparse.ArgumentParser(description='Generate the list of packages provided by the registry at <baseline>.')\n+parser.add_argument(\n+    '--baseline',\n+    action='store',\n+    help='The baseline (git commit) of the vcpkg-duckdb-ports',\n+    required=True,\n+)\n+args = parser.parse_args()\n+\n+GITHUB_API = \"https://api.github.com/repos/duckdb/vcpkg-duckdb-ports/git/trees\"\n+\n+\n+def main():\n+    # Get the tree recursively for the commit\n+    response = requests.get(f\"{GITHUB_API}/{args.baseline}?recursive=1\")\n+    response.raise_for_status()\n+\n+    # Extract package names from ports directory\n+    packages = set()\n+    for item in response.json()['tree']:\n+        path = item['path']\n+        if path.startswith('ports/'):\n+            parts = path.split('/')\n+            if len(parts) > 2:\n+                packages.add(parts[1])\n+    print(sorted(list(packages)))\n+\n+\n+if __name__ == '__main__':\n+    main()\n---\nscripts/merge_vcpkg_deps.py\n@@ -57,14 +57,27 @@ def prefix_overlay_port(overlay_port):\n         # dedup_set.add(dep)\n \n data = {\n-    \"description\": f\"Auto-generated vcpkg.json for combined DuckDB extension build\",\n+    \"description\": f\"Auto-generated vcpkg.json for combined DuckDB extension build, generated by 'scripts/merge_vcpkg_deps.py'\",\n     \"builtin-baseline\": \"5e5d0e1cd7785623065e77eff011afdeec1a3574\",\n     \"dependencies\": final_deduplicated_deps,\n     \"overrides\": [{\"name\": \"openssl\", \"version\": \"3.0.8\"}],\n }\n \n if merged_overlay_ports:\n     data['vcpkg-configuration'] = {'overlay-ports': merged_overlay_ports}\n+else:\n+    data['vcpkg-configuration'] = {}\n+\n+REGISTRY_BASELINE = '0f9bf648ba1ee29291890a1ca9a49a80bba017eb'\n+# NOTE: use 'scripts/list_vcpkg_registry_packages.py --baseline <baseline>' to generate the list of packages\n+data['vcpkg-configuration']['registries'] = [\n+    {\n+        \"kind\": \"git\",\n+        \"repository\": \"https://github.com/duckdb/vcpkg-duckdb-ports\",\n+        \"baseline\": REGISTRY_BASELINE,\n+        \"packages\": ['avro-c', 'vcpkg-cmake'],\n+    }\n+]\n \n # Print output\n print(\"Writing to 'build/extension_configuration/vcpkg.json': \")\n---\n"}], "ideal": "Title: [Dev] Add \"registries\" to `vcpkg.json`, add script to list the packages of the registry.\n\nBody: This PR adds <https://github.com/duckdb/vcpkg-duckdb-ports> to the generated vcpkg.json file.\r\nThis serves as an alternative to the copy-and-pasted ports we had previously.\r\n\r\nAll of the ports currently in (out of tree) extensions should move to the registry, this avoids needing to copy and paste the port when another extension depends on the same dependency, or even depends on a different extension that has a port."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "scripts/extension-upload-from-nightly.sh\n@@ -28,10 +28,10 @@ CLOUDFRONT_DISTRIBUTION_ID=E2Z28NDMI4PVXP\n \n ### COPY THE FILES\n ## REAL_RUN is to be used to move non-Wasm extensions\n-REAL_RUN=\"aws s3 cp s3://$FROM_BUCKET/$BASE_NIGHTLY_DIR s3://$TO_BUCKET/$2 --recursive --exclude '*' --include '*/$1.duckdb_extension.gz' --acl public-read\"\n+REAL_RUN=\"aws s3 cp s3://$FROM_BUCKET/$BASE_NIGHTLY_DIR s3://$TO_BUCKET/$2 --recursive --exclude '*' --include '*/$1.duckdb_extension.gz' --acl public-read --region us-east-2\"\n DRY_RUN=\"$REAL_RUN --dryrun\"\n ## REAL_RUN_WASM is to be used to move Wasm extensions to new style path (no extra duckdb-wasm)\n-REAL_RUN_WASM=\"aws s3 cp s3://$FROM_BUCKET/$BASE_NIGHTLY_DIR s3://$TO_BUCKET/$2 --recursive --exclude '*' --include '*/$1.duckdb_extension.wasm' --acl public-read --content-encoding br --content-type='application/wasm'\"\n+REAL_RUN_WASM=\"aws s3 cp s3://$FROM_BUCKET/$BASE_NIGHTLY_DIR s3://$TO_BUCKET/$2 --recursive --exclude '*' --include '*/$1.duckdb_extension.wasm' --acl public-read --content-encoding br --content-type='application/wasm' --region us-east-2\"\n DRY_RUN_WASM=\"$REAL_RUN_WASM --dryrun\"\n \n if [ \"$DUCKDB_DEPLOY_SCRIPT_MODE\" == \"for_real\" ]; then\n---\n"}], "ideal": "Title: extension-upload-from-nightly.sh: Add --region\n\nBody: Currently the script assumes the region is inherited from the environment.\r\n\r\nIt seems better to just set it to the correct region."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/json/include/json_common.hpp\n@@ -205,12 +205,15 @@ struct JSONCommon {\n \n \tstatic string FormatParseError(const char *data, idx_t length, yyjson_read_err &error, const string &extra = \"\") {\n \t\tD_ASSERT(error.code != YYJSON_READ_SUCCESS);\n+\t\t// Go to blob so we can have a better error message for weird strings\n+\t\tauto blob = Value::BLOB(string(data, length));\n \t\t// Truncate, so we don't print megabytes worth of JSON\n-\t\tstring input = length > 50 ? string(data, 47) + \"...\" : string(data, length);\n+\t\tstring input = blob.ToString();\n+\t\tinput = input.length() > 50 ? string(input.c_str(), 47) + \"...\" : input;\n \t\t// Have to replace \\r, otherwise output is unreadable\n \t\tinput = StringUtil::Replace(input, \"\\r\", \"\\\\r\");\n-\t\treturn StringUtil::Format(\"Malformed JSON at byte %lld of input: %s. %s Input: %s\", error.pos, error.msg, extra,\n-\t\t                          input);\n+\t\treturn StringUtil::Format(\"Malformed JSON at byte %lld of input: %s. %s Input: \\\"%s\\\"\", error.pos, error.msg,\n+\t\t                          extra, input);\n \t}\n \tstatic void ThrowParseError(const char *data, idx_t length, yyjson_read_err &error, const string &extra = \"\") {\n \t\tthrow InvalidInputException(FormatParseError(data, length, error, extra));\n---\nextension/json/json_functions/json_create.cpp\n@@ -547,8 +547,6 @@ static void CreateValues(const StructNames &names, yyjson_mut_doc *doc, yyjson_m\n \tcase LogicalTypeId::DOUBLE:\n \t\tTemplatedCreateValues<double, double>(doc, vals, value_v, count);\n \t\tbreak;\n-\tcase LogicalTypeId::BIT:\n-\tcase LogicalTypeId::BLOB:\n \tcase LogicalTypeId::VARCHAR:\n \t\tTemplatedCreateValues<string_t, string_t>(doc, vals, value_v, count);\n \t\tbreak;\n@@ -567,6 +565,8 @@ static void CreateValues(const StructNames &names, yyjson_mut_doc *doc, yyjson_m\n \tcase LogicalTypeId::ARRAY:\n \t\tCreateValuesArray(names, doc, vals, value_v, count);\n \t\tbreak;\n+\tcase LogicalTypeId::BIT:\n+\tcase LogicalTypeId::BLOB:\n \tcase LogicalTypeId::AGGREGATE_STATE:\n \tcase LogicalTypeId::ENUM:\n \tcase LogicalTypeId::DATE:\n---\nextension/json/json_functions/read_json.cpp\n@@ -188,7 +188,8 @@ void JSONScan::AutoDetect(ClientContext &context, MultiFileBindData &bind_data,\n \t}\n \t// set the max threads/estimated per-file cardinality\n \tif (auto_detect_state.files_scanned > 0 && auto_detect_state.tuples_scanned > 0) {\n-\t\tauto average_tuple_size = auto_detect_state.bytes_scanned / auto_detect_state.tuples_scanned;\n+\t\tauto average_tuple_size =\n+\t\t    MaxValue<idx_t>(auto_detect_state.bytes_scanned / auto_detect_state.tuples_scanned, 1);\n \t\tjson_data.estimated_cardinality_per_file = auto_detect_state.total_file_size / average_tuple_size;\n \t\tif (auto_detect_state.files_scanned == 1) {\n \t\t\tjson_data.max_threads =\n---\ntest/sql/json/issues/issue16968.test\n@@ -0,0 +1,28 @@\n+# name: test/sql/json/issues/issue16968.test\n+# description: Test issue 16968 - A not descriptive error message when value of BLOB is passed to JSON function\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+pragma enable_verification\n+\n+query I\n+select from_hex('aa')::json;\n+----\n+\"\\\\xAA\"\n+\n+query I\n+select json(from_hex('aa'));\n+----\n+\"\\\\xAA\"\n+\n+query I\n+select '1101'::BIT::JSON\n+----\n+\"1101\"\n+\n+query I\n+select json('1101'::BIT)\n+----\n+\"1101\"\n---\n"}], "ideal": "Title: JSON Bugfixes\n\nBody: Fixes:\r\n * https://github.com/duckdblabs/duckdb-internal/issues/4401\r\n * https://github.com/duckdblabs/duckdb-internal/issues/4572\r\n * https://github.com/duckdblabs/duckdb-internal/issues/4494"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/InvokeCI.yml\n@@ -87,9 +87,8 @@ jobs:\n       git_ref: ${{ inputs.git_ref }}\n       skip_tests: ${{ inputs.skip_tests }}\n \n-  notify-external-repos-main:\n-    uses: ./.github/workflows/NotifyExternalRepositories.yml\n-    secrets: inherit\n+  prepare-status:\n+    runs-on: ubuntu-latest\n     needs:\n       - osx\n       - linux-release\n@@ -99,24 +98,46 @@ jobs:\n       - R\n       - Wasm\n       - static-libraries\n-    if: ${{ inputs.git_ref == '' }}\n+    outputs:\n+      is-success: ${{ steps.set-output.outputs.success }}\n+    steps:\n+      - id: set-output\n+        shell: bash\n+        run:\n+          if [[ \"${{ needs.osx.result }}\" == \"success\" && \\\n+                \"${{ needs.linux-release.result }}\" == \"success\" && \\\n+                \"${{ needs.linux-release.result }}\" == \"success\" && \\\n+                \"${{ needs.windows.result }}\" == \"success\" && \\\n+                \"${{ needs.python.result }}\" == \"success\" && \\\n+                \"${{ needs.pyodide.result }}\" == \"success\" && \\\n+                \"${{ needs.R.result }}\" == \"success\" && \\\n+                \"${{ needs.Wasm.result }}\" == \"success\" && \\\n+                \"${{ needs.static-libraries.result }}\" == \"success\" ]]; then\n+            echo \"success=true\" >> $GITHUB_OUTPUT\n+          else\n+            echo \"success=false\" >> $GITHUB_OUTPUT\n+          fi\n+    \n+  notify-external-repos-main:\n+    uses: ./.github/workflows/NotifyExternalRepositories.yml\n+    secrets: inherit\n+    needs: prepare-status  \n+    if: ${{ inputs.git_ref == '' && always() }} \n     with:\n+      is-success: ${{ needs.prepare-status.outputs.is-success }}\n       target-branch: ${{ github.ref_name }}\n       duckdb-sha: ${{ github.ref }}\n+      triggering-event: ${{ github.event_name }}\n+      should-publish: true\n \n   notify-specific-branch-on-external-repos:\n     uses: ./.github/workflows/NotifyExternalRepositories.yml\n     secrets: inherit\n-    needs:\n-      - osx\n-      - linux-release\n-      - windows\n-      - python\n-      - pyodide\n-      - R\n-      - Wasm\n-      - static-libraries\n-    if: ${{ inputs.git_ref != '' }}\n+    needs: prepare-status\n+    if: ${{ inputs.git_ref != '' && always() }}\n     with:\n+      is-success: ${{ needs.prepare-status.outputs.is-success }}\n       target-branch: ${{ github.ref_name }}\n-      duckdb-sha: ${{ inputs.git_ref }}\n\\ No newline at end of file\n+      duckdb-sha: ${{ inputs.git_ref }}\n+      triggering-event: ${{ github.event_name }}\n+      should-publish: true\n\\ No newline at end of file\n---\n.github/workflows/NotifyExternalRepositories.yml\n@@ -12,6 +12,18 @@ on:\n         required: true\n         default: ''\n         type: 'string'\n+      triggering-event:\n+        description: 'Which event triggered the run'\n+        default: ''\n+        type: 'string'\n+      should-publish:\n+        description: 'Should the called workflow push updates or not'\n+        default: false\n+        type: 'boolean'\n+      is-success:\n+        description: 'True, if all the builds in InvokeCI had succeeded'\n+        default: false\n+        type: 'boolean'\n   workflow_dispatch:\n     inputs:\n       duckdb-sha:\n@@ -24,6 +36,18 @@ on:\n         required: true\n         default: ''\n         type: 'string'\n+      triggering-event:\n+        description: 'Which event triggered the run'\n+        default: ''\n+        type: 'string'\n+      should-publish:\n+        description: 'Should the called workflow push updates'\n+        default: false\n+        type: 'boolean'\n+      is-success:\n+        description: 'True, if all the builds in InvokeCI had succeeded'\n+        default: false\n+        type: 'boolean'\n \n concurrency:\n   group: ${{ github.workflow }}\n@@ -33,6 +57,7 @@ jobs:\n   notify-odbc-run:\n     name: Run ODBC Vendor\n     runs-on: ubuntu-latest\n+    if: ${{ inputs.is-success }}\n     env:\n       PAT_USER: ${{ secrets.PAT_USERNAME }}\n       PAT_TOKEN: ${{ secrets.PAT_TOKEN }}\n@@ -56,5 +81,5 @@ jobs:\n         if: ${{ github.repository == 'duckdb/duckdb' }}\n         run: |\n           export URL=https://api.github.com/repos/duckdb/duckdb-build-status/actions/workflows/NightlyBuildsCheck.yml/dispatches\n-          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"duckdb-sha\": \"${{ inputs.duckdb-sha }}\"}}'\n+          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"branch\": \"${{ inputs.target-branch }}\", \"event\": \"${{ inputs.triggering_event }}\", \"should_publish\": \"${{ inputs.should_publish }}\"}}'\n           curl -v -XPOST -u \"${PAT_USER}:${PAT_TOKEN}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" $URL --data \"$DATA\"\n---\n"}], "ideal": "Title: Notify nightly build status\n\nBody: The idea is to trigger the `NightlyBuildsStatus.yml` in the end of the `InvokeCI.yml` run.\r\nThere is a `NotifyExternalRepositories.yml` workflow for triggering workflow run on the external repos - we just need to pass there all the inputs needed by `NightlyBuildsStatus.yml` run (branch name, event name, should the status report be published or not).\r\n\r\nThe `NotifyExternalRepositories.yml` triggers two workflows: [duckdb-odbc](https://github.com/duckdb/duckdb-odbc/tree/main)/[.github](https://github.com/duckdb/duckdb-odbc/tree/main/.github)/[workflows](https://github.com/duckdb/duckdb-odbc/tree/main/.github/workflows)/`Vendor.yml` and [duckdb-build-status](https://github.com/duckdb/duckdb-build-status/tree/main)/[.github](https://github.com/duckdb/duckdb-build-status/tree/main/.github)/[workflows](https://github.com/duckdb/duckdb-build-status/tree/main/.github/workflows)`/NightlyBuildsCheck.yml` workflows.\r\n`Vendor.yml` should be triggered only when all builds had succeeded.\r\n`NightlyBuildsStatus.yml` should run in any case. \r\n\r\nSo we need to add one more job to `InvokeCI.yml` to create `is-success` status, which is `true` only when **all** builds had succeeded. Then `is-success` status should be passed to the `NotifyExternalRepositories.yml` and its value should be checked before the `Vendor.yml` workflow run is called. We should add `always()` condition to the \"notifying\" jobs `if:` conditions - that makes them not get skipped in case there a builds not succeeded ([tested here](https://github.com/hmeriann/trigger_nightly_check/actions/runs/14446186717))."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/Python.yml\n@@ -62,9 +62,75 @@ env:\n   CIBW_TEST_SKIP: ${{ inputs.skip_tests == 'true' && '*-*' || '{cp37,cp38}-*' }}\n \n jobs:\n+# This is just a sanity check of Python 3.10 running with Arrow / Spark\n+  linux-python3-10:\n+    name: Python 3.10 Linux\n+    runs-on: ubuntu-22.04\n+\n+    env:\n+      CIBW_BUILD: 'cp310-*'\n+      CIBW_ARCHS: 'x86_64'\n+      CIBW_TEST_COMMAND: 'python -m pytest {project}/tests --verbose'\n+      # FIXME: add this back to CIBW_TEST_COMMAND '&& USE_ACTUAL_SPARK=true JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java)))) SPARK_HOME={project}/spark_installation/spark python -m pytest {project}/tests/fast/spark --verbose'\n+      PYTEST_TIMEOUT: '600'\n+      CIBW_ENVIRONMENT: 'OVERRIDE_GIT_DESCRIBE=${{ inputs.override_git_describe }}'\n+      CIBW_BEFORE_ALL: 'cd {project} && ./scripts/install_spark_in_cibuildwheels_linux_container.sh'\n+      CIBW_MANYLINUX_X86_64_IMAGE: 'manylinux_2_28'\n+      CIBW_MANYLINUX_PYPY_X86_64_IMAGE: 'manylinux_2_28'\n+\n+    steps:\n+    - uses: actions/checkout@v4\n+      with:\n+        fetch-depth: 0\n+        ref: ${{ inputs.git_ref }}\n+\n+    - name: Check/Act on inputs.override_git_describe\n+      shell: bash\n+      run: |\n+        if [[ \"${{ inputs.override_git_describe }}\" == *-* ]]; then\n+            echo \"override_git_describe ${{ inputs.override_git_describe }}: provide either vX.Y.Z or empty string\"\n+            exit 1\n+        elif [[ -z \"${{ inputs.override_git_describe }}\" ]]; then\n+            echo \"No override_git_describe provided\"\n+        else\n+            echo \"UPLOAD_ASSETS_TO_STAGING_TARGET=$(git log -1 --format=%h)\" >> \"$GITHUB_ENV\"\n+            echo \"override_git_describe ${{ inputs.override_git_describe }}: add tag\"\n+            git tag ${{ inputs.override_git_describe }}\n+        fi\n+\n+    - uses: actions/setup-python@v5\n+      with:\n+        python-version: '3.11'\n+\n+    - name: Install\n+      shell: bash\n+      run: pip install 'cibuildwheel>=2.16.2' build\n+\n+    - name: Setup Ccache\n+      uses: hendrikmuhs/ccache-action@main\n+      with:\n+        key: ${{ github.job }}\n+        save: ${{ github.ref == 'refs/heads/main' || github.repository != 'duckdb/duckdb' }}\n+\n+    - name: Build source dist\n+      shell: bash\n+      working-directory: tools/pythonpkg\n+      run: |\n+        pyproject-build . --sdist\n+        mkdir duckdb_tarball && tar xvf dist/duckdb-*.tar.gz --strip-components=1 -C duckdb_tarball\n+\n+    - name: Build\n+      shell: bash\n+      working-directory: tools/pythonpkg\n+      run: |\n+        export DISTUTILS_C_COMPILER_LAUNCHER=ccache\n+        # TODO: Use ccache inside container, see https://github.com/pypa/cibuildwheel/issues/1030\n+        cibuildwheel --output-dir wheelhouse --config-file pyproject.toml duckdb_tarball\n+\n   linux-python3:\n     name: Python 3 Linux\n     runs-on: ubuntu-22.04\n+    needs: linux-python3-10\n     strategy:\n       fail-fast: false\n       matrix:\n@@ -132,7 +198,6 @@ jobs:\n         pip install 'cibuildwheel>=2.16.2' build\n         python -m pip install numpy --config-settings=setup-args=\"-Dallow-noblas=true\"\n \n-\n     - name: Setup Ccache\n       uses: hendrikmuhs/ccache-action@v1.2.11 # Note: pinned due to GLIBC incompatibility in later releases\n       with:\n@@ -167,6 +232,7 @@ jobs:\n   osx-python3:\n       if: startsWith(github.ref, 'refs/tags/v') || github.ref == 'refs/heads/main' || inputs.run_all == 'true'\n       name: Python 3 OSX\n+      needs: linux-python3-10\n       runs-on: macos-latest\n       strategy:\n        matrix:\n@@ -229,6 +295,7 @@ jobs:\n   win-python3:\n       name: Python 3 Windows\n       runs-on: windows-2019\n+      needs: linux-python3-10\n       strategy:\n        matrix:\n         python_build: [cp39-*, cp310-*, cp311-*, cp312-*, cp313-*]\n---\n"}], "ideal": "Title: Python.yml: Add back logic to perform fast-fail on Python 3.10\n\nBody: This adds back a initial step to validate stuff actually compiles, before launching N parallel jobs.\r\n\r\nSpark compatibility checks should also been here, but have not found a simple way to have them working, someone else can have a look."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "extension/json/json_functions/read_json.cpp\n@@ -150,42 +150,32 @@ void JSONScan::AutoDetect(ClientContext &context, MultiFileBindData &bind_data,\n \tJSONStructureNode node;\n \tauto &options = json_data.options;\n \tauto files = bind_data.file_list->GetAllFiles();\n-\tauto file_count = files.size();\n+\tauto file_count = bind_data.file_options.union_by_name\n+\t                      ? files.size()\n+\t                      : MinValue<idx_t>(options.maximum_sample_files, files.size());\n \tbind_data.union_readers.resize(files.empty() ? 0 : files.size());\n \n \tAutoDetectState auto_detect_state(context, bind_data, files, date_format_map);\n-\tif (bind_data.file_options.union_by_name) {\n-\t\tconst auto num_threads = NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads());\n-\t\tconst auto files_per_task = (file_count + num_threads - 1) / num_threads;\n-\t\tconst auto num_tasks = file_count / files_per_task;\n-\t\tvector<JSONStructureNode> task_nodes(num_tasks);\n-\n-\t\t// Same idea as in union_by_name.hpp\n-\t\tTaskExecutor executor(context);\n-\t\tfor (idx_t task_idx = 0; task_idx < num_tasks; task_idx++) {\n-\t\t\tconst auto file_idx_start = task_idx * files_per_task;\n-\t\t\tauto task = make_uniq<JSONSchemaTask>(executor, auto_detect_state, task_nodes[task_idx], file_idx_start,\n-\t\t\t                                      file_idx_start + files_per_task);\n-\t\t\texecutor.ScheduleTask(std::move(task));\n-\t\t}\n-\t\texecutor.WorkOnTasks();\n+\tconst auto num_threads = NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads());\n+\tconst auto files_per_task = (file_count + num_threads - 1) / num_threads;\n+\tconst auto num_tasks = file_count / files_per_task;\n+\tvector<JSONStructureNode> task_nodes(num_tasks);\n+\n+\t// Same idea as in union_by_name.hpp\n+\tTaskExecutor executor(context);\n+\tfor (idx_t task_idx = 0; task_idx < num_tasks; task_idx++) {\n+\t\tconst auto file_idx_start = task_idx * files_per_task;\n+\t\tauto task = make_uniq<JSONSchemaTask>(executor, auto_detect_state, task_nodes[task_idx], file_idx_start,\n+\t\t                                      file_idx_start + files_per_task);\n+\t\texecutor.ScheduleTask(std::move(task));\n+\t}\n+\texecutor.WorkOnTasks();\n \n-\t\t// Merge task nodes into one\n-\t\tfor (auto &task_node : task_nodes) {\n-\t\t\tJSONStructure::MergeNodes(node, task_node);\n-\t\t}\n-\t} else {\n-\t\tArenaAllocator allocator(BufferAllocator::Get(context));\n-\t\tVector string_vector(LogicalType::VARCHAR);\n-\t\tidx_t remaining = options.sample_size;\n-\t\tfor (idx_t file_idx = 0; file_idx < file_count; file_idx++) {\n-\t\t\tremaining =\n-\t\t\t    JSONSchemaTask::ExecuteInternal(auto_detect_state, node, file_idx, allocator, string_vector, remaining);\n-\t\t\tif (remaining == 0 || file_idx == options.maximum_sample_files - 1) {\n-\t\t\t\tbreak; // We sample sample_size in total (across the first maximum_sample_files files)\n-\t\t\t}\n-\t\t}\n+\t// Merge task nodes into one\n+\tfor (auto &task_node : task_nodes) {\n+\t\tJSONStructure::MergeNodes(node, task_node);\n \t}\n+\n \t// set the max threads/estimated per-file cardinality\n \tif (auto_detect_state.files_scanned > 0 && auto_detect_state.tuples_scanned > 0) {\n \t\tauto average_tuple_size = auto_detect_state.bytes_scanned / auto_detect_state.tuples_scanned;\n---\ntest/sql/json/table/json_multi_file_reader.test\n@@ -145,12 +145,16 @@ order by j;\n statement ok\n SELECT * FROM read_json_auto(['data/json/with_uuid.json', 'data/json/example_n.ndjson'])\n \n-# both have 5 rows, so if we set sample_size=1, we cannot read them together anymore, because we only sample 1 file\n+# both have 5 rows, so if we set sample_size=1, and maximum_sample_files=1, we cannot read them together anymore\n statement error\n-SELECT * FROM read_json_auto(['data/json/with_uuid.json', 'data/json/example_n.ndjson'], sample_size=1)\n+SELECT * FROM read_json_auto(['data/json/with_uuid.json', 'data/json/example_n.ndjson'], sample_size=1, maximum_sample_files=1)\n ----\n Invalid Input Error\n \n+# if we increase maximum_sample_files, or set union_by_name=true, then we can read them again\n+statement ok\n+SELECT * FROM read_json_auto(['data/json/with_uuid.json', 'data/json/example_n.ndjson'], sample_size=1, maximum_sample_files=99)\n+\n # if we set union_by_name=true, then we sample sample_size rows per file, so then we can read them again\n statement ok\n SELECT * FROM read_json_auto(['data/json/with_uuid.json', 'data/json/example_n.ndjson'], sample_size=1, union_by_name=true)\n---\n"}], "ideal": "Title: Always parallelize `read_json` schema detection\n\nBody: We already implemented this for `union_by_name=true`, but it makes sense to just always do this, as we may read multiple files during the bind to infer the schema.\r\n\r\nFixes https://github.com/duckdblabs/duckdb-internal/issues/4225"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/NightlyTests.yml\n@@ -65,14 +65,12 @@ jobs:\n \n   release-assert:\n     name: Release Assertions\n-    runs-on: ubuntu-22.04\n+    runs-on: ubuntu-latest\n     needs: linux-memory-leaks\n     env:\n-      CC: gcc-10\n-      CXX: g++-10\n       GEN: ninja\n       BUILD_JEMALLOC: 1\n-      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet\"\n+      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet;httpfs\"\n       DISABLE_SANITIZER: 1\n       CRASH_ON_ASSERT: 1\n       RUN_SLOW_VERIFIERS: 1\n@@ -107,7 +105,7 @@ jobs:\n     needs: linux-memory-leaks\n     env:\n       GEN: ninja\n-      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet\"\n+      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet;httpfs\"\n       DISABLE_SANITIZER: 1\n       CRASH_ON_ASSERT: 1\n       RUN_SLOW_VERIFIERS: 1\n@@ -121,7 +119,7 @@ jobs:\n         python-version: '3.12'\n \n     - name: Install Ninja\n-      run: brew install ninja\n+      run: brew install ninja llvm\n \n     - name: Setup Ccache\n       uses: hendrikmuhs/ccache-action@main\n@@ -131,7 +129,7 @@ jobs:\n \n     - name: Build\n       shell: bash\n-      run: UNSAFE_NUMERIC_CAST=1 make relassert\n+      run: CMAKE_LLVM_PATH='/opt/homebrew/opt/llvm' UNSAFE_NUMERIC_CAST=1 make relassert\n \n     - name: Test\n       shell: bash\n@@ -211,33 +209,28 @@ jobs:\n       run: |\n           build/release/test/unittest \"*\"\n \n-  linux-clang:\n-    name: Clang 14\n-    runs-on: ubuntu-22.04\n+  release-assert-clang:\n+    name: Release Assertions with Clang\n+    runs-on: ubuntu-latest\n     needs: linux-memory-leaks\n     env:\n-      CC: /home/runner/work/llvm/bin/clang\n-      CPP: /home/runner/work/llvm/bin/clang-cpp\n-      CXX: /home/runner/work/llvm/bin/clang++\n-      LD: /home/runner/work/llvm/bin/ld.lld\n-      EXTENSION_STATIC_BUILD: 1\n-      CORE_EXTENSIONS: \"json\"\n-      TREAT_WARNINGS_AS_ERRORS: 1\n+      CC: clang\n+      CXX: clang++\n+      GEN: ninja\n+      BUILD_JEMALLOC: 1\n+      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet;httpfs\"\n+      DISABLE_SANITIZER: 1\n+      CRASH_ON_ASSERT: 1\n+      RUN_SLOW_VERIFIERS: 1\n \n     steps:\n     - uses: actions/checkout@v3\n       with:\n         fetch-depth: 0\n \n-    - name: Install LLVM and Clang\n-      uses: KyleMayes/install-llvm-action@v1\n-      with:\n-        version: \"14.0\"\n-        directory: '/home/runner/work/llvm'\n-\n-    - uses: actions/setup-python@v5\n-      with:\n-        python-version: '3.12'\n+    - name: Install\n+      shell: bash\n+      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build llvm\n \n     - name: Setup Ccache\n       uses: hendrikmuhs/ccache-action@main\n@@ -247,11 +240,12 @@ jobs:\n \n     - name: Build\n       shell: bash\n-      run: make release\n+      run: UNSAFE_NUMERIC_CAST=1 make relassert\n \n     - name: Test\n       shell: bash\n-      run: make allunit\n+      run: |\n+          python3 scripts/run_tests_one_by_one.py build/relassert/test/unittest \"*\" --no-exit --timeout 1200\n \n   linux-compile-32:\n     name: Linux (32 Bit)\n---\n"}], "ideal": "Title: Refactor relassert runs, adding some variations in compiler / statically linked extensions\n\nBody: Cleans up various `relassert` runs, varying things a bit so that ideally different scenarios are stress tested.\r\n\r\nAfter this:\r\n* \"Release Assertions\" runs on `ubuntu-latest`, using default compiler (at this moment `GNU 13.3.0`) and tests also `httpfs`\r\n* \"Release Assertions with Clang\" runs on `ubuntu-latest`, using latest default LLVM from apt (that is `Clang 18.1.3`)\r\n* \"Release Assertions OSX\" runs on `macos-latest`, using default LLVM from brew (`Clang 19.1.7`) and tests also `httpfs`\r\n* \"Release Assertions OSX Storage\" runs on `macos-latest` uses default compiler (`AppleClang 15.0.0.15000309 `) and tests with `--force-storage`\r\n\r\nNote that there are some more failures, but this I would say it's good, and this is a workflow where this is OK."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/common/virtual_file_system.cpp\n@@ -9,27 +9,27 @@ VirtualFileSystem::VirtualFileSystem() : default_fs(FileSystem::CreateLocal()) {\n \tVirtualFileSystem::RegisterSubSystem(FileCompressionType::GZIP, make_uniq<GZipFileSystem>());\n }\n \n-unique_ptr<FileHandle> VirtualFileSystem::OpenFile(const string &path, FileOpenFlags flags,\n-                                                   optional_ptr<FileOpener> opener) {\n+unique_ptr<FileHandle> VirtualFileSystem::OpenFileExtended(const OpenFileInfo &file, FileOpenFlags flags,\n+                                                           optional_ptr<FileOpener> opener) {\n \tauto compression = flags.Compression();\n \tif (compression == FileCompressionType::AUTO_DETECT) {\n \t\t// auto-detect compression settings based on file name\n-\t\tauto lower_path = StringUtil::Lower(path);\n+\t\tauto lower_path = StringUtil::Lower(file.path);\n \t\tif (StringUtil::EndsWith(lower_path, \".tmp\")) {\n \t\t\t// strip .tmp\n \t\t\tlower_path = lower_path.substr(0, lower_path.length() - 4);\n \t\t}\n-\t\tif (IsFileCompressed(path, FileCompressionType::GZIP)) {\n+\t\tif (IsFileCompressed(file.path, FileCompressionType::GZIP)) {\n \t\t\tcompression = FileCompressionType::GZIP;\n-\t\t} else if (IsFileCompressed(path, FileCompressionType::ZSTD)) {\n+\t\t} else if (IsFileCompressed(file.path, FileCompressionType::ZSTD)) {\n \t\t\tcompression = FileCompressionType::ZSTD;\n \t\t} else {\n \t\t\tcompression = FileCompressionType::UNCOMPRESSED;\n \t\t}\n \t}\n \t// open the base file handle in UNCOMPRESSED mode\n \tflags.SetCompression(FileCompressionType::UNCOMPRESSED);\n-\tauto file_handle = FindFileSystem(path).OpenFile(path, flags, opener);\n+\tauto file_handle = FindFileSystem(file.path).OpenFile(file, flags, opener);\n \tif (!file_handle) {\n \t\treturn nullptr;\n \t}\n---\nsrc/function/table/read_file.cpp\n@@ -133,7 +133,7 @@ static void ReadFileExecute(ClientContext &context, TableFunctionInput &input, D\n \n \t\t// Given the columns requested, do we even need to open the file?\n \t\tif (state.requires_file_open) {\n-\t\t\tfile_handle = fs.OpenFile(file.path, FileFlags::FILE_FLAGS_READ);\n+\t\t\tfile_handle = fs.OpenFile(file, FileFlags::FILE_FLAGS_READ);\n \t\t}\n \n \t\tfor (idx_t col_idx = 0; col_idx < state.column_ids.size(); col_idx++) {\n---\nsrc/include/duckdb/common/opener_file_system.hpp\n@@ -22,13 +22,6 @@ class OpenerFileSystem : public FileSystem {\n \tvoid VerifyCanAccessDirectory(const string &path);\n \tvoid VerifyCanAccessFile(const string &path);\n \n-\tunique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags,\n-\t                                optional_ptr<FileOpener> opener = nullptr) override {\n-\t\tVerifyNoOpener(opener);\n-\t\tVerifyCanAccessFile(path);\n-\t\treturn GetFileSystem().OpenFile(path, flags, GetOpener());\n-\t}\n-\n \tvoid Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override {\n \t\tGetFileSystem().Read(handle, buffer, nr_bytes, location);\n \t};\n@@ -152,6 +145,18 @@ class OpenerFileSystem : public FileSystem {\n \t\treturn GetFileSystem().ListSubSystems();\n \t}\n \n+protected:\n+\tunique_ptr<FileHandle> OpenFileExtended(const OpenFileInfo &file, FileOpenFlags flags,\n+\t                                        optional_ptr<FileOpener> opener = nullptr) override {\n+\t\tVerifyNoOpener(opener);\n+\t\tVerifyCanAccessFile(file.path);\n+\t\treturn GetFileSystem().OpenFile(file, flags, GetOpener());\n+\t}\n+\n+\tbool SupportsOpenFileExtended() const override {\n+\t\treturn true;\n+\t}\n+\n private:\n \tvoid VerifyCanAccessFileInternal(const string &path, FileType type);\n };\n---\nsrc/include/duckdb/common/virtual_file_system.hpp\n@@ -19,9 +19,6 @@ class VirtualFileSystem : public FileSystem {\n public:\n \tVirtualFileSystem();\n \n-\tunique_ptr<FileHandle> OpenFile(const string &path, FileOpenFlags flags,\n-\t                                optional_ptr<FileOpener> opener = nullptr) override;\n-\n \tvoid Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n \tvoid Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) override;\n \n@@ -71,6 +68,13 @@ class VirtualFileSystem : public FileSystem {\n \n \tstring PathSeparator(const string &path) override;\n \n+protected:\n+\tunique_ptr<FileHandle> OpenFileExtended(const OpenFileInfo &file, FileOpenFlags flags,\n+\t                                        optional_ptr<FileOpener> opener) override;\n+\tbool SupportsOpenFileExtended() const override {\n+\t\treturn true;\n+\t}\n+\n private:\n \tFileSystem &FindFileSystem(const string &path);\n \tFileSystem &FindFileSystemInternal(const string &path);\n---\n"}], "ideal": "Title: Pull OpenFileExtended through the opener and virtual file system layers\n\nBody: This allows the `OpenFileInfo` to actually be used in the file systems' `OpenFileExtended` call."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/Main.yml\n@@ -267,21 +267,22 @@ jobs:\n  valgrind:\n     name: Valgrind\n     if: ${{ !startsWith(github.ref, 'refs/tags/v') }}\n-    runs-on: ubuntu-22.04\n+    runs-on: ubuntu-24.04\n     needs: force-storage\n     env:\n-      CC: gcc-10\n-      CXX: g++-10\n+      CC: clang\n+      CXX: clang++\n       DISABLE_SANITIZER: 1\n       BUILD_JEMALLOC: 1\n+      CORE_EXTENSIONS: 'icu;json;parquet;tpch'\n       GEN: ninja\n \n     steps:\n     - uses: actions/checkout@v4\n \n     - name: Install\n       shell: bash\n-      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build valgrind\n+      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build valgrind clang\n \n     - name: Setup Ccache\n       uses: hendrikmuhs/ccache-action@main\n@@ -291,15 +292,15 @@ jobs:\n \n     - name: Build\n       shell: bash\n-      run: make debug\n+      run: make relassert\n \n     - name: Output version info\n       shell: bash\n-      run: ./build/debug/duckdb -c \"PRAGMA version;\"\n+      run: ./build/relassert/duckdb -c \"PRAGMA version;\"\n \n     - name: Test\n       shell: bash\n-      run: valgrind ./build/debug/test/unittest test/sql/tpch/tpch_sf001.test_slow\n+      run: valgrind ./build/relassert/test/unittest test/sql/tpch/tpch_sf001.test_slow\n \n  old-gcc:\n     name: GCC 4.8\n---\n"}], "ideal": "Title: Valgrind requires tpch\n\nBody: Fixup valgrind test in Main.yml, currently not actually running any test."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": "src/include/duckdb/storage/table/update_segment.hpp\n@@ -93,7 +93,7 @@ class UpdateSegment {\n \tstatistics_update_function_t statistics_update_function;\n \n private:\n-\tUndoBufferPointer GetUpdateNode(idx_t vector_idx) const;\n+\tUndoBufferPointer GetUpdateNode(StorageLockKey &lock, idx_t vector_idx) const;\n \tvoid InitializeUpdateInfo(idx_t vector_idx);\n \tvoid InitializeUpdateInfo(UpdateInfo &info, row_t *ids, const SelectionVector &sel, idx_t count, idx_t vector_index,\n \t                          idx_t vector_offset);\n---\nsrc/storage/table/update_segment.cpp\n@@ -198,7 +198,7 @@ static UpdateSegment::fetch_update_function_t GetFetchUpdateFunction(PhysicalTyp\n \t}\n }\n \n-UndoBufferPointer UpdateSegment::GetUpdateNode(idx_t vector_idx) const {\n+UndoBufferPointer UpdateSegment::GetUpdateNode(StorageLockKey &, idx_t vector_idx) const {\n \tif (!root) {\n \t\treturn UndoBufferPointer();\n \t}\n@@ -210,7 +210,7 @@ UndoBufferPointer UpdateSegment::GetUpdateNode(idx_t vector_idx) const {\n \n void UpdateSegment::FetchUpdates(TransactionData transaction, idx_t vector_index, Vector &result) {\n \tauto lock_handle = lock.GetSharedLock();\n-\tauto node = GetUpdateNode(vector_index);\n+\tauto node = GetUpdateNode(*lock_handle, vector_index);\n \tif (!node.IsSet()) {\n \t\treturn;\n \t}\n@@ -280,7 +280,7 @@ static UpdateSegment::fetch_committed_function_t GetFetchCommittedFunction(Physi\n \n void UpdateSegment::FetchCommitted(idx_t vector_index, Vector &result) {\n \tauto lock_handle = lock.GetSharedLock();\n-\tauto node = GetUpdateNode(vector_index);\n+\tauto node = GetUpdateNode(*lock_handle, vector_index);\n \tif (!node.IsSet()) {\n \t\treturn;\n \t}\n@@ -387,8 +387,9 @@ void UpdateSegment::FetchCommittedRange(idx_t start_row, idx_t count, Vector &re\n \tidx_t end_vector = (end_row - 1) / STANDARD_VECTOR_SIZE;\n \tD_ASSERT(start_vector <= end_vector);\n \n+\tauto lock_handle = lock.GetSharedLock();\n \tfor (idx_t vector_idx = start_vector; vector_idx <= end_vector; vector_idx++) {\n-\t\tauto entry = GetUpdateNode(vector_idx);\n+\t\tauto entry = GetUpdateNode(*lock_handle, vector_idx);\n \t\tif (!entry.IsSet()) {\n \t\t\tcontinue;\n \t\t}\n@@ -483,7 +484,8 @@ static UpdateSegment::fetch_row_function_t GetFetchRowFunction(PhysicalType type\n \n void UpdateSegment::FetchRow(TransactionData transaction, idx_t row_id, Vector &result, idx_t result_idx) {\n \tidx_t vector_index = (row_id - column_data.start) / STANDARD_VECTOR_SIZE;\n-\tauto entry = GetUpdateNode(vector_index);\n+\tauto lock_handle = lock.GetSharedLock();\n+\tauto entry = GetUpdateNode(*lock_handle, vector_index);\n \tif (!entry.IsSet()) {\n \t\treturn;\n \t}\n@@ -556,7 +558,7 @@ void UpdateSegment::RollbackUpdate(UpdateInfo &info) {\n \tauto lock_handle = lock.GetExclusiveLock();\n \n \t// move the data from the UpdateInfo back into the base info\n-\tauto entry = GetUpdateNode(info.vector_index);\n+\tauto entry = GetUpdateNode(*lock_handle, info.vector_index);\n \tif (!entry.IsSet()) {\n \t\treturn;\n \t}\n@@ -1292,12 +1294,12 @@ bool UpdateSegment::HasUpdates() const {\n \n bool UpdateSegment::HasUpdates(idx_t vector_index) const {\n \tauto read_lock = lock.GetSharedLock();\n-\treturn GetUpdateNode(vector_index).IsSet();\n+\treturn GetUpdateNode(*read_lock, vector_index).IsSet();\n }\n \n bool UpdateSegment::HasUncommittedUpdates(idx_t vector_index) {\n \tauto read_lock = lock.GetSharedLock();\n-\tauto entry = GetUpdateNode(vector_index);\n+\tauto entry = GetUpdateNode(*read_lock, vector_index);\n \tif (!entry.IsSet()) {\n \t\treturn false;\n \t}\n@@ -1317,7 +1319,7 @@ bool UpdateSegment::HasUpdates(idx_t start_row_index, idx_t end_row_index) {\n \tidx_t base_vector_index = start_row_index / STANDARD_VECTOR_SIZE;\n \tidx_t end_vector_index = end_row_index / STANDARD_VECTOR_SIZE;\n \tfor (idx_t i = base_vector_index; i <= end_vector_index; i++) {\n-\t\tauto entry = GetUpdateNode(i);\n+\t\tauto entry = GetUpdateNode(*read_lock, i);\n \t\tif (entry.IsSet()) {\n \t\t\treturn true;\n \t\t}\n---\ntest/sql/parallelism/interquery/concurrent_index_reads_while_updating.test_slow\n@@ -0,0 +1,40 @@\n+# name: test/sql/parallelism/interquery/concurrent_index_reads_while_updating.test_slow\n+# description: Test concurrent index reads while updating\n+# group: [interquery]\n+\n+statement ok\n+CREATE TABLE integers(i INTEGER PRIMARY KEY, value BIGINT)\n+\n+statement ok\n+INSERT INTO integers SELECT i, i%10 FROM range(10) t(i);\n+\n+# 10 update threads, 10 reading threads\n+concurrentloop threadid 0 20\n+\n+loop i 0 500\n+\n+skipif threadid<=10\n+statement ok\n+SELECT * FROM integers WHERE i=(hash(${threadid} + ${i})%100)\n+\n+endloop\n+\n+loop i 0 100\n+\n+skipif threadid>10\n+statement ok\n+UPDATE integers SET value = value + 1 WHERE i=${threadid}\n+\n+skipif threadid>10\n+statement ok\n+UPDATE integers SET value = value - 1 WHERE i=${threadid}\n+\n+\n+endloop\n+\n+endloop\n+\n+query II\n+SELECT COUNT(*), SUM(i) FROM integers\n+----\n+10\t45\n---\n"}], "ideal": "Title: Add missing lock to UpdateSegment::FetchRow, and cleanup API to require the lock\n\nBody: Fixes a data race when mixing index lookups together with updates.\r\n\r\nAlso clean-up the API to require the lock so this cannot be forgotten again going forward.\r\n\r\nCC @staticlibs"}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/NotifyExternalRepositories.yml\n@@ -44,6 +44,19 @@ jobs:\n           export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"duckdb-sha\": \"${{ inputs.duckdb-sha }}\"}}'\n           curl -v -XPOST -u \"${PAT_USER}:${PAT_TOKEN}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" $URL --data \"$DATA\"\n \n+  notify-jdbc-run:\n+    name: Run JDBC Vendor\n+    runs-on: ubuntu-latest\n+    env:\n+      PAT_USER: ${{ secrets.PAT_USERNAME }}\n+      PAT_TOKEN: ${{ secrets.PAT_TOKEN }}\n+    steps:\n+      - name: Run JDBC Vendor\n+        if: ${{ github.repository == 'duckdb/duckdb' }}\n+        run: |\n+          export URL=https://api.github.com/repos/duckdb/duckdb-java/actions/workflows/Vendor.yml/dispatches\n+          export DATA='{\"ref\": \"refs/heads/${{ inputs.target-branch }}\", \"inputs\": {\"duckdb-sha\": \"${{ inputs.duckdb-sha }}\"}}'\n+          curl -v -XPOST -u \"${PAT_USER}:${PAT_TOKEN}\" -H \"Accept: application/vnd.github.everest-preview+json\" -H \"Content-Type: application/json\" $URL --data \"$DATA\"\n \n   notify-nightly-build-status:\n     name: Run Nightly build status\n---\n"}], "ideal": "Title: Notify JDBC repo to run Vendor.yml workflow\n\nBody: This change adds a dispatch call to `Vendor.yml` JDBC workflow the same way it is now done for ODBC repo. On JDBC side the workflow changes are being added in PR duckdb/duckdb-java#172 .\r\n\r\nODBC call was initially added in #16730, see details in that PR."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/Extensions.yml\n@@ -0,0 +1,105 @@\n+name: Python\n+on:\n+  workflow_call:\n+    inputs:\n+      override_git_describe:\n+        type: string\n+      git_ref:\n+        type: string\n+      skip_tests:\n+        type: string\n+      run_all:\n+        type: string\n+  workflow_dispatch:\n+    inputs:\n+      override_git_describe:\n+        type: string\n+      git_ref:\n+        type: string\n+      skip_tests:\n+        type: string\n+      run_all:\n+        type: string\n+  push:\n+    branches-ignore:\n+      - 'main'\n+      - 'feature'\n+      - 'v*.*-*'\n+    paths-ignore:\n+      - '**.md'\n+      - 'examples/**'\n+      - 'test/**'\n+      - 'tools/**'\n+      - '!tools/pythonpkg/**'\n+      - '.github/patches/duckdb-wasm/**'\n+      - '.github/workflows/**'\n+      - '!.github/workflows/Extensions.yml'\n+  merge_group:\n+  pull_request:\n+    types: [opened, reopened, ready_for_review]\n+    paths-ignore:\n+      - '**.md'\n+      - 'examples/**'\n+      - 'test/**'\n+      - 'tools/**'\n+      - '!tools/pythonpkg/**'\n+      - '.github/patches/duckdb-wasm/**'\n+      - '.github/workflows/**'\n+      - '!.github/workflows/Extensions.yml'\n+\n+\n+concurrency:\n+  group: extensions-${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || '' }}-${{ github.base_ref || '' }}-${{ github.ref != 'refs/heads/main' || github.sha }}-${{ inputs.override_git_describe }}\n+  cancel-in-progress: true\n+\n+env:\n+  GH_TOKEN: ${{ secrets.GH_TOKEN }}\n+  OVERRIDE_GIT_DESCRIBE: ${{ inputs.override_git_describe }}\n+\n+jobs:\n+  manylinux-extensions-x64:\n+    name: Linux Extensions (linux_amd64_gcc4)\n+    runs-on: ubuntu-latest\n+    strategy:\n+      matrix:\n+        duckdb_arch: [linux_amd64_gcc4]\n+        vcpkg_triplet: [x64-linux]\n+  \n+    env:\n+      VCPKG_TARGET_TRIPLET: ${{ matrix.vcpkg_triplet }}\n+      VCPKG_TOOLCHAIN_PATH: ${{ github.workspace }}/vcpkg/scripts/buildsystems/vcpkg.cmake\n+      GEN: ninja\n+      DUCKDB_PLATFORM: ${{ matrix.duckdb_arch }}\n+\n+    steps:\n+      - uses: actions/checkout@v4\n+        with:\n+          path: 'duckdb'\n+          fetch-depth: 0\n+          ref: ${{ inputs.git_ref }}\n+\n+      - uses: ./duckdb/.github/actions/build_extensions_dockerized\n+        with:\n+          vcpkg_target_triplet: x64-linux\n+          duckdb_arch: linux_amd64_gcc4\n+          run_tests: ${{ inputs.skip_tests != 'true' }}\n+          override_git_describe: ${{ inputs.override_git_describe }}\n+\n+      - uses: actions/upload-artifact@v4\n+        with:\n+          name: duckdb-extensions-${{ matrix.duckdb_arch }}\n+          path: |\n+            build/release/extension/*/*.duckdb_extension\n+\n+  upload-linux-extensions-gcc4:\n+    name: Upload Linux Extensions (gcc4)\n+    needs: manylinux-extensions-x64\n+    strategy:\n+      matrix:\n+        duckdb_arch: [linux_amd64_gcc4]\n+    uses: ./.github/workflows/_sign_deploy_extensions.yml\n+    secrets: inherit\n+    with:\n+      extension_artifact_name: duckdb-extensions-${{ matrix.duckdb_arch }}\n+      duckdb_arch: ${{ matrix.duckdb_arch }}\n+      duckdb_sha: ${{ github.sha }}\n---\n.github/workflows/InvokeCI.yml\n@@ -19,6 +19,15 @@ concurrency:\n   cancel-in-progress: true\n \n jobs:\n+  extensions:\n+    uses: ./.github/workflows/Extensions.yml\n+    secrets: inherit\n+    with:\n+      override_git_describe: ${{ inputs.override_git_describe }}\n+      git_ref: ${{ inputs.git_ref }}\n+      skip_tests: ${{ inputs.skip_tests }}\n+      run_all: ${{ inputs.run_all }}\n+\n   osx:\n     uses: ./.github/workflows/OSX.yml\n     secrets: inherit\n@@ -91,6 +100,7 @@ jobs:\n     uses: ./.github/workflows/NotifyExternalRepositories.yml\n     secrets: inherit\n     needs:\n+      - extensions\n       - osx\n       - linux-release\n       - windows\n@@ -108,6 +118,7 @@ jobs:\n     uses: ./.github/workflows/NotifyExternalRepositories.yml\n     secrets: inherit\n     needs:\n+      - extensions\n       - osx\n       - linux-release\n       - windows\n@@ -119,4 +130,4 @@ jobs:\n     if: ${{ inputs.git_ref != '' }}\n     with:\n       target-branch: ${{ github.ref_name }}\n-      duckdb-sha: ${{ inputs.git_ref }}\n\\ No newline at end of file\n+      duckdb-sha: ${{ inputs.git_ref }}\n---\n.github/workflows/Python.yml\n@@ -123,54 +123,6 @@ jobs:\n         # TODO: Use ccache inside container, see https://github.com/pypa/cibuildwheel/issues/1030\n         cibuildwheel --output-dir wheelhouse --config-file pyproject.toml duckdb_tarball\n \n-  manylinux-extensions-x64:\n-    name: Linux Extensions (linux_amd64_gcc4)\n-    needs: linux-python3-10\n-    runs-on: ubuntu-latest\n-    strategy:\n-      matrix:\n-        duckdb_arch: [linux_amd64_gcc4]\n-        vcpkg_triplet: [x64-linux]\n-  \n-    env:\n-      VCPKG_TARGET_TRIPLET: ${{ matrix.vcpkg_triplet }}\n-      VCPKG_TOOLCHAIN_PATH: ${{ github.workspace }}/vcpkg/scripts/buildsystems/vcpkg.cmake\n-      GEN: ninja\n-      DUCKDB_PLATFORM: ${{ matrix.duckdb_arch }}\n-\n-    steps:\n-      - uses: actions/checkout@v4\n-        with:\n-          path: 'duckdb'\n-          fetch-depth: 0\n-          ref: ${{ inputs.git_ref }}\n-\n-      - uses: ./duckdb/.github/actions/build_extensions_dockerized\n-        with:\n-          vcpkg_target_triplet: x64-linux\n-          duckdb_arch: linux_amd64_gcc4\n-          run_tests: ${{ inputs.skip_tests != 'true' }}\n-          override_git_describe: ${{ inputs.override_git_describe }}\n-\n-      - uses: actions/upload-artifact@v4\n-        with:\n-          name: duckdb-extensions-${{ matrix.duckdb_arch }}\n-          path: |\n-            build/release/extension/*/*.duckdb_extension\n-\n-  upload-linux-extensions-gcc4:\n-    name: Upload Linux Extensions (gcc4)\n-    needs: manylinux-extensions-x64\n-    strategy:\n-      matrix:\n-        duckdb_arch: [linux_amd64_gcc4]\n-    uses: ./.github/workflows/_sign_deploy_extensions.yml\n-    secrets: inherit\n-    with:\n-      extension_artifact_name: duckdb-extensions-${{ matrix.duckdb_arch }}\n-      duckdb_arch: ${{ matrix.duckdb_arch }}\n-      duckdb_sha: ${{ github.sha }}\n-\n   linux-python3:\n     name: Python 3 Linux\n     needs: manylinux-extensions-x64\n@@ -247,11 +199,12 @@ jobs:\n         pip install 'cibuildwheel>=2.16.2' build\n         python -m pip install numpy --config-settings=setup-args=\"-Dallow-noblas=true\"\n \n-    - uses: actions/download-artifact@v4\n-      if: ${{ matrix.arch == 'x86_64' }}\n-      with:\n-        name: duckdb-extensions-${{ matrix.duckdb_arch }}\n-        path: tools/pythonpkg\n+    # This could be considered to be added back IF step is executed later than Extensions.yml\n+    #- uses: actions/download-artifact@v4\n+    #  if: ${{ matrix.arch == 'x86_64' }}\n+    #  with:\n+    #    name: duckdb-extensions-${{ matrix.duckdb_arch }}\n+    #    path: tools/pythonpkg\n \n     - name: List extensions to be tested\n       shell: bash\n---\n"}], "ideal": "Title: Remove linux_amd64_gcc4 extensions build job from Python.yml to Extensions.yml\n\nBody: Currently Pyhton.yml does 2 different things:\r\n* build python source codes and wheels\r\n* build extensions for `linux_amd64_gcc4`\r\n\r\nThis is somewhat wasteful, since `linux_amd64_gcc4` are being moved out in any case, so this particular problem is solved, but more in general I would think we should structure CI like:\r\n1. a job, Extensions.yml, responsible for building and deploying extensions on all platforms\r\n2. other jobs, in separate workflows, build binaries that might use those extensions\r\n3. a testing job that combines some extensions and some binaries, testing they work together\r\n4. workflows in separate repositories that might depend on extensions being built\r\n\r\nFor example with Python:\r\n* as long as part of duckdb/duckdb repository, it can still test extensions paired with Python wheels, but without being restricted on only `linux_amd64_gcc4`\r\n* once moved out of core: the same can be implemented but mono-directional, python repository is informed of extensions being buillt, and then it can build wheels AND test them in combinations with extensions (and publish only on success).\r\n\r\nThis PR has conflicts with https://github.com/duckdb/duckdb/pull/16956, maybe thinking about this that needs to go in first."}
{"input": [{"role": "system", "content": "Given the following code diff, generate a title and a description of a GitHub pull request that best represents this code diff.\n\nWrite your answer in the following format:\n\nTitle: [[Title of the GitHub Pull Request]]\n\nBody:  [[Description of the GitHub Pull Request]]"}, {"role": "user", "content": ".github/workflows/NightlyTests.yml\n@@ -101,6 +101,80 @@ jobs:\n       run: |\n           python3 scripts/run_tests_one_by_one.py build/relassert/test/unittest \"*\" --no-exit --timeout 1200\n \n+  release-assert-osx:\n+    name: Release Assertions OSX\n+    runs-on: macos-latest\n+    needs: linux-memory-leaks\n+    env:\n+      GEN: ninja\n+      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet\"\n+      DISABLE_SANITIZER: 1\n+      CRASH_ON_ASSERT: 1\n+      RUN_SLOW_VERIFIERS: 1\n+\n+    steps:\n+    - uses: actions/checkout@v3\n+      with:\n+        fetch-depth: 0\n+    - uses: actions/setup-python@v5\n+      with:\n+        python-version: '3.12'\n+\n+    - name: Install Ninja\n+      run: brew install ninja\n+\n+    - name: Setup Ccache\n+      uses: hendrikmuhs/ccache-action@main\n+      with:\n+        key: ${{ github.job }}\n+        save: ${{ env.CCACHE_SAVE }}\n+\n+    - name: Build\n+      shell: bash\n+      run: UNSAFE_NUMERIC_CAST=1 make relassert\n+\n+    - name: Test\n+      shell: bash\n+      run: |\n+          python3 scripts/run_tests_one_by_one.py build/relassert/test/unittest \"*\" --no-exit --timeout 1200\n+\n+  release-assert-osx-storage:\n+    name: Release Assertions OSX Storage\n+    runs-on: macos-latest\n+    needs: linux-memory-leaks\n+    env:\n+      GEN: ninja\n+      CORE_EXTENSIONS: \"icu;tpch;tpcds;fts;json;inet\"\n+      DISABLE_SANITIZER: 1\n+      CRASH_ON_ASSERT: 1\n+      RUN_SLOW_VERIFIERS: 1\n+\n+    steps:\n+    - uses: actions/checkout@v3\n+      with:\n+        fetch-depth: 0\n+    - uses: actions/setup-python@v5\n+      with:\n+        python-version: '3.12'\n+\n+    - name: Install Ninja\n+      run: brew install ninja\n+\n+    - name: Setup Ccache\n+      uses: hendrikmuhs/ccache-action@main\n+      with:\n+        key: ${{ github.job }}\n+        save: ${{ env.CCACHE_SAVE }}\n+\n+    - name: Build\n+      shell: bash\n+      run: UNSAFE_NUMERIC_CAST=1 make relassert\n+\n+    - name: Test\n+      shell: bash\n+      run: |\n+          python3 scripts/run_tests_one_by_one.py build/relassert/test/unittest \"*\" --no-exit --timeout 1200 --force-storage\n+\n   smaller-binary:\n     name: Smaller Binary\n     runs-on: ubuntu-24.04\n---\n.github/workflows/R_CMD_CHECK.yml\n@@ -39,6 +39,7 @@ env:\n \n jobs:\n   R-CMD-check:\n+    if: false\n     runs-on: ${{ matrix.config.os }}\n \n     name: ${{ matrix.config.os }} (${{ matrix.config.r }})\n---\ntools/sqlite3_api_wrapper/CMakeLists.txt\n@@ -28,7 +28,8 @@ link_threads(sqlite3_api_wrapper_static \"\")\n \n if(NOT WIN32 AND NOT ZOS)\n   add_library(sqlite3_api_wrapper SHARED ${SQLITE_API_WRAPPER_FILES})\n-  target_link_libraries(sqlite3_api_wrapper duckdb ${DUCKDB_EXTRA_LINK_FLAGS})\n+  target_link_libraries(sqlite3_api_wrapper duckdb_static\n+                        ${DUCKDB_EXTRA_LINK_FLAGS})\n   link_threads(sqlite3_api_wrapper \"\")\n endif()\n \n---\n"}], "ideal": "Title: Fix sqlite3 api wrapper link + remove R-CMD-check + add more nightly tests\n\nBody: Three fixes:\r\n1. Fix Relassert build via linking `sqlite3_api_wrapper` with `duckdb_static`.\r\n\r\n2. Skip R-CMD-check.yml (this has been failing due to a warning, needs to be fixed and re-enabled)\r\n\r\n3. add two new nightly tests:\r\n* OSX + relassert\r\n* OSX + relassert + `--force-storage`\r\n\r\nThis has also the nice consequence that sqlite3_api_wrapper linking (that caused Release Assert step to fail linking) is now tested in this PR."}
